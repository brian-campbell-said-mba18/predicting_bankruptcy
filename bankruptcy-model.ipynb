{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: The Efficacy of Multilayer Perceptron Algorithms in Predicting Bankruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><a href=\"#introduction\">INTRODUCTION</a></li>\n",
    "<li><a href=\"#Benchmark Logistic Regression\">Benchmark Logistic Regression</a></li>\n",
    "<li><a href=\"#MLP Model\">MLP Model</a></li>\n",
    "<li><a href=\"#assess\">Data Assessment</a></li>\n",
    "<li><a href=\"#cleaning\">Data Cleaning</a></li>\n",
    "<li><a href=\"#target\">Separation of Target Variables</a></li>\n",
    "<li><a href=\"#no nulls\">No Nulls Data</a></li>\n",
    "<li><a href=\"#one-hot null\">Creation of One-hot Null Variable</a></li>\n",
    "<li><a href=\"#Creation of Sum Null Variables\">Creation of Sum Null Variables</a></li>\n",
    "<li><a href=\"#PIVOT\">PIVOT</a></li>\n",
    "<li><a href=\"#Data Reorganization\">Data Reorganization</a></li>\n",
    "<li><a href=\"#Data Exploration - Descriptive Statistics\">Data Exploration - Descriptive Statistics</a></li>\n",
    "<li><a href=\"#Exploratory Visualization\">Exploratory Visualization</a></li>\n",
    "<li><a href=\"#Preprocessing\">Preprocessing</a></li>\n",
    "<li><a href=\"#Benchmark: Logistic Regression\">Benchmark: Logistic Regression</a></li>\n",
    "<li><a href=\"#originalMLP\">Origial MLP</a></li>\n",
    "<li><a href=\"#conclusion\">Conclusion</a></li> \n",
    "<li><a href=\"#references\">References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Benchmark Logistic Regression'></a>\n",
    "## Benchmark Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary libraries for the logistic regression models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This imports the AUC score for scoring the models.\n",
    "# This comes from Reference 27 in References.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# These are libraries that will be needed to organize data,\n",
    "# graph data, and change the working directory.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the no_nulls X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nonulls = pd.read_csv('no-peaking/Xtrain_nonulls.csv')\n",
    "Xtrain_nonulls = np.array(Xtrain_nonulls)\n",
    "Xtest_nonulls = pd.read_csv('no-peaking/Xtest_nonulls.csv')\n",
    "Xtest_nonulls = np.array(Xtest_nonulls)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nonulls = pd.read_csv('no-peaking/Ytrain_nonulls.csv')\n",
    "Ytrain_nonulls = np.array(Ytrain_nonulls)\n",
    "Ytrain_nonulls = Ytrain_nonulls.ravel() \n",
    "Ytest_nonulls = pd.read_csv('no-peaking/Ytest_nonulls.csv')\n",
    "Ytest_nonulls = np.array(Ytest_nonulls)\n",
    "Ytest_nonulls = Ytest_nonulls.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Hold-out Cross-Validation on just the training set\n",
    "K-fold Cross-Validation is a standard method for preventing a logistic regression model from overfitting. However, because the testing set, the fifth year of the dataset, is arbitrarily chosen (not random), K-fold Cross-Validation cannot be applied to the dataset (Reference 3). K-fold Cross-Validation would corrupt the testing set with data leakage considering that the dataset is a time-series set (Reference 3). To prevent data leakage, Hold-out cross-Validation will only be applied to the training set (References 3 & 4). Hold-out Cross-Validation takes a percentage of the training set as a validation set to test the accuracy of the model during the training stage. This method of cross validation, like all methods, is used to prevent the overfitting of a model and poor accuracy performance when applying the testing data to the fitted model (Reference 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Nulls Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import train_test_split from sklearn.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the No Nulls dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nonulls, Xval_nonulls, Ytrain_nonulls, Yval_nonulls = train_test_split(\n",
    "                    Xtrain_nonulls, Ytrain_nonulls, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8506022247116249"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the No Nulls Dataset.\n",
    "log_nonulls = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_nonulls model with the training data.\n",
    "log_nonulls.fit(Xtrain_nonulls,Ytrain_nonulls)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nonulls = log_nonulls.predict(Xval_nonulls)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nonulls = roc_auc_score(Yval_nonulls, yval_pred_nonulls)\n",
    "VAL_auc_nonulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.3163.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nonulls = log_nonulls.predict(Xtest_nonulls)\n",
    "TEST_auc_nonulls = roc_auc_score(Ytest_nonulls, ytest_pred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Nulls only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This loads the Nulls only training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nullsonly = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_nullsonly = Xtrain_nullsonly.drop(\n",
    "    Xtrain_nullsonly.columns[64], axis=1)\n",
    "Xtrain_nullsonly = np.array(Xtrain_nullsonly)\n",
    "Xtest_nullsonly = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_nullsonly = Xtest_nullsonly.drop(\n",
    "    Xtest_nullsonly.columns[64], axis=1)\n",
    "Xtest_nullsonly = np.array(Xtest_nullsonly)\n",
    "\n",
    "# This loads the Nulls only Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nullsonly = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_nullsonly = np.array(Ytrain_nullsonly)\n",
    "Ytrain_nullsonly = Ytrain_nullsonly.ravel() \n",
    "Ytest_nullsonly = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_nullsonly = np.array(Ytest_nullsonly)\n",
    "Ytest_nullsonly = Ytest_nullsonly.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls Only Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nullsonly, Xval_nullsonly, Ytrain_nullsonly, Yval_nullsonly = train_test_split(\n",
    "                    Xtrain_nullsonly, Ytrain_nullsonly, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7350826303765834"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the Nulls Only Dataset.\n",
    "log_nullsonly = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_nullsonly.fit(Xtrain_nullsonly,Ytrain_nullsonly)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nullsonly = log_nullsonly.predict(Xval_nullsonly)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nullsonly = roc_auc_score(Yval_nullsonly, yval_pred_nullsonly)\n",
    "VAL_auc_nullsonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 68.29 %\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nullsonly = log_nullsonly.predict(Xtest_nullsonly)\n",
    "TEST_auc_nullsonly = roc_auc_score(Ytest_nullsonly, ytest_pred_nullsonly)\n",
    "TEST_auc_nullsonly = TEST_auc_nullsonly * 100\n",
    "print(\"The AUC score for the model is %.2f\" % TEST_auc_nullsonly, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the one hot X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_onehot = pd.read_csv('no-peaking/Xtrain_onehot.csv')\n",
    "Xtrain_onehot = np.array(Xtrain_onehot)\n",
    "Xtest_onehot = pd.read_csv('no-peaking/Xtest_onehot.csv')\n",
    "Xtest_onehot = np.array(Xtest_onehot)\n",
    "\n",
    "# This loads the one hot Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_onehot = pd.read_csv('no-peaking/Ytrain_onehot.csv')\n",
    "Ytrain_onehot = np.array(Ytrain_onehot)\n",
    "Ytrain_onehot = Ytrain_onehot.ravel() \n",
    "Ytest_onehot = pd.read_csv('no-peaking/Ytest_onehot.csv')\n",
    "Ytest_onehot = np.array(Ytest_onehot)\n",
    "Ytest_onehot = Ytest_onehot.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the One Hot dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_onehot, Xval_onehot, Ytrain_onehot, Yval_onehot = train_test_split(\n",
    "                    Xtrain_onehot, Ytrain_onehot, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8647068328667264"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_onehot = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_onehot.fit(Xtrain_onehot,Ytrain_onehot)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_onehot = log_onehot.predict(Xval_onehot)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_onehot = roc_auc_score(Yval_onehot, yval_pred_onehot)\n",
    "VAL_auc_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6406.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_onehot = log_onehot.predict(Xtest_onehot)\n",
    "TEST_auc_onehot = roc_auc_score(Ytest_onehot, ytest_pred_onehot)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the SUM X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_sum = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_sum = np.array(Xtrain_sum)\n",
    "Xtest_sum = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_sum = np.array(Xtest_sum)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_sum = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_sum = np.array(Ytrain_sum)\n",
    "Ytrain_sum = Ytrain_sum.ravel() \n",
    "Ytest_sum = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_sum = np.array(Ytest_sum)\n",
    "Ytest_sum = Ytest_sum.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_sum, Xval_sum, Ytrain_sum, Yval_sum = train_test_split(\n",
    "                    Xtrain_sum, Ytrain_sum, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7437902237728553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_sum = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_sum.fit(Xtrain_sum,Ytrain_sum)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.c\n",
    "yval_pred_sum = log_sum.predict(Xval_sum)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_sum = roc_auc_score(Yval_sum, yval_pred_sum)\n",
    "VAL_auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6606.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_sum = log_sum.predict(Xtest_sum)\n",
    "TEST_auc_sum = roc_auc_score(Ytest_sum, ytest_pred_sum)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MLP Model'></a>\n",
    "## MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'saved_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-0e0cd0986d66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This creates a directory to save the best models for the MLP.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved_models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'saved_models'"
     ]
    }
   ],
   "source": [
    "# This creates a directory to save the best models for the MLP.\n",
    "os.mkdir('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary libraries for the MLP.\n",
    "\n",
    "# This imports the sequential model, the layers,\n",
    "# the SGD optimizer, the regularizers from keras.\n",
    "# This comes from Reference 5 in Referenes.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Nadam\n",
    "from keras import regularizers\n",
    "\n",
    "# This imports checkpointer, which records the best weights\n",
    "# for the algorithm.\n",
    "# This comes from Reference 6 in References.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drop_rate, l2_factor, first_dense, second_dense,\n",
    "                third_dense, hidden_act, out_act, x):\n",
    "    dim_int = int(np.size(x,1))\n",
    "    # This defines the model as a sequential model.\n",
    "    # This comes from References 1 in References.\n",
    "    model = Sequential()\n",
    "\n",
    "    # This is the input layer.\n",
    "    # This comes from References 1 & 3 in References.\n",
    "    model.add(Dense(first_dense, activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor),\n",
    "        input_dim = dim_int))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the first hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(second_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # This creates the second hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(third_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the output layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(1, activation=out_act))\n",
    "    # This returns the model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the original MLP model for year 1.\n",
    "# using Mattson and Steinart's original hyperparameters.\n",
    "mlp_nonulls = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Null data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nonulls.compile(loss='binary_crossentropy',\n",
    "              optimizer= RMS,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nonulls.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11018 samples, validate on 2755 samples\n",
      "Epoch 1/100\n",
      "11018/11018 [==============================] - 3s 264us/step - loss: 0.7436 - acc: 0.5856 - val_loss: 0.6388 - val_acc: 0.7579\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63881, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 2/100\n",
      "11018/11018 [==============================] - 1s 68us/step - loss: 0.6589 - acc: 0.6594 - val_loss: 0.5617 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63881 to 0.56172, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 3/100\n",
      "11018/11018 [==============================] - 1s 69us/step - loss: 0.5997 - acc: 0.7087 - val_loss: 0.5029 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56172 to 0.50287, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 4/100\n",
      "11018/11018 [==============================] - 1s 68us/step - loss: 0.5485 - acc: 0.7541 - val_loss: 0.4589 - val_acc: 0.8051\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.50287 to 0.45895, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 5/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.5138 - acc: 0.7626 - val_loss: 0.4277 - val_acc: 0.8232\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45895 to 0.42772, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 6/100\n",
      "11018/11018 [==============================] - 1s 75us/step - loss: 0.4895 - acc: 0.7779 - val_loss: 0.4088 - val_acc: 0.8385\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42772 to 0.40880, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 7/100\n",
      "11018/11018 [==============================] - 1s 75us/step - loss: 0.4757 - acc: 0.7861 - val_loss: 0.3966 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40880 to 0.39656, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 8/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.4611 - acc: 0.7976 - val_loss: 0.3841 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39656 to 0.38408, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 9/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.4515 - acc: 0.7990 - val_loss: 0.3714 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.38408 to 0.37144, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 10/100\n",
      "11018/11018 [==============================] - 1s 72us/step - loss: 0.4399 - acc: 0.8088 - val_loss: 0.3721 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37144\n",
      "Epoch 11/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.4308 - acc: 0.8101 - val_loss: 0.3581 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.37144 to 0.35805, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 12/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.4256 - acc: 0.8143 - val_loss: 0.3540 - val_acc: 0.8780\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35805 to 0.35399, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 13/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.4143 - acc: 0.8190 - val_loss: 0.3380 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35399 to 0.33797, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 14/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.4158 - acc: 0.8187 - val_loss: 0.3318 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.33797 to 0.33176, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 15/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.4066 - acc: 0.8244 - val_loss: 0.3290 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.33176 to 0.32901, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 16/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.4024 - acc: 0.8222 - val_loss: 0.3269 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.32901 to 0.32695, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 17/100\n",
      "11018/11018 [==============================] - 1s 76us/step - loss: 0.4019 - acc: 0.8290 - val_loss: 0.3217 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.32695 to 0.32166, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 18/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3948 - acc: 0.8325 - val_loss: 0.3126 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.32166 to 0.31257, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 19/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.3911 - acc: 0.8339 - val_loss: 0.3102 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.31257 to 0.31021, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 20/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3890 - acc: 0.8335 - val_loss: 0.3090 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.31021 to 0.30904, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 21/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3888 - acc: 0.8355 - val_loss: 0.2972 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30904 to 0.29725, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 22/100\n",
      "11018/11018 [==============================] - 1s 75us/step - loss: 0.3931 - acc: 0.8315 - val_loss: 0.3046 - val_acc: 0.9074\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29725\n",
      "Epoch 23/100\n",
      "11018/11018 [==============================] - 1s 70us/step - loss: 0.3822 - acc: 0.8349 - val_loss: 0.3031 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.29725\n",
      "Epoch 24/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3810 - acc: 0.8435 - val_loss: 0.2970 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.29725 to 0.29702, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 25/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3790 - acc: 0.8679 - val_loss: 0.2925 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.29702 to 0.29247, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 26/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3794 - acc: 0.8706 - val_loss: 0.2916 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.29247 to 0.29155, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 27/100\n",
      "11018/11018 [==============================] - 1s 76us/step - loss: 0.3671 - acc: 0.8602 - val_loss: 0.2973 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.29155\n",
      "Epoch 28/100\n",
      "11018/11018 [==============================] - 1s 72us/step - loss: 0.3685 - acc: 0.8609 - val_loss: 0.2968 - val_acc: 0.9194\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.29155\n",
      "Epoch 29/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3706 - acc: 0.8773 - val_loss: 0.2904 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.29155 to 0.29044, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 30/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3717 - acc: 0.8703 - val_loss: 0.2881 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.29044 to 0.28814, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 31/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3690 - acc: 0.8760 - val_loss: 0.2885 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.28814\n",
      "Epoch 32/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3624 - acc: 0.8822 - val_loss: 0.2864 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.28814 to 0.28638, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 33/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3633 - acc: 0.8822 - val_loss: 0.2938 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.28638\n",
      "Epoch 34/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3627 - acc: 0.8853 - val_loss: 0.2906 - val_acc: 0.9241\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.28638\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018/11018 [==============================] - 1s 70us/step - loss: 0.3732 - acc: 0.8801 - val_loss: 0.2833 - val_acc: 0.9267\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.28638 to 0.28333, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 36/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3617 - acc: 0.8807 - val_loss: 0.2898 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.28333\n",
      "Epoch 37/100\n",
      "11018/11018 [==============================] - 1s 72us/step - loss: 0.3632 - acc: 0.8775 - val_loss: 0.2773 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.28333 to 0.27729, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 38/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3709 - acc: 0.8769 - val_loss: 0.2803 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.27729\n",
      "Epoch 39/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3572 - acc: 0.8820 - val_loss: 0.2750 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.27729 to 0.27501, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 40/100\n",
      "11018/11018 [==============================] - 1s 69us/step - loss: 0.3537 - acc: 0.8864 - val_loss: 0.2794 - val_acc: 0.9278\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.27501\n",
      "Epoch 41/100\n",
      "11018/11018 [==============================] - 1s 79us/step - loss: 0.3552 - acc: 0.8895 - val_loss: 0.2706 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.27501 to 0.27058, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 42/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3540 - acc: 0.8864 - val_loss: 0.2734 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.27058\n",
      "Epoch 43/100\n",
      "11018/11018 [==============================] - 1s 72us/step - loss: 0.3583 - acc: 0.8876 - val_loss: 0.2750 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.27058\n",
      "Epoch 44/100\n",
      "11018/11018 [==============================] - 1s 72us/step - loss: 0.3574 - acc: 0.8836 - val_loss: 0.2713 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.27058\n",
      "Epoch 45/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3583 - acc: 0.8858 - val_loss: 0.2795 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.27058\n",
      "Epoch 46/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3583 - acc: 0.8839 - val_loss: 0.2920 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.27058\n",
      "Epoch 47/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3533 - acc: 0.8818 - val_loss: 0.2748 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.27058\n",
      "Epoch 48/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3614 - acc: 0.8885 - val_loss: 0.2759 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.27058\n",
      "Epoch 49/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3605 - acc: 0.8853 - val_loss: 0.2709 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.27058\n",
      "Epoch 50/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3589 - acc: 0.8856 - val_loss: 0.2727 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.27058\n",
      "Epoch 51/100\n",
      "11018/11018 [==============================] - 1s 71us/step - loss: 0.3503 - acc: 0.8885 - val_loss: 0.2605 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.27058 to 0.26054, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 52/100\n",
      "11018/11018 [==============================] - 1s 73us/step - loss: 0.3509 - acc: 0.8885 - val_loss: 0.2767 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.26054\n",
      "Epoch 53/100\n",
      "11018/11018 [==============================] - 1s 74us/step - loss: 0.3546 - acc: 0.8865 - val_loss: 0.2618 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.26054\n",
      "Epoch 54/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3537 - acc: 0.8895 - val_loss: 0.2626 - val_acc: 0.9336\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.26054\n",
      "Epoch 55/100\n",
      "11018/11018 [==============================] - 1s 76us/step - loss: 0.3468 - acc: 0.8870 - val_loss: 0.2704 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.26054\n",
      "Epoch 56/100\n",
      "11018/11018 [==============================] - 1s 78us/step - loss: 0.3559 - acc: 0.8870 - val_loss: 0.2727 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.26054\n",
      "Epoch 57/100\n",
      "11018/11018 [==============================] - 1s 77us/step - loss: 0.3616 - acc: 0.8885 - val_loss: 0.2703 - val_acc: 0.9336\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.26054\n",
      "Epoch 58/100\n",
      "11018/11018 [==============================] - 1s 78us/step - loss: 0.3520 - acc: 0.8875 - val_loss: 0.2669 - val_acc: 0.9354\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.26054\n",
      "Epoch 59/100\n",
      "11018/11018 [==============================] - 1s 80us/step - loss: 0.3555 - acc: 0.8916 - val_loss: 0.2620 - val_acc: 0.9394\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.26054\n",
      "Epoch 60/100\n",
      "11018/11018 [==============================] - 1s 78us/step - loss: 0.3496 - acc: 0.8905 - val_loss: 0.2604 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.26054 to 0.26040, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 61/100\n",
      "11018/11018 [==============================] - 1s 79us/step - loss: 0.3559 - acc: 0.8872 - val_loss: 0.2554 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.26040 to 0.25545, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 62/100\n",
      "11018/11018 [==============================] - 1s 80us/step - loss: 0.3565 - acc: 0.8899 - val_loss: 0.2647 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.25545\n",
      "Epoch 63/100\n",
      "11018/11018 [==============================] - 1s 79us/step - loss: 0.3510 - acc: 0.8875 - val_loss: 0.2634 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.25545\n",
      "Epoch 64/100\n",
      "11018/11018 [==============================] - 1s 80us/step - loss: 0.3500 - acc: 0.8911 - val_loss: 0.2535 - val_acc: 0.9390\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.25545 to 0.25345, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 65/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3462 - acc: 0.8927 - val_loss: 0.2590 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.25345\n",
      "Epoch 66/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.3466 - acc: 0.8897 - val_loss: 0.2557 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.25345\n",
      "Epoch 67/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3526 - acc: 0.8906 - val_loss: 0.2582 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.25345\n",
      "Epoch 68/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.3491 - acc: 0.8927 - val_loss: 0.2493 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.25345 to 0.24926, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 69/100\n",
      "11018/11018 [==============================] - 1s 102us/step - loss: 0.3365 - acc: 0.8940 - val_loss: 0.2647 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.24926\n",
      "Epoch 70/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.3480 - acc: 0.8935 - val_loss: 0.2510 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.24926\n",
      "Epoch 71/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.3432 - acc: 0.8974 - val_loss: 0.2618 - val_acc: 0.9332\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24926\n",
      "Epoch 72/100\n",
      "11018/11018 [==============================] - 1s 86us/step - loss: 0.3433 - acc: 0.8932 - val_loss: 0.2581 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24926\n",
      "Epoch 73/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3418 - acc: 0.8941 - val_loss: 0.2558 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24926\n",
      "Epoch 74/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.3461 - acc: 0.8977 - val_loss: 0.2529 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24926\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.3495 - acc: 0.8902 - val_loss: 0.2637 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24926\n",
      "Epoch 76/100\n",
      "11018/11018 [==============================] - 1s 80us/step - loss: 0.3516 - acc: 0.8912 - val_loss: 0.2589 - val_acc: 0.9401\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.24926\n",
      "Epoch 77/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3457 - acc: 0.8940 - val_loss: 0.2693 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.24926\n",
      "Epoch 78/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3454 - acc: 0.8934 - val_loss: 0.2627 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24926\n",
      "Epoch 79/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3489 - acc: 0.8964 - val_loss: 0.2512 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24926\n",
      "Epoch 80/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.3495 - acc: 0.8917 - val_loss: 0.2554 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.24926\n",
      "Epoch 81/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.3410 - acc: 0.8932 - val_loss: 0.2505 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.24926\n",
      "Epoch 82/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.3403 - acc: 0.8980 - val_loss: 0.2507 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24926\n",
      "Epoch 83/100\n",
      "11018/11018 [==============================] - 1s 86us/step - loss: 0.3476 - acc: 0.8930 - val_loss: 0.2445 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.24926 to 0.24447, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 84/100\n",
      "11018/11018 [==============================] - 1s 76us/step - loss: 0.3360 - acc: 0.8981 - val_loss: 0.2604 - val_acc: 0.9285\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24447\n",
      "Epoch 85/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3382 - acc: 0.8958 - val_loss: 0.2451 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.24447\n",
      "Epoch 86/100\n",
      "11018/11018 [==============================] - 1s 76us/step - loss: 0.3399 - acc: 0.8955 - val_loss: 0.2532 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24447\n",
      "Epoch 87/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.3383 - acc: 0.8952 - val_loss: 0.2580 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24447\n",
      "Epoch 88/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.3390 - acc: 0.8961 - val_loss: 0.2580 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24447\n",
      "Epoch 89/100\n",
      "11018/11018 [==============================] - 1s 91us/step - loss: 0.3469 - acc: 0.8949 - val_loss: 0.2611 - val_acc: 0.9336\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24447\n",
      "Epoch 90/100\n",
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.3339 - acc: 0.8959 - val_loss: 0.2509 - val_acc: 0.9379\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24447\n",
      "Epoch 91/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3354 - acc: 0.9015 - val_loss: 0.2527 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24447\n",
      "Epoch 92/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3437 - acc: 0.8992 - val_loss: 0.2452 - val_acc: 0.9419\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.24447\n",
      "Epoch 93/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3357 - acc: 0.9027 - val_loss: 0.2562 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.24447\n",
      "Epoch 94/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3341 - acc: 0.8991 - val_loss: 0.2422 - val_acc: 0.9452\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.24447 to 0.24220, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 95/100\n",
      "11018/11018 [==============================] - 1s 90us/step - loss: 0.3255 - acc: 0.9011 - val_loss: 0.2479 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.24220\n",
      "Epoch 96/100\n",
      "11018/11018 [==============================] - 1s 97us/step - loss: 0.3349 - acc: 0.9006 - val_loss: 0.2499 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.24220\n",
      "Epoch 97/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.3393 - acc: 0.8955 - val_loss: 0.2477 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.24220\n",
      "Epoch 98/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.3400 - acc: 0.8997 - val_loss: 0.2425 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.24220\n",
      "Epoch 99/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3264 - acc: 0.9011 - val_loss: 0.2426 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.24220\n",
      "Epoch 100/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.3374 - acc: 0.8993 - val_loss: 0.2363 - val_acc: 0.9499\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.24220 to 0.23629, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efb28c0e80>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nonulls.fit(Xtrain_nonulls, Ytrain_nonulls, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_nonulls.load_weights('saved_models/weights.best.mlp_nonulls.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 88.9149%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_nonulls.evaluate(Xtest_nonulls, Ytest_nonulls, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.5934.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_nonulls = mlp_nonulls.predict(Xtest_nonulls)\n",
    "mlp_nonulls_ROC = roc_auc_score(Ytest_nonulls, Ypred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nonulls_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Ypred_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Frequency of unique values of the said array:\n",
      "[[   0.]\n",
      " [2995.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original array:\")\n",
    "print(Ypred_nonulls)\n",
    "unique_elements, counts_elements = np.unique(Ypred_nonulls, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Nulls Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the original MLP model for year 1.\n",
    "# using Mattson and Steinart's original hyperparameters.\n",
    "mlp_nullsonly = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nullsonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Null data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nullsonly.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nullsonly.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45858 samples, validate on 11465 samples\n",
      "Epoch 1/100\n",
      "45858/45858 [==============================] - 6s 124us/step - loss: 0.8238 - acc: 0.4551 - val_loss: 0.7691 - val_acc: 0.5593\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76908, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 2/100\n",
      "45858/45858 [==============================] - 3s 68us/step - loss: 0.7909 - acc: 0.4635 - val_loss: 0.7659 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76908 to 0.76588, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 3/100\n",
      "45858/45858 [==============================] - 3s 69us/step - loss: 0.7818 - acc: 0.4832 - val_loss: 0.7642 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76588 to 0.76417, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 4/100\n",
      "45858/45858 [==============================] - 3s 69us/step - loss: 0.7794 - acc: 0.5057 - val_loss: 0.7622 - val_acc: 0.5745\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.76417 to 0.76215, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 5/100\n",
      "45858/45858 [==============================] - 3s 70us/step - loss: 0.7777 - acc: 0.5284 - val_loss: 0.7592 - val_acc: 0.6226\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.76215 to 0.75916, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 6/100\n",
      "45858/45858 [==============================] - 3s 74us/step - loss: 0.7738 - acc: 0.5457 - val_loss: 0.7561 - val_acc: 0.6417\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75916 to 0.75607, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 7/100\n",
      "45858/45858 [==============================] - 3s 70us/step - loss: 0.7705 - acc: 0.5527 - val_loss: 0.7531 - val_acc: 0.6557\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.75607 to 0.75315, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 8/100\n",
      "45858/45858 [==============================] - 3s 70us/step - loss: 0.7683 - acc: 0.5617 - val_loss: 0.7499 - val_acc: 0.6737\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.75315 to 0.74991, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 9/100\n",
      "45858/45858 [==============================] - 3s 72us/step - loss: 0.7661 - acc: 0.5676 - val_loss: 0.7472 - val_acc: 0.6819\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.74991 to 0.74716, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 10/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.7623 - acc: 0.5781 - val_loss: 0.7447 - val_acc: 0.6866\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.74716 to 0.74465, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 11/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7610 - acc: 0.5757 - val_loss: 0.7426 - val_acc: 0.6883\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.74465 to 0.74262, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 12/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7610 - acc: 0.5804 - val_loss: 0.7408 - val_acc: 0.6890\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.74262 to 0.74082, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 13/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7582 - acc: 0.5809 - val_loss: 0.7391 - val_acc: 0.6899\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.74082 to 0.73911, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 14/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.7580 - acc: 0.5792 - val_loss: 0.7374 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.73911 to 0.73744, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 15/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7578 - acc: 0.5823 - val_loss: 0.7361 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.73744 to 0.73607, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 16/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7540 - acc: 0.5882 - val_loss: 0.7346 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.73607 to 0.73455, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 17/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7557 - acc: 0.5891 - val_loss: 0.7329 - val_acc: 0.6928\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.73455 to 0.73287, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 18/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7495 - acc: 0.5932 - val_loss: 0.7310 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.73287 to 0.73103, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 19/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7494 - acc: 0.5893 - val_loss: 0.7294 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73103 to 0.72942, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 20/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7460 - acc: 0.5928 - val_loss: 0.7276 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.72942 to 0.72759, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 21/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7481 - acc: 0.5912 - val_loss: 0.7260 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.72759 to 0.72601, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 22/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7460 - acc: 0.5941 - val_loss: 0.7247 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.72601 to 0.72469, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 23/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.7469 - acc: 0.5946 - val_loss: 0.7231 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.72469 to 0.72314, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 24/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.7438 - acc: 0.5962 - val_loss: 0.7213 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.72314 to 0.72134, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 25/100\n",
      "45858/45858 [==============================] - 4s 89us/step - loss: 0.7441 - acc: 0.5965 - val_loss: 0.7196 - val_acc: 0.6976\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.72134 to 0.71965, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 26/100\n",
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.7431 - acc: 0.5971 - val_loss: 0.7184 - val_acc: 0.6979\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.71965 to 0.71837, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 27/100\n",
      "45858/45858 [==============================] - 4s 87us/step - loss: 0.7418 - acc: 0.6000 - val_loss: 0.7171 - val_acc: 0.6985\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.71837 to 0.71706, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 28/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.7395 - acc: 0.5985 - val_loss: 0.7153 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.71706 to 0.71531, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 29/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.7408 - acc: 0.6018 - val_loss: 0.7139 - val_acc: 0.6990\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.71531 to 0.71395, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 30/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.7388 - acc: 0.6011 - val_loss: 0.7126 - val_acc: 0.6987\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.71395 to 0.71257, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 31/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.7367 - acc: 0.6044 - val_loss: 0.7111 - val_acc: 0.6995\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.71257 to 0.71106, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 32/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.7329 - acc: 0.6075 - val_loss: 0.7095 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.71106 to 0.70949, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 4s 77us/step - loss: 0.7350 - acc: 0.6084 - val_loss: 0.7078 - val_acc: 0.7001\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.70949 to 0.70781, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 34/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.7319 - acc: 0.6151 - val_loss: 0.7060 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.70781 to 0.70604, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 35/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.7297 - acc: 0.6105 - val_loss: 0.7042 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.70604 to 0.70423, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 36/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.7285 - acc: 0.6162 - val_loss: 0.7022 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.70423 to 0.70224, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 37/100\n",
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.7256 - acc: 0.6169 - val_loss: 0.7001 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.70224 to 0.70011, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 38/100\n",
      "45858/45858 [==============================] - 4s 87us/step - loss: 0.7270 - acc: 0.6147 - val_loss: 0.6981 - val_acc: 0.6993\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.70011 to 0.69811, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 39/100\n",
      "45858/45858 [==============================] - 4s 85us/step - loss: 0.7247 - acc: 0.6152 - val_loss: 0.6962 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.69811 to 0.69624, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 40/100\n",
      "45858/45858 [==============================] - 4s 89us/step - loss: 0.7212 - acc: 0.6201 - val_loss: 0.6939 - val_acc: 0.7001\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.69624 to 0.69389, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 41/100\n",
      "45858/45858 [==============================] - 5s 105us/step - loss: 0.7218 - acc: 0.6184 - val_loss: 0.6920 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.69389 to 0.69197, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 42/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.7163 - acc: 0.6254 - val_loss: 0.6896 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.69197 to 0.68958, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 43/100\n",
      "45858/45858 [==============================] - 4s 86us/step - loss: 0.7188 - acc: 0.6168 - val_loss: 0.6878 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.68958 to 0.68783, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 44/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.7164 - acc: 0.6249 - val_loss: 0.6858 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.68783 to 0.68579, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 45/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.7130 - acc: 0.6221 - val_loss: 0.6837 - val_acc: 0.7003\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.68579 to 0.68368, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 46/100\n",
      "45858/45858 [==============================] - 4s 86us/step - loss: 0.7119 - acc: 0.6242 - val_loss: 0.6818 - val_acc: 0.7005\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.68368 to 0.68183, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 47/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.7085 - acc: 0.6292 - val_loss: 0.6795 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.68183 to 0.67954, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 48/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.7098 - acc: 0.6275 - val_loss: 0.6781 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.67954 to 0.67814, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 49/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.7067 - acc: 0.6263 - val_loss: 0.6764 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.67814 to 0.67638, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 50/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.7067 - acc: 0.6271 - val_loss: 0.6749 - val_acc: 0.7009\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.67638 to 0.67490, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 51/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.7062 - acc: 0.6278 - val_loss: 0.6735 - val_acc: 0.7007\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.67490 to 0.67348, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 52/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.7020 - acc: 0.6316 - val_loss: 0.6716 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.67348 to 0.67163, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 53/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.7022 - acc: 0.6316 - val_loss: 0.6704 - val_acc: 0.7009\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.67163 to 0.67036, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 54/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.7012 - acc: 0.6347 - val_loss: 0.6689 - val_acc: 0.7007\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.67036 to 0.66894, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 55/100\n",
      "45858/45858 [==============================] - 4s 88us/step - loss: 0.7001 - acc: 0.6328 - val_loss: 0.6677 - val_acc: 0.7018\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.66894 to 0.66773, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 56/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.6986 - acc: 0.6321 - val_loss: 0.6665 - val_acc: 0.7021\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.66773 to 0.66651, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 57/100\n",
      "45858/45858 [==============================] - 4s 87us/step - loss: 0.7033 - acc: 0.6312 - val_loss: 0.6660 - val_acc: 0.7022\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.66651 to 0.66603, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 58/100\n",
      "45858/45858 [==============================] - 4s 89us/step - loss: 0.6981 - acc: 0.6359 - val_loss: 0.6648 - val_acc: 0.7019\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.66603 to 0.66481, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 59/100\n",
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.6970 - acc: 0.6353 - val_loss: 0.6635 - val_acc: 0.7022\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.66481 to 0.66347, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 60/100\n",
      "45858/45858 [==============================] - 4s 87us/step - loss: 0.6969 - acc: 0.6365 - val_loss: 0.6622 - val_acc: 0.7027\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.66347 to 0.66224, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 61/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.6981 - acc: 0.6351 - val_loss: 0.6613 - val_acc: 0.7030\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.66224 to 0.66133, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 62/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.6938 - acc: 0.6362 - val_loss: 0.6601 - val_acc: 0.7034\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.66133 to 0.66014, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 63/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.6944 - acc: 0.6411 - val_loss: 0.6591 - val_acc: 0.7041\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.66014 to 0.65910, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 64/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.6958 - acc: 0.6381 - val_loss: 0.6582 - val_acc: 0.7049\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.65910 to 0.65820, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6921 - acc: 0.6401 - val_loss: 0.6571 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.65820 to 0.65713, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 66/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6947 - acc: 0.6377 - val_loss: 0.6566 - val_acc: 0.7057\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.65713 to 0.65658, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 67/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6924 - acc: 0.6364 - val_loss: 0.6560 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.65658 to 0.65599, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 68/100\n",
      "45858/45858 [==============================] - 4s 86us/step - loss: 0.6912 - acc: 0.6410 - val_loss: 0.6548 - val_acc: 0.7060\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.65599 to 0.65475, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 69/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.6923 - acc: 0.6402 - val_loss: 0.6541 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.65475 to 0.65412, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 70/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.6902 - acc: 0.6417 - val_loss: 0.6534 - val_acc: 0.7065\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.65412 to 0.65339, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 71/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.6888 - acc: 0.6431 - val_loss: 0.6526 - val_acc: 0.7064\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.65339 to 0.65262, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 72/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6888 - acc: 0.6444 - val_loss: 0.6519 - val_acc: 0.7069\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.65262 to 0.65190, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 73/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6883 - acc: 0.6413 - val_loss: 0.6510 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.65190 to 0.65096, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 74/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.6864 - acc: 0.6436 - val_loss: 0.6502 - val_acc: 0.7072\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.65096 to 0.65024, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 75/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6879 - acc: 0.6437 - val_loss: 0.6497 - val_acc: 0.7066\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.65024 to 0.64966, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 76/100\n",
      "45858/45858 [==============================] - 4s 80us/step - loss: 0.6870 - acc: 0.6437 - val_loss: 0.6488 - val_acc: 0.7068\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.64966 to 0.64880, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 77/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.6864 - acc: 0.6431 - val_loss: 0.6483 - val_acc: 0.7071\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.64880 to 0.64827, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 78/100\n",
      "45858/45858 [==============================] - 4s 89us/step - loss: 0.6874 - acc: 0.6434 - val_loss: 0.6476 - val_acc: 0.7075\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.64827 to 0.64763, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 79/100\n",
      "45858/45858 [==============================] - 4s 77us/step - loss: 0.6858 - acc: 0.6427 - val_loss: 0.6470 - val_acc: 0.7077\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.64763 to 0.64703, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 80/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.6843 - acc: 0.6451 - val_loss: 0.6461 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.64703 to 0.64606, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 81/100\n",
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.6824 - acc: 0.6452 - val_loss: 0.6450 - val_acc: 0.7087\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.64606 to 0.64496, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 82/100\n",
      "45858/45858 [==============================] - 4s 84us/step - loss: 0.6841 - acc: 0.6456 - val_loss: 0.6443 - val_acc: 0.7087\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.64496 to 0.64435, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 83/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.6823 - acc: 0.6467 - val_loss: 0.6435 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.64435 to 0.64352, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 84/100\n",
      "45858/45858 [==============================] - 4s 85us/step - loss: 0.6825 - acc: 0.6448 - val_loss: 0.6431 - val_acc: 0.7101\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.64352 to 0.64312, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 85/100\n",
      "45858/45858 [==============================] - 4s 89us/step - loss: 0.6838 - acc: 0.6416 - val_loss: 0.6424 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.64312 to 0.64242, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 86/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.6810 - acc: 0.6480 - val_loss: 0.6413 - val_acc: 0.7104\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.64242 to 0.64132, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 87/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.6774 - acc: 0.6481 - val_loss: 0.6404 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.64132 to 0.64038, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 88/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.6787 - acc: 0.6488 - val_loss: 0.6394 - val_acc: 0.7110\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.64038 to 0.63944, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 89/100\n",
      "45858/45858 [==============================] - 4s 88us/step - loss: 0.6781 - acc: 0.6477 - val_loss: 0.6385 - val_acc: 0.7114\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.63944 to 0.63852, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 90/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.6775 - acc: 0.6466 - val_loss: 0.6378 - val_acc: 0.7125\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.63852 to 0.63780, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 91/100\n",
      "45858/45858 [==============================] - 4s 86us/step - loss: 0.6758 - acc: 0.6503 - val_loss: 0.6370 - val_acc: 0.7124\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.63780 to 0.63698, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 92/100\n",
      "45858/45858 [==============================] - 4s 85us/step - loss: 0.6761 - acc: 0.6477 - val_loss: 0.6364 - val_acc: 0.7132\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.63698 to 0.63639, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 93/100\n",
      "45858/45858 [==============================] - 4s 82us/step - loss: 0.6775 - acc: 0.6475 - val_loss: 0.6357 - val_acc: 0.7138\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.63639 to 0.63569, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 94/100\n",
      "45858/45858 [==============================] - 4s 83us/step - loss: 0.6738 - acc: 0.6509 - val_loss: 0.6347 - val_acc: 0.7142\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.63569 to 0.63472, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 95/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.6742 - acc: 0.6484 - val_loss: 0.6343 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.63472 to 0.63428, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 96/100\n",
      "45858/45858 [==============================] - 4s 81us/step - loss: 0.6753 - acc: 0.6490 - val_loss: 0.6335 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.63428 to 0.63355, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.6722 - acc: 0.6530 - val_loss: 0.6326 - val_acc: 0.7149\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.63355 to 0.63262, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 98/100\n",
      "45858/45858 [==============================] - 4s 79us/step - loss: 0.6728 - acc: 0.6510 - val_loss: 0.6320 - val_acc: 0.7155\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.63262 to 0.63198, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 99/100\n",
      "24850/45858 [===============>..............] - ETA: 1s - loss: 0.6740 - acc: 0.6505"
     ]
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nullsonly.fit(Xtrain_nullsonly, Ytrain_nullsonly, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_nonulls.load_weights('saved_models/weights.best.mlp_nullsonly.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 9.7496%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_nonulls.evaluate(Xtest_nonulls, Ytest_nonulls, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.3032.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_nonulls = mlp_nonulls.predict(Xtest_nonulls)\n",
    "mlp_nonulls_ROC = roc_auc_score(Ytest_nonulls, Ypred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nonulls_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "3. https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9\n",
    "4. https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\n",
    "5. https://keras.io/getting-started/sequential-model-guide/\n",
    "6. Udacity Machine Learning Engineer Nanodegree Program, Semester 2, Brian Campbell - Dog Breed Classifier Project\n",
    "7. https://keras.io/getting-started/sequential-model-guide/\n",
    "8. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.size.html\n",
    "9. https://keras.io/models/sequential/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
