{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: The Efficacy of Multilayer Perceptron Algorithms in Predicting Bankruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><a href=\"#introduction\">INTRODUCTION</a></li>\n",
    "<li><a href=\"#Benchmark Logistic Regression\">Benchmark Logistic Regression</a></li>\n",
    "<li><a href=\"#MLP Model\">MLP Model</a></li>\n",
    "<li><a href=\"#assess\">Data Assessment</a></li>\n",
    "<li><a href=\"#cleaning\">Data Cleaning</a></li>\n",
    "<li><a href=\"#target\">Separation of Target Variables</a></li>\n",
    "<li><a href=\"#no nulls\">No Nulls Data</a></li>\n",
    "<li><a href=\"#one-hot null\">Creation of One-hot Null Variable</a></li>\n",
    "<li><a href=\"#Creation of Sum Null Variables\">Creation of Sum Null Variables</a></li>\n",
    "<li><a href=\"#PIVOT\">PIVOT</a></li>\n",
    "<li><a href=\"#Data Reorganization\">Data Reorganization</a></li>\n",
    "<li><a href=\"#Data Exploration - Descriptive Statistics\">Data Exploration - Descriptive Statistics</a></li>\n",
    "<li><a href=\"#Exploratory Visualization\">Exploratory Visualization</a></li>\n",
    "<li><a href=\"#Preprocessing\">Preprocessing</a></li>\n",
    "<li><a href=\"#Benchmark: Logistic Regression\">Benchmark: Logistic Regression</a></li>\n",
    "<li><a href=\"#originalMLP\">Origial MLP</a></li>\n",
    "<li><a href=\"#conclusion\">Conclusion</a></li> \n",
    "<li><a href=\"#references\">References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Benchmark Logistic Regression'></a>\n",
    "## Benchmark Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary libraries for the logistic regression models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This imports the AUC score for scoring the models.\n",
    "# This comes from Reference 27 in References.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# These are libraries that will be needed to organize data,\n",
    "# graph data, and change the working directory.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the no_nulls X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nonulls = pd.read_csv('no-peaking/Xtrain_nonulls.csv')\n",
    "Xtrain_nonulls = np.array(Xtrain_nonulls)\n",
    "Xtest_nonulls = pd.read_csv('no-peaking/Xtest_nonulls.csv')\n",
    "Xtest_nonulls = np.array(Xtest_nonulls)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nonulls = pd.read_csv('no-peaking/Ytrain_nonulls.csv')\n",
    "Ytrain_nonulls = np.array(Ytrain_nonulls)\n",
    "Ytrain_nonulls = Ytrain_nonulls.ravel() \n",
    "Ytest_nonulls = pd.read_csv('no-peaking/Ytest_nonulls.csv')\n",
    "Ytest_nonulls = np.array(Ytest_nonulls)\n",
    "Ytest_nonulls = Ytest_nonulls.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Hold-out Cross-Validation on just the training set\n",
    "K-fold Cross-Validation is a standard method for preventing a logistic regression model from overfitting. However, because the testing set, the fifth year of the dataset, is arbitrarily chosen (not random), K-fold Cross-Validation cannot be applied to the dataset (Reference 3). K-fold Cross-Validation would corrupt the testing set with data leakage considering that the dataset is a time-series set (Reference 3). To prevent data leakage, Hold-out cross-Validation will only be applied to the training set (References 3 & 4). Hold-out Cross-Validation takes a percentage of the training set as a validation set to test the accuracy of the model during the training stage. This method of cross validation, like all methods, is used to prevent the overfitting of a model and poor accuracy performance when applying the testing data to the fitted model (Reference 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Nulls Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import train_test_split from sklearn.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the No Nulls dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nonulls, Xval_nonulls, Ytrain_nonulls, Yval_nonulls = train_test_split(\n",
    "                    Xtrain_nonulls, Ytrain_nonulls, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8506022247116249"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the No Nulls Dataset.\n",
    "log_nonulls = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_nonulls model with the training data.\n",
    "log_nonulls.fit(Xtrain_nonulls,Ytrain_nonulls)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nonulls = log_nonulls.predict(Xval_nonulls)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nonulls = roc_auc_score(Yval_nonulls, yval_pred_nonulls)\n",
    "VAL_auc_nonulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.3163.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nonulls = log_nonulls.predict(Xtest_nonulls)\n",
    "TEST_auc_nonulls = roc_auc_score(Ytest_nonulls, ytest_pred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Nulls only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This loads the Nulls only training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nullsonly = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_nullsonly = Xtrain_nullsonly.drop(\n",
    "    Xtrain_nullsonly.columns[64], axis=1)\n",
    "Xtrain_nullsonly = np.array(Xtrain_nullsonly)\n",
    "Xtest_nullsonly = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_nullsonly = Xtest_nullsonly.drop(\n",
    "    Xtest_nullsonly.columns[64], axis=1)\n",
    "Xtest_nullsonly = np.array(Xtest_nullsonly)\n",
    "\n",
    "# This loads the Nulls only Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nullsonly = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_nullsonly = np.array(Ytrain_nullsonly)\n",
    "Ytrain_nullsonly = Ytrain_nullsonly.ravel() \n",
    "Ytest_nullsonly = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_nullsonly = np.array(Ytest_nullsonly)\n",
    "Ytest_nullsonly = Ytest_nullsonly.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls Only Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nullsonly, Xval_nullsonly, Ytrain_nullsonly, Yval_nullsonly = train_test_split(\n",
    "                    Xtrain_nullsonly, Ytrain_nullsonly, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7350826303765834"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the Nulls Only Dataset.\n",
    "log_nullsonly = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_nullsonly.fit(Xtrain_nullsonly,Ytrain_nullsonly)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nullsonly = log_nullsonly.predict(Xval_nullsonly)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nullsonly = roc_auc_score(Yval_nullsonly, yval_pred_nullsonly)\n",
    "VAL_auc_nullsonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 68.29 %\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nullsonly = log_nullsonly.predict(Xtest_nullsonly)\n",
    "TEST_auc_nullsonly = roc_auc_score(Ytest_nullsonly, ytest_pred_nullsonly)\n",
    "TEST_auc_nullsonly = TEST_auc_nullsonly * 100\n",
    "print(\"The AUC score for the model is %.2f\" % TEST_auc_nullsonly, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the one hot X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_onehot = pd.read_csv('no-peaking/Xtrain_onehot.csv')\n",
    "Xtrain_onehot = np.array(Xtrain_onehot)\n",
    "Xtest_onehot = pd.read_csv('no-peaking/Xtest_onehot.csv')\n",
    "Xtest_onehot = np.array(Xtest_onehot)\n",
    "\n",
    "# This loads the one hot Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_onehot = pd.read_csv('no-peaking/Ytrain_onehot.csv')\n",
    "Ytrain_onehot = np.array(Ytrain_onehot)\n",
    "Ytrain_onehot = Ytrain_onehot.ravel() \n",
    "Ytest_onehot = pd.read_csv('no-peaking/Ytest_onehot.csv')\n",
    "Ytest_onehot = np.array(Ytest_onehot)\n",
    "Ytest_onehot = Ytest_onehot.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the One Hot dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_onehot, Xval_onehot, Ytrain_onehot, Yval_onehot = train_test_split(\n",
    "                    Xtrain_onehot, Ytrain_onehot, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8647068328667264"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_onehot = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_onehot.fit(Xtrain_onehot,Ytrain_onehot)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_onehot = log_onehot.predict(Xval_onehot)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_onehot = roc_auc_score(Yval_onehot, yval_pred_onehot)\n",
    "VAL_auc_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6406.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_onehot = log_onehot.predict(Xtest_onehot)\n",
    "TEST_auc_onehot = roc_auc_score(Ytest_onehot, ytest_pred_onehot)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the SUM X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_sum = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_sum = np.array(Xtrain_sum)\n",
    "Xtest_sum = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_sum = np.array(Xtest_sum)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_sum = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_sum = np.array(Ytrain_sum)\n",
    "Ytrain_sum = Ytrain_sum.ravel() \n",
    "Ytest_sum = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_sum = np.array(Ytest_sum)\n",
    "Ytest_sum = Ytest_sum.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_sum, Xval_sum, Ytrain_sum, Yval_sum = train_test_split(\n",
    "                    Xtrain_sum, Ytrain_sum, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7437902237728553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_sum = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_sum.fit(Xtrain_sum,Ytrain_sum)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.c\n",
    "yval_pred_sum = log_sum.predict(Xval_sum)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_sum = roc_auc_score(Yval_sum, yval_pred_sum)\n",
    "VAL_auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6606.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_sum = log_sum.predict(Xtest_sum)\n",
    "TEST_auc_sum = roc_auc_score(Ytest_sum, ytest_pred_sum)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MLP Model'></a>\n",
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This imports the necessary libraries for the MLP.\n",
    "\n",
    "# This imports the sequential model, the layers,\n",
    "# the SGD optimizer, the regularizers from keras.\n",
    "# This comes from Reference 5 in Referenes.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "# This imports checkpointer, which records the best weights\n",
    "# for the algorithm.\n",
    "# This comes from Reference 6 in References.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drop_rate, l2_factor, first_dense, second_dense,\n",
    "                third_dense, hidden_act, out_act, x):\n",
    "    dim_int = int(np.size(x,1))\n",
    "    # This defines the model as a sequential model.\n",
    "    # This comes from References 1 in References.\n",
    "    model = Sequential()\n",
    "\n",
    "    # This is the input layer.\n",
    "    # This comes from References 1 & 3 in References.\n",
    "    model.add(Dense(first_dense, activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor),\n",
    "        input_dim = dim_int))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the first hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(second_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # This creates the second hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(third_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the output layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(1, activation=out_act))\n",
    "    # This returns the model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD()\n",
    "\n",
    "# This builds the original MLP model for year 1.\n",
    "# using Mattson and Steinart's original hyperparameters.\n",
    "mlp_nonulls = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Null data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nonulls.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 15 & 16 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nonulls.hdf5',\n",
    "                              monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "                            mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 11018 samples, validate on 2755 samples\n",
      "Epoch 1/100\n",
      "11018/11018 [==============================] - 2s 172us/step - loss: 0.8054 - acc: 0.5232 - val_loss: 0.7487 - val_acc: 0.6868\n",
      "Epoch 2/100\n",
      " 3200/11018 [=======>......................] - ETA: 0s - loss: 0.7870 - acc: 0.5450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_accuracy available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.7730 - acc: 0.5632 - val_loss: 0.7417 - val_acc: 0.7082\n",
      "Epoch 3/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.7575 - acc: 0.5809 - val_loss: 0.7333 - val_acc: 0.7209\n",
      "Epoch 4/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.7544 - acc: 0.5948 - val_loss: 0.7246 - val_acc: 0.7216\n",
      "Epoch 5/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.7315 - acc: 0.6100 - val_loss: 0.7103 - val_acc: 0.7372\n",
      "Epoch 6/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.7297 - acc: 0.6200 - val_loss: 0.6999 - val_acc: 0.7419\n",
      "Epoch 7/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.7213 - acc: 0.6286 - val_loss: 0.6890 - val_acc: 0.7448\n",
      "Epoch 8/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.7186 - acc: 0.6338 - val_loss: 0.6812 - val_acc: 0.7463\n",
      "Epoch 9/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.7084 - acc: 0.6462 - val_loss: 0.6712 - val_acc: 0.7492\n",
      "Epoch 10/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.6902 - acc: 0.6567 - val_loss: 0.6552 - val_acc: 0.7546\n",
      "Epoch 11/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.6853 - acc: 0.6681 - val_loss: 0.6432 - val_acc: 0.7615\n",
      "Epoch 12/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.6781 - acc: 0.6710 - val_loss: 0.6320 - val_acc: 0.7706\n",
      "Epoch 13/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.6687 - acc: 0.6744 - val_loss: 0.6206 - val_acc: 0.7688\n",
      "Epoch 14/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.6631 - acc: 0.6898 - val_loss: 0.6111 - val_acc: 0.7691\n",
      "Epoch 15/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.6532 - acc: 0.6948 - val_loss: 0.6020 - val_acc: 0.7739\n",
      "Epoch 16/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.6436 - acc: 0.7017 - val_loss: 0.5878 - val_acc: 0.7728\n",
      "Epoch 17/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.6407 - acc: 0.6999 - val_loss: 0.5801 - val_acc: 0.7768\n",
      "Epoch 18/100\n",
      "11018/11018 [==============================] - 1s 61us/step - loss: 0.6290 - acc: 0.7139 - val_loss: 0.5698 - val_acc: 0.7800\n",
      "Epoch 19/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.6161 - acc: 0.7197 - val_loss: 0.5547 - val_acc: 0.7855\n",
      "Epoch 20/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.6075 - acc: 0.7268 - val_loss: 0.5445 - val_acc: 0.7917\n",
      "Epoch 21/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.5972 - acc: 0.7362 - val_loss: 0.5329 - val_acc: 0.7975\n",
      "Epoch 22/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.5872 - acc: 0.7392 - val_loss: 0.5241 - val_acc: 0.7985\n",
      "Epoch 23/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.5815 - acc: 0.7435 - val_loss: 0.5134 - val_acc: 0.8033\n",
      "Epoch 24/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.5672 - acc: 0.7501 - val_loss: 0.5023 - val_acc: 0.8076\n",
      "Epoch 25/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.5607 - acc: 0.7465 - val_loss: 0.4941 - val_acc: 0.8076\n",
      "Epoch 26/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.5610 - acc: 0.7479 - val_loss: 0.4850 - val_acc: 0.8116\n",
      "Epoch 27/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.5530 - acc: 0.7510 - val_loss: 0.4775 - val_acc: 0.8163\n",
      "Epoch 28/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.5423 - acc: 0.7604 - val_loss: 0.4671 - val_acc: 0.8185\n",
      "Epoch 29/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.5362 - acc: 0.7598 - val_loss: 0.4621 - val_acc: 0.8192\n",
      "Epoch 30/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.5252 - acc: 0.7636 - val_loss: 0.4552 - val_acc: 0.8196\n",
      "Epoch 31/100\n",
      "11018/11018 [==============================] - 1s 59us/step - loss: 0.5259 - acc: 0.7669 - val_loss: 0.4464 - val_acc: 0.8243\n",
      "Epoch 32/100\n",
      "11018/11018 [==============================] - 1s 60us/step - loss: 0.5291 - acc: 0.7680 - val_loss: 0.4407 - val_acc: 0.8367\n",
      "Epoch 33/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.5111 - acc: 0.7749 - val_loss: 0.4334 - val_acc: 0.8356\n",
      "Epoch 34/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.5069 - acc: 0.7788 - val_loss: 0.4280 - val_acc: 0.8377\n",
      "Epoch 35/100\n",
      "11018/11018 [==============================] - 1s 50us/step - loss: 0.5061 - acc: 0.7771 - val_loss: 0.4215 - val_acc: 0.8472\n",
      "Epoch 36/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.5036 - acc: 0.7815 - val_loss: 0.4164 - val_acc: 0.8505\n",
      "Epoch 37/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4993 - acc: 0.7832 - val_loss: 0.4124 - val_acc: 0.8530\n",
      "Epoch 38/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4959 - acc: 0.7853 - val_loss: 0.4083 - val_acc: 0.8574\n",
      "Epoch 39/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4809 - acc: 0.7893 - val_loss: 0.4000 - val_acc: 0.8610\n",
      "Epoch 40/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.4781 - acc: 0.7942 - val_loss: 0.3961 - val_acc: 0.8621\n",
      "Epoch 41/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.4758 - acc: 0.7943 - val_loss: 0.3894 - val_acc: 0.8642\n",
      "Epoch 42/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.4710 - acc: 0.7972 - val_loss: 0.3858 - val_acc: 0.8679\n",
      "Epoch 43/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.4665 - acc: 0.7971 - val_loss: 0.3826 - val_acc: 0.8646\n",
      "Epoch 44/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.4619 - acc: 0.8021 - val_loss: 0.3795 - val_acc: 0.8726\n",
      "Epoch 45/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4695 - acc: 0.7975 - val_loss: 0.3804 - val_acc: 0.8664\n",
      "Epoch 46/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.4534 - acc: 0.8089 - val_loss: 0.3730 - val_acc: 0.8715\n",
      "Epoch 47/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4615 - acc: 0.7994 - val_loss: 0.3735 - val_acc: 0.8733\n",
      "Epoch 48/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4565 - acc: 0.8032 - val_loss: 0.3696 - val_acc: 0.8751\n",
      "Epoch 49/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.4496 - acc: 0.8091 - val_loss: 0.3676 - val_acc: 0.8711\n",
      "Epoch 50/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4488 - acc: 0.8040 - val_loss: 0.3645 - val_acc: 0.8777\n",
      "Epoch 51/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.4452 - acc: 0.8111 - val_loss: 0.3580 - val_acc: 0.8795\n",
      "Epoch 52/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.4439 - acc: 0.8107 - val_loss: 0.3630 - val_acc: 0.8740\n",
      "Epoch 53/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.4422 - acc: 0.8101 - val_loss: 0.3569 - val_acc: 0.8857\n",
      "Epoch 54/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.4378 - acc: 0.8151 - val_loss: 0.3542 - val_acc: 0.8795\n",
      "Epoch 55/100\n",
      "11018/11018 [==============================] - 1s 59us/step - loss: 0.4320 - acc: 0.8167 - val_loss: 0.3520 - val_acc: 0.8864\n",
      "Epoch 56/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.4332 - acc: 0.8161 - val_loss: 0.3498 - val_acc: 0.8857\n",
      "Epoch 57/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.4292 - acc: 0.8134 - val_loss: 0.3458 - val_acc: 0.8857\n",
      "Epoch 58/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.4301 - acc: 0.8153 - val_loss: 0.3487 - val_acc: 0.8784\n",
      "Epoch 59/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.4278 - acc: 0.8177 - val_loss: 0.3496 - val_acc: 0.8788\n",
      "Epoch 60/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.4300 - acc: 0.8177 - val_loss: 0.3463 - val_acc: 0.8828\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018/11018 [==============================] - 1s 50us/step - loss: 0.4298 - acc: 0.8129 - val_loss: 0.3424 - val_acc: 0.8846\n",
      "Epoch 62/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.4255 - acc: 0.8227 - val_loss: 0.3421 - val_acc: 0.8875\n",
      "Epoch 63/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4231 - acc: 0.8189 - val_loss: 0.3372 - val_acc: 0.8907\n",
      "Epoch 64/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4148 - acc: 0.8227 - val_loss: 0.3309 - val_acc: 0.8984\n",
      "Epoch 65/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4123 - acc: 0.8224 - val_loss: 0.3362 - val_acc: 0.8864\n",
      "Epoch 66/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4158 - acc: 0.8221 - val_loss: 0.3358 - val_acc: 0.8864\n",
      "Epoch 67/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.4074 - acc: 0.8300 - val_loss: 0.3319 - val_acc: 0.8893\n",
      "Epoch 68/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4072 - acc: 0.8244 - val_loss: 0.3237 - val_acc: 0.9013\n",
      "Epoch 69/100\n",
      "11018/11018 [==============================] - 1s 50us/step - loss: 0.4093 - acc: 0.8255 - val_loss: 0.3265 - val_acc: 0.8969\n",
      "Epoch 70/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4117 - acc: 0.8209 - val_loss: 0.3320 - val_acc: 0.8911\n",
      "Epoch 71/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4118 - acc: 0.8206 - val_loss: 0.3313 - val_acc: 0.8915\n",
      "Epoch 72/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4125 - acc: 0.8245 - val_loss: 0.3249 - val_acc: 0.8900\n",
      "Epoch 73/100\n",
      "11018/11018 [==============================] - ETA: 0s - loss: 0.4051 - acc: 0.828 - 1s 50us/step - loss: 0.4076 - acc: 0.8266 - val_loss: 0.3230 - val_acc: 0.8995\n",
      "Epoch 74/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4030 - acc: 0.8265 - val_loss: 0.3173 - val_acc: 0.9016\n",
      "Epoch 75/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4007 - acc: 0.8297 - val_loss: 0.3191 - val_acc: 0.8966\n",
      "Epoch 76/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.4003 - acc: 0.8293 - val_loss: 0.3195 - val_acc: 0.8944\n",
      "Epoch 77/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.3973 - acc: 0.8306 - val_loss: 0.3166 - val_acc: 0.8973\n",
      "Epoch 78/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.3962 - acc: 0.8342 - val_loss: 0.3100 - val_acc: 0.9107\n",
      "Epoch 79/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3991 - acc: 0.8275 - val_loss: 0.3155 - val_acc: 0.8991\n",
      "Epoch 80/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.4037 - acc: 0.8253 - val_loss: 0.3073 - val_acc: 0.9064\n",
      "Epoch 81/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3938 - acc: 0.8315 - val_loss: 0.3076 - val_acc: 0.9038\n",
      "Epoch 82/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.3906 - acc: 0.8331 - val_loss: 0.3071 - val_acc: 0.9005\n",
      "Epoch 83/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3907 - acc: 0.8335 - val_loss: 0.3067 - val_acc: 0.9027\n",
      "Epoch 84/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.3939 - acc: 0.8337 - val_loss: 0.3026 - val_acc: 0.9078\n",
      "Epoch 85/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3879 - acc: 0.8319 - val_loss: 0.3063 - val_acc: 0.9053\n",
      "Epoch 86/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3930 - acc: 0.8315 - val_loss: 0.3008 - val_acc: 0.9056\n",
      "Epoch 87/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.3945 - acc: 0.8335 - val_loss: 0.3035 - val_acc: 0.9038\n",
      "Epoch 88/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.3862 - acc: 0.8337 - val_loss: 0.3030 - val_acc: 0.9067\n",
      "Epoch 89/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.3878 - acc: 0.8340 - val_loss: 0.3010 - val_acc: 0.9053\n",
      "Epoch 90/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.3914 - acc: 0.8332 - val_loss: 0.2968 - val_acc: 0.9140\n",
      "Epoch 91/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.3847 - acc: 0.8384 - val_loss: 0.3120 - val_acc: 0.8973\n",
      "Epoch 92/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.3817 - acc: 0.8378 - val_loss: 0.2951 - val_acc: 0.9064\n",
      "Epoch 93/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3827 - acc: 0.8364 - val_loss: 0.2957 - val_acc: 0.9089\n",
      "Epoch 94/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.3822 - acc: 0.8394 - val_loss: 0.2943 - val_acc: 0.9114\n",
      "Epoch 95/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.3845 - acc: 0.8383 - val_loss: 0.2988 - val_acc: 0.9016\n",
      "Epoch 96/100\n",
      "11018/11018 [==============================] - 1s 50us/step - loss: 0.3810 - acc: 0.8371 - val_loss: 0.2946 - val_acc: 0.9089\n",
      "Epoch 97/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.3790 - acc: 0.8393 - val_loss: 0.2913 - val_acc: 0.9238\n",
      "Epoch 98/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.3839 - acc: 0.8367 - val_loss: 0.2953 - val_acc: 0.9034\n",
      "Epoch 99/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.3819 - acc: 0.8386 - val_loss: 0.2947 - val_acc: 0.9089\n",
      "Epoch 100/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.3732 - acc: 0.8404 - val_loss: 0.2908 - val_acc: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef91439d68>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nonulls.fit(Xtrain_nonulls, Ytrain_nonulls, validation_split=0.2,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "3. https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9\n",
    "4. https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\n",
    "5. https://keras.io/getting-started/sequential-model-guide/\n",
    "6. Udacity Machine Learning Engineer Nanodegree Program, Semester 2, Brian Campbell - Dog Breed Classifier Project\n",
    "7. https://keras.io/getting-started/sequential-model-guide/\n",
    "8. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.size.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
