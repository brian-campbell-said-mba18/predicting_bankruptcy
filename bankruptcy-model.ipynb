{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: The Efficacy of Multilayer Perceptron Algorithms in Predicting Bankruptcy, The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><a href=\"#DISCLOSURE\">DISCLOSURE</a></li>\n",
    "<li><a href=\"#introduction\">INTRODUCTION</a></li>\n",
    "<li><a href=\"#Benchmark Logistic Regression\">Benchmark Logistic Regression</a></li>\n",
    "<li><a href=\"#MLP Model\">MLP Model</a></li>\n",
    "<li><a href=\"#Conclusion\">Conclusion</a></li>\n",
    "<li><a href=\"#references\">References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DISCLOSURE'></a>\n",
    "## DISCLOSURE\n",
    "Please note, that this notebook, \"bankruptcy-model.ipynb\" ONLY COVERS the modeling of the data. Simply, the jupyter notebook, \"bankruptcy-data.ipynb\" was too big to include the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## INTRODUCTION\n",
    "This part of the analyis loads the preprocessed data: a dataset with no null values (\"No Nulls\"), a dataset with imputed null values (\"Nulls Only'), a dataset that has a dummy variable for each feature variable that signals whether a value in the feature column is null (\"One Hot\"), and a dataset with imputed null values and a variable that counts the number of null values in a given row of data (\"Sum\"). Logistic regression models are applied to these four datasets as the benchmark models. Next, Multi Layer Perceptron (MLP) models are made for the four datasets. The AUC scores are calculated for all the models. The conclusion compares the AUC scores of the MLP models to the Logistic Regression models, and offers additional insight into these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Benchmark Logistic Regression'></a>\n",
    "## Benchmark Logistic Regression\n",
    "Logistic regression servers as a good benchmark model for predicting bankruptcies. It's a basic model that analyzes a binary target variable (bankrupt vs not bankrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary libraries for the logistic regression models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This imports the AUC score for scoring the models.\n",
    "# This comes from Reference 27 in References.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# These are libraries that will be needed to organize data,\n",
    "# graph data, and change the working directory.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the no_nulls X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nonulls = pd.read_csv('no-peaking/Xtrain_nonulls.csv')\n",
    "Xtrain_nonulls = np.array(Xtrain_nonulls)\n",
    "Xtest_nonulls = pd.read_csv('no-peaking/Xtest_nonulls.csv')\n",
    "Xtest_nonulls = np.array(Xtest_nonulls)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nonulls = pd.read_csv('no-peaking/Ytrain_nonulls.csv')\n",
    "Ytrain_nonulls = np.array(Ytrain_nonulls)\n",
    "Ytrain_nonulls = Ytrain_nonulls.ravel() \n",
    "Ytest_nonulls = pd.read_csv('no-peaking/Ytest_nonulls.csv')\n",
    "Ytest_nonulls = np.array(Ytest_nonulls)\n",
    "Ytest_nonulls = Ytest_nonulls.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Hold-out Cross-Validation on just the training set\n",
    "K-fold Cross-Validation is a standard method for preventing a logistic regression model from overfitting. However, because the testing set, the fifth year of the dataset, is arbitrarily chosen (not random), K-fold Cross-Validation cannot be applied to the dataset (Reference 3). K-fold Cross-Validation would corrupt the testing set with data leakage considering that the dataset is a time-series set (Reference 3). To prevent data leakage, Hold-out cross-Validation will only be applied to the training set (References 3 & 4). Hold-out Cross-Validation takes a percentage of the training set as a validation set to test the accuracy of the model during the training stage. This method of cross validation, like all methods, is used to prevent the overfitting of a model and poor accuracy performance when applying the testing data to the fitted model (Reference 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Nulls Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import train_test_split from sklearn.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the No Nulls dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nonulls, Xval_nonulls, Ytrain_nonulls, Yval_nonulls = train_test_split(\n",
    "                    Xtrain_nonulls, Ytrain_nonulls, test_size = 0.2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8493093121316346"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the No Nulls Dataset.\n",
    "log_nonulls = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_nonulls model with the training data.\n",
    "log_nonulls.fit(Xtrain_nonulls,Ytrain_nonulls)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nonulls = log_nonulls.predict(Xval_nonulls)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nonulls = roc_auc_score(Yval_nonulls, yval_pred_nonulls)\n",
    "VAL_auc_nonulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.3675.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nonulls = log_nonulls.predict(Xtest_nonulls)\n",
    "TEST_auc_nonulls = roc_auc_score(Ytest_nonulls, ytest_pred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Nulls only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the Nulls only training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nullsonly = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_nullsonly = Xtrain_nullsonly.drop(\n",
    "    Xtrain_nullsonly.columns[64], axis=1)\n",
    "Xtrain_nullsonly = np.array(Xtrain_nullsonly)\n",
    "Xtest_nullsonly = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_nullsonly = Xtest_nullsonly.drop(\n",
    "    Xtest_nullsonly.columns[64], axis=1)\n",
    "Xtest_nullsonly = np.array(Xtest_nullsonly)\n",
    "\n",
    "# This loads the Nulls only Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nullsonly = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_nullsonly = np.array(Ytrain_nullsonly)\n",
    "Ytrain_nullsonly = Ytrain_nullsonly.ravel() \n",
    "Ytest_nullsonly = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_nullsonly = np.array(Ytest_nullsonly)\n",
    "Ytest_nullsonly = Ytest_nullsonly.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls Only Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Nulls Only dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nullsonly, Xval_nullsonly, Ytrain_nullsonly, Yval_nullsonly = train_test_split(\n",
    "                    Xtrain_nullsonly, Ytrain_nullsonly, test_size = 0.2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7335101563282769"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the Nulls Only Dataset.\n",
    "log_nullsonly = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the Log_nullsonly model with the training data.\n",
    "log_nullsonly.fit(Xtrain_nullsonly,Ytrain_nullsonly)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nullsonly = log_nullsonly.predict(Xval_nullsonly)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nullsonly = roc_auc_score(Yval_nullsonly, yval_pred_nullsonly)\n",
    "VAL_auc_nullsonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6903.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nullsonly = log_nullsonly.predict(Xtest_nullsonly)\n",
    "TEST_auc_nullsonly = roc_auc_score(Ytest_nullsonly, ytest_pred_nullsonly)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_nullsonly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the one hot X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_onehot = pd.read_csv('no-peaking/Xtrain_onehot.csv')\n",
    "Xtrain_onehot = np.array(Xtrain_onehot)\n",
    "Xtest_onehot = pd.read_csv('no-peaking/Xtest_onehot.csv')\n",
    "Xtest_onehot = np.array(Xtest_onehot)\n",
    "\n",
    "# This loads the one hot Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_onehot = pd.read_csv('no-peaking/Ytrain_onehot.csv')\n",
    "Ytrain_onehot = np.array(Ytrain_onehot)\n",
    "Ytrain_onehot = Ytrain_onehot.ravel() \n",
    "Ytest_onehot = pd.read_csv('no-peaking/Ytest_onehot.csv')\n",
    "Ytest_onehot = np.array(Ytest_onehot)\n",
    "Ytest_onehot = Ytest_onehot.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the One Hot dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_onehot, Xval_onehot, Ytrain_onehot, Yval_onehot = train_test_split(\n",
    "                    Xtrain_onehot, Ytrain_onehot, test_size = 0.2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.866889447207181"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_onehot = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_onehot.fit(Xtrain_onehot,Ytrain_onehot)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_onehot = log_onehot.predict(Xval_onehot)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_onehot = roc_auc_score(Yval_onehot, yval_pred_onehot)\n",
    "VAL_auc_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6872.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_onehot = log_onehot.predict(Xtest_onehot)\n",
    "TEST_auc_onehot = roc_auc_score(Ytest_onehot, ytest_pred_onehot)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the SUM X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_sum = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_sum = np.array(Xtrain_sum)\n",
    "Xtest_sum = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_sum = np.array(Xtest_sum)\n",
    "\n",
    "# This loads the SUM Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_sum = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_sum = np.array(Ytrain_sum)\n",
    "Ytrain_sum = Ytrain_sum.ravel() \n",
    "Ytest_sum = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_sum = np.array(Ytest_sum)\n",
    "Ytest_sum = Ytest_sum.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_sum, Xval_sum, Ytrain_sum, Yval_sum = train_test_split(\n",
    "                    Xtrain_sum, Ytrain_sum, test_size = 0.2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7454176998689093"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the Sum Dataset.\n",
    "log_sum = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_sun model with the training data.\n",
    "log_sum.fit(Xtrain_sum,Ytrain_sum)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.c\n",
    "yval_pred_sum = log_sum.predict(Xval_sum)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_sum = roc_auc_score(Yval_sum, yval_pred_sum)\n",
    "VAL_auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6822.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_sum = log_sum.predict(Xtest_sum)\n",
    "TEST_auc_sum = roc_auc_score(Ytest_sum, ytest_pred_sum)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MLP Model'></a>\n",
    "## MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists.\n"
     ]
    }
   ],
   "source": [
    "# This creates a directory to save the best models for the MLP.\n",
    "# This comes from Reference 13 in References.\n",
    "import os\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.mkdir('saved_models')\n",
    "else:\n",
    "    print(\"Directory already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This imports the necessary libraries for the MLP.\n",
    "\n",
    "# This imports the sequential model, the layers,\n",
    "# the SGD optimizer, the regularizers from keras.\n",
    "# This comes from Reference 5 in Referenes.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Nadam\n",
    "from keras import regularizers\n",
    "\n",
    "# This imports checkpointer, which records the best weights\n",
    "# for the algorithm.\n",
    "# This comes from Reference 6 in References.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets the random seeds for reproducible results.\n",
    "# This comes from reference 10 of references.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drop_rate, l2_factor, first_dense, second_dense,\n",
    "                third_dense, hidden_act, out_act, x):\n",
    "    dim_int = int(np.size(x,1))\n",
    "    # This defines the model as a sequential model.\n",
    "    # This comes from References 1 in References.\n",
    "    model = Sequential()\n",
    "\n",
    "    # This is the input layer.\n",
    "    # This comes from References 1 & 3 in References.\n",
    "    model.add(Dense(first_dense, activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor),\n",
    "        input_dim = dim_int))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the first hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(second_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # This creates the second hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(third_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the output layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(1, activation=out_act))\n",
    "    # This returns the model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP NO Nulls, default learning rate in Stochastic Loss Optimizer\n",
    "This is a comparison MLP model that uses the default learning rate of 0.01 to descend down the loss gradient in an attempt to find the global minimum (Reference 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "DEFAULT_stochastic = SGD()\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for No NULLS.\n",
    "mlp_nonulls_DEFAULT = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Nulls data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nonulls_DEFAULT.compile(loss='binary_crossentropy',\n",
    "              optimizer= DEFAULT_stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15740 samples, validate on 3936 samples\n",
      "Epoch 1/100\n",
      "15740/15740 [==============================] - 2s 142us/step - loss: 0.7697 - acc: 0.5891 - val_loss: 0.9342 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93422, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 2/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.7429 - acc: 0.6299 - val_loss: 0.9950 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.93422\n",
      "Epoch 3/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.7285 - acc: 0.6356 - val_loss: 1.0060 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.93422\n",
      "Epoch 4/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.7221 - acc: 0.6382 - val_loss: 0.9887 - val_acc: 0.0145\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.93422\n",
      "Epoch 5/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.7108 - acc: 0.6509 - val_loss: 0.9597 - val_acc: 0.0582\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.93422\n",
      "Epoch 6/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.7018 - acc: 0.6600 - val_loss: 0.9458 - val_acc: 0.0732\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.93422\n",
      "Epoch 7/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.6891 - acc: 0.6691 - val_loss: 0.9039 - val_acc: 0.1847\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.93422 to 0.90392, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 8/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.6779 - acc: 0.6745 - val_loss: 0.8800 - val_acc: 0.2604\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.90392 to 0.87999, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 9/100\n",
      "15740/15740 [==============================] - 1s 50us/step - loss: 0.6698 - acc: 0.6807 - val_loss: 0.8684 - val_acc: 0.2861\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.87999 to 0.86836, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 10/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.6671 - acc: 0.6797 - val_loss: 0.8556 - val_acc: 0.3135\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.86836 to 0.85564, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 11/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.6475 - acc: 0.6956 - val_loss: 0.8324 - val_acc: 0.3923\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.85564 to 0.83235, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 12/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.6403 - acc: 0.6956 - val_loss: 0.8116 - val_acc: 0.4319\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.83235 to 0.81156, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 13/100\n",
      "15740/15740 [==============================] - 1s 53us/step - loss: 0.6365 - acc: 0.7003 - val_loss: 0.8264 - val_acc: 0.4057\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.81156\n",
      "Epoch 14/100\n",
      "15740/15740 [==============================] - 1s 53us/step - loss: 0.6242 - acc: 0.7049 - val_loss: 0.7878 - val_acc: 0.4995\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.81156 to 0.78781, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 15/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.6119 - acc: 0.7112 - val_loss: 0.7728 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78781 to 0.77277, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 16/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.6065 - acc: 0.7113 - val_loss: 0.7710 - val_acc: 0.5701\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.77277 to 0.77098, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 17/100\n",
      "15740/15740 [==============================] - 1s 50us/step - loss: 0.6007 - acc: 0.7182 - val_loss: 0.7312 - val_acc: 0.6758\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.77098 to 0.73117, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 18/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.5900 - acc: 0.7247 - val_loss: 0.7366 - val_acc: 0.6629\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73117\n",
      "Epoch 19/100\n",
      "15740/15740 [==============================] - 1s 64us/step - loss: 0.5789 - acc: 0.7278 - val_loss: 0.6805 - val_acc: 0.7873\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.73117 to 0.68048, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 20/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.5777 - acc: 0.7252 - val_loss: 0.6749 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.68048 to 0.67487, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 21/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.5692 - acc: 0.7297 - val_loss: 0.6777 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.67487\n",
      "Epoch 22/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.5606 - acc: 0.7369 - val_loss: 0.6569 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.67487 to 0.65690, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 23/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.5488 - acc: 0.7454 - val_loss: 0.6351 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.65690 to 0.63514, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 24/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.5442 - acc: 0.7492 - val_loss: 0.6421 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.63514\n",
      "Epoch 25/100\n",
      "15740/15740 [==============================] - 1s 59us/step - loss: 0.5377 - acc: 0.7517 - val_loss: 0.6014 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.63514 to 0.60145, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 26/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.5334 - acc: 0.7544 - val_loss: 0.6063 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.60145\n",
      "Epoch 27/100\n",
      "15740/15740 [==============================] - 1s 63us/step - loss: 0.5303 - acc: 0.7643 - val_loss: 0.6055 - val_acc: 0.8874\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.60145\n",
      "Epoch 28/100\n",
      "15740/15740 [==============================] - 1s 57us/step - loss: 0.5182 - acc: 0.7703 - val_loss: 0.5653 - val_acc: 0.8976\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.60145 to 0.56533, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 29/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.5126 - acc: 0.7706 - val_loss: 0.5838 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.56533\n",
      "Epoch 30/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.5080 - acc: 0.7740 - val_loss: 0.5563 - val_acc: 0.9007\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.56533 to 0.55628, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 31/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.5040 - acc: 0.7777 - val_loss: 0.5379 - val_acc: 0.8999\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.55628 to 0.53791, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 32/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4968 - acc: 0.7874 - val_loss: 0.5454 - val_acc: 0.8829\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.53791\n",
      "Epoch 33/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.4943 - acc: 0.7897 - val_loss: 0.5217 - val_acc: 0.8935\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.53791 to 0.52166, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 34/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.4857 - acc: 0.7931 - val_loss: 0.5115 - val_acc: 0.8923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss improved from 0.52166 to 0.51146, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 35/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.4832 - acc: 0.7968 - val_loss: 0.5204 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51146\n",
      "Epoch 36/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.4839 - acc: 0.7990 - val_loss: 0.5125 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51146\n",
      "Epoch 37/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.4777 - acc: 0.7992 - val_loss: 0.4955 - val_acc: 0.8905\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.51146 to 0.49550, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 38/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.4688 - acc: 0.8062 - val_loss: 0.4873 - val_acc: 0.8941\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.49550 to 0.48733, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 39/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.4699 - acc: 0.8044 - val_loss: 0.4795 - val_acc: 0.8991\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.48733 to 0.47945, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 40/100\n",
      "15740/15740 [==============================] - 1s 61us/step - loss: 0.4596 - acc: 0.8137 - val_loss: 0.4949 - val_acc: 0.8753\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47945\n",
      "Epoch 41/100\n",
      "15740/15740 [==============================] - 1s 57us/step - loss: 0.4567 - acc: 0.8157 - val_loss: 0.4715 - val_acc: 0.8915\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.47945 to 0.47150, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 42/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.4609 - acc: 0.8161 - val_loss: 0.5102 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47150\n",
      "Epoch 43/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.4506 - acc: 0.8206 - val_loss: 0.4520 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.47150 to 0.45197, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 44/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4496 - acc: 0.8178 - val_loss: 0.4481 - val_acc: 0.9055\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.45197 to 0.44805, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 45/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.4478 - acc: 0.8189 - val_loss: 0.4602 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.44805\n",
      "Epoch 46/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.4452 - acc: 0.8205 - val_loss: 0.4519 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.44805\n",
      "Epoch 47/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4368 - acc: 0.8280 - val_loss: 0.4450 - val_acc: 0.8961\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.44805 to 0.44503, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 48/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.4378 - acc: 0.8228 - val_loss: 0.4542 - val_acc: 0.9057\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.44503\n",
      "Epoch 49/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.4362 - acc: 0.8279 - val_loss: 0.4255 - val_acc: 0.9220\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.44503 to 0.42545, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 50/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4291 - acc: 0.8299 - val_loss: 0.4148 - val_acc: 0.9248\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.42545 to 0.41483, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 51/100\n",
      "15740/15740 [==============================] - 1s 53us/step - loss: 0.4275 - acc: 0.8309 - val_loss: 0.4156 - val_acc: 0.9240\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.41483\n",
      "Epoch 52/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4282 - acc: 0.8328 - val_loss: 0.4507 - val_acc: 0.8994\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.41483\n",
      "Epoch 53/100\n",
      "15740/15740 [==============================] - 1s 57us/step - loss: 0.4292 - acc: 0.8290 - val_loss: 0.4329 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.41483\n",
      "Epoch 54/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4230 - acc: 0.8358 - val_loss: 0.4408 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.41483\n",
      "Epoch 55/100\n",
      "15740/15740 [==============================] - 1s 57us/step - loss: 0.4226 - acc: 0.8344 - val_loss: 0.4076 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.41483 to 0.40762, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 56/100\n",
      "15740/15740 [==============================] - 1s 58us/step - loss: 0.4199 - acc: 0.8367 - val_loss: 0.4194 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.40762\n",
      "Epoch 57/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4160 - acc: 0.8417 - val_loss: 0.3564 - val_acc: 0.9548\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.40762 to 0.35644, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 58/100\n",
      "15740/15740 [==============================] - 1s 59us/step - loss: 0.4121 - acc: 0.8412 - val_loss: 0.3978 - val_acc: 0.9243\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.35644\n",
      "Epoch 59/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4128 - acc: 0.8386 - val_loss: 0.3866 - val_acc: 0.9258\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.35644\n",
      "Epoch 60/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.4059 - acc: 0.8401 - val_loss: 0.3831 - val_acc: 0.9278\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.35644\n",
      "Epoch 61/100\n",
      "15740/15740 [==============================] - 1s 57us/step - loss: 0.4089 - acc: 0.8419 - val_loss: 0.3945 - val_acc: 0.9380\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.35644\n",
      "Epoch 62/100\n",
      "15740/15740 [==============================] - 1s 50us/step - loss: 0.4091 - acc: 0.8449 - val_loss: 0.4302 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.35644\n",
      "Epoch 63/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.4067 - acc: 0.8464 - val_loss: 0.3938 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.35644\n",
      "Epoch 64/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3972 - acc: 0.8536 - val_loss: 0.3668 - val_acc: 0.9370\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.35644\n",
      "Epoch 65/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3975 - acc: 0.8506 - val_loss: 0.4011 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.35644\n",
      "Epoch 66/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3976 - acc: 0.8482 - val_loss: 0.3699 - val_acc: 0.9291\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.35644\n",
      "Epoch 67/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3996 - acc: 0.8455 - val_loss: 0.3695 - val_acc: 0.9433\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.35644\n",
      "Epoch 68/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3918 - acc: 0.8507 - val_loss: 0.3578 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.35644\n",
      "Epoch 69/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3909 - acc: 0.8497 - val_loss: 0.4153 - val_acc: 0.8981\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.35644\n",
      "Epoch 70/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3928 - acc: 0.8524 - val_loss: 0.3678 - val_acc: 0.9352\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.35644\n",
      "Epoch 71/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3955 - acc: 0.8544 - val_loss: 0.3474 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.35644 to 0.34739, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 72/100\n",
      "15740/15740 [==============================] - 1s 50us/step - loss: 0.3892 - acc: 0.8534 - val_loss: 0.3609 - val_acc: 0.9347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_loss did not improve from 0.34739\n",
      "Epoch 73/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3842 - acc: 0.8527 - val_loss: 0.3294 - val_acc: 0.9563\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.34739 to 0.32939, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 74/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3872 - acc: 0.8548 - val_loss: 0.3160 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.32939 to 0.31601, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 75/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3830 - acc: 0.8557 - val_loss: 0.3722 - val_acc: 0.9286\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.31601\n",
      "Epoch 76/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3861 - acc: 0.8534 - val_loss: 0.3466 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.31601\n",
      "Epoch 77/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3781 - acc: 0.8588 - val_loss: 0.3712 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.31601\n",
      "Epoch 78/100\n",
      "15740/15740 [==============================] - 1s 51us/step - loss: 0.3798 - acc: 0.8565 - val_loss: 0.3446 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.31601\n",
      "Epoch 79/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3806 - acc: 0.8566 - val_loss: 0.3397 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.31601\n",
      "Epoch 80/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.3771 - acc: 0.8627 - val_loss: 0.3400 - val_acc: 0.9507\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.31601\n",
      "Epoch 81/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3802 - acc: 0.8595 - val_loss: 0.3300 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.31601\n",
      "Epoch 82/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3712 - acc: 0.8630 - val_loss: 0.3472 - val_acc: 0.9413\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.31601\n",
      "Epoch 83/100\n",
      "15740/15740 [==============================] - 1s 52us/step - loss: 0.3717 - acc: 0.8617 - val_loss: 0.3202 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.31601\n",
      "Epoch 84/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3728 - acc: 0.8616 - val_loss: 0.3920 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.31601\n",
      "Epoch 85/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.3733 - acc: 0.8644 - val_loss: 0.3285 - val_acc: 0.9461\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.31601\n",
      "Epoch 86/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3722 - acc: 0.8615 - val_loss: 0.3238 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.31601\n",
      "Epoch 87/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.3691 - acc: 0.8642 - val_loss: 0.3376 - val_acc: 0.9395\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.31601\n",
      "Epoch 88/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.3705 - acc: 0.8649 - val_loss: 0.3247 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.31601\n",
      "Epoch 89/100\n",
      "15740/15740 [==============================] - 1s 53us/step - loss: 0.3638 - acc: 0.8673 - val_loss: 0.3198 - val_acc: 0.9527\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.31601\n",
      "Epoch 90/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.3678 - acc: 0.8651 - val_loss: 0.3416 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.31601\n",
      "Epoch 91/100\n",
      "15740/15740 [==============================] - 1s 53us/step - loss: 0.3646 - acc: 0.8656 - val_loss: 0.3204 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.31601\n",
      "Epoch 92/100\n",
      "15740/15740 [==============================] - 1s 56us/step - loss: 0.3646 - acc: 0.8661 - val_loss: 0.3142 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.31601 to 0.31419, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 93/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3635 - acc: 0.8726 - val_loss: 0.3130 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.31419 to 0.31301, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 94/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.3638 - acc: 0.8667 - val_loss: 0.3454 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.31301\n",
      "Epoch 95/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3603 - acc: 0.8692 - val_loss: 0.3459 - val_acc: 0.9502\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.31301\n",
      "Epoch 96/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3607 - acc: 0.8694 - val_loss: 0.2874 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.31301 to 0.28740, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n",
      "Epoch 97/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3638 - acc: 0.8684 - val_loss: 0.3197 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.28740\n",
      "Epoch 98/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3557 - acc: 0.8717 - val_loss: 0.3137 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.28740\n",
      "Epoch 99/100\n",
      "15740/15740 [==============================] - 1s 55us/step - loss: 0.3651 - acc: 0.8694 - val_loss: 0.2988 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.28740\n",
      "Epoch 100/100\n",
      "15740/15740 [==============================] - 1s 54us/step - loss: 0.3538 - acc: 0.8729 - val_loss: 0.2868 - val_acc: 0.9494\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.28740 to 0.28684, saving model to saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b91f0f7ef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nonulls_DEFAULT.fit(Xtrain_nonulls, Ytrain_nonulls, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_nonulls_DEFAULT.load_weights('saved_models/weights.best.mlp_nonulls_DEFAULT.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 96.6277%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_nonulls_DEFAULT.evaluate(Xtest_nonulls, Ytest_nonulls, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.5000.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_nonulls_DEFAULT = mlp_nonulls_DEFAULT.predict(Xtest_nonulls)\n",
    "mlp_nonulls_ROC_DEFAULT = roc_auc_score(Ytest_nonulls, Ypred_nonulls_DEFAULT)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nonulls_ROC_DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model No Nulls Final\n",
    "This model is the final MLP No Nulls Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for No NULLS.\n",
    "mlp_nonulls = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Nulls data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nonulls.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nonulls.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11018 samples, validate on 2755 samples\n",
      "Epoch 1/100\n",
      "11018/11018 [==============================] - 5s 427us/step - loss: 0.8007 - acc: 0.5077 - val_loss: 0.7505 - val_acc: 0.6650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.75053, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 2/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.7898 - acc: 0.5230 - val_loss: 0.7485 - val_acc: 0.6650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.75053 to 0.74853, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 3/100\n",
      "11018/11018 [==============================] - 1s 86us/step - loss: 0.7858 - acc: 0.5197 - val_loss: 0.7468 - val_acc: 0.6613\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.74853 to 0.74682, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 4/100\n",
      "11018/11018 [==============================] - 1s 97us/step - loss: 0.7775 - acc: 0.5286 - val_loss: 0.7451 - val_acc: 0.6584\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.74682 to 0.74515, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 5/100\n",
      "11018/11018 [==============================] - 1s 86us/step - loss: 0.7787 - acc: 0.5276 - val_loss: 0.7436 - val_acc: 0.6613\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.74515 to 0.74359, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 6/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.7739 - acc: 0.5320 - val_loss: 0.7423 - val_acc: 0.6672\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.74359 to 0.74227, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 7/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.7770 - acc: 0.5380 - val_loss: 0.7411 - val_acc: 0.6711\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.74227 to 0.74113, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 8/100\n",
      "11018/11018 [==============================] - 1s 98us/step - loss: 0.7711 - acc: 0.5371 - val_loss: 0.7399 - val_acc: 0.6770\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.74113 to 0.73993, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 9/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.7656 - acc: 0.5413 - val_loss: 0.7386 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.73993 to 0.73860, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 10/100\n",
      "11018/11018 [==============================] - 1s 90us/step - loss: 0.7642 - acc: 0.5463 - val_loss: 0.7374 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.73860 to 0.73741, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 11/100\n",
      "11018/11018 [==============================] - 1s 93us/step - loss: 0.7584 - acc: 0.5487 - val_loss: 0.7362 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.73741 to 0.73621, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 12/100\n",
      "11018/11018 [==============================] - 1s 104us/step - loss: 0.7613 - acc: 0.5501 - val_loss: 0.7349 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.73621 to 0.73490, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 13/100\n",
      "11018/11018 [==============================] - 1s 95us/step - loss: 0.7579 - acc: 0.5503 - val_loss: 0.7338 - val_acc: 0.6984\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.73490 to 0.73380, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 14/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.7583 - acc: 0.5515 - val_loss: 0.7326 - val_acc: 0.7020\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.73380 to 0.73263, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 15/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.7575 - acc: 0.5512 - val_loss: 0.7316 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.73263 to 0.73162, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 16/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.7556 - acc: 0.5559 - val_loss: 0.7302 - val_acc: 0.7071\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.73162 to 0.73022, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 17/100\n",
      "11018/11018 [==============================] - 1s 100us/step - loss: 0.7502 - acc: 0.5626 - val_loss: 0.7290 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.73022 to 0.72901, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 18/100\n",
      "11018/11018 [==============================] - 1s 90us/step - loss: 0.7463 - acc: 0.5709 - val_loss: 0.7275 - val_acc: 0.7129\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.72901 to 0.72754, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 19/100\n",
      "11018/11018 [==============================] - 1s 97us/step - loss: 0.7442 - acc: 0.5735 - val_loss: 0.7259 - val_acc: 0.7147\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.72754 to 0.72594, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 20/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.7455 - acc: 0.5672 - val_loss: 0.7245 - val_acc: 0.7162\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.72594 to 0.72449, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 21/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7455 - acc: 0.5712 - val_loss: 0.7232 - val_acc: 0.7205\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.72449 to 0.72318, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 22/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7430 - acc: 0.5742 - val_loss: 0.7218 - val_acc: 0.7220\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.72318 to 0.72176, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 23/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7378 - acc: 0.5765 - val_loss: 0.7202 - val_acc: 0.7216\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.72176 to 0.72018, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 24/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7394 - acc: 0.5706 - val_loss: 0.7186 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.72018 to 0.71860, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 25/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7346 - acc: 0.5801 - val_loss: 0.7168 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.71860 to 0.71680, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 26/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7402 - acc: 0.5751 - val_loss: 0.7153 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.71680 to 0.71532, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 27/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7391 - acc: 0.5763 - val_loss: 0.7137 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.71532 to 0.71372, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 28/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7357 - acc: 0.5841 - val_loss: 0.7120 - val_acc: 0.7328\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.71372 to 0.71200, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 29/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.7333 - acc: 0.5848 - val_loss: 0.7102 - val_acc: 0.7354\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.71200 to 0.71018, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 30/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7318 - acc: 0.5920 - val_loss: 0.7083 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.71018 to 0.70833, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 31/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.7330 - acc: 0.5870 - val_loss: 0.7065 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.70833 to 0.70647, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 32/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7340 - acc: 0.5848 - val_loss: 0.7050 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.70647 to 0.70500, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.7302 - acc: 0.5926 - val_loss: 0.7031 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.70500 to 0.70313, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 34/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7316 - acc: 0.5870 - val_loss: 0.7012 - val_acc: 0.7441\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.70313 to 0.70124, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 35/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.7206 - acc: 0.5869 - val_loss: 0.6993 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.70124 to 0.69934, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 36/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7343 - acc: 0.5843 - val_loss: 0.6982 - val_acc: 0.7448\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.69934 to 0.69818, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 37/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.7255 - acc: 0.6002 - val_loss: 0.6965 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.69818 to 0.69647, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 38/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7225 - acc: 0.5969 - val_loss: 0.6946 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.69647 to 0.69457, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 39/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7176 - acc: 0.6066 - val_loss: 0.6924 - val_acc: 0.7495\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.69457 to 0.69236, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 40/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7212 - acc: 0.5999 - val_loss: 0.6908 - val_acc: 0.7492\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.69236 to 0.69076, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 41/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7169 - acc: 0.6067 - val_loss: 0.6886 - val_acc: 0.7514\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.69076 to 0.68861, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 42/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7158 - acc: 0.6141 - val_loss: 0.6867 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.68861 to 0.68671, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 43/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7147 - acc: 0.6086 - val_loss: 0.6844 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.68671 to 0.68441, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 44/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.7187 - acc: 0.6099 - val_loss: 0.6824 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.68441 to 0.68239, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 45/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7126 - acc: 0.6134 - val_loss: 0.6806 - val_acc: 0.7554\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.68239 to 0.68060, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 46/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7093 - acc: 0.6117 - val_loss: 0.6786 - val_acc: 0.7561\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.68060 to 0.67859, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 47/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7128 - acc: 0.6190 - val_loss: 0.6767 - val_acc: 0.7557\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.67859 to 0.67672, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 48/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7149 - acc: 0.6151 - val_loss: 0.6753 - val_acc: 0.7568\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.67672 to 0.67531, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 49/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7083 - acc: 0.6182 - val_loss: 0.6734 - val_acc: 0.7586\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.67531 to 0.67338, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 50/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7023 - acc: 0.6185 - val_loss: 0.6715 - val_acc: 0.7586\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.67338 to 0.67155, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 51/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7012 - acc: 0.6262 - val_loss: 0.6696 - val_acc: 0.7583\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.67155 to 0.66965, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 52/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.7052 - acc: 0.6265 - val_loss: 0.6679 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.66965 to 0.66791, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 53/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7063 - acc: 0.6195 - val_loss: 0.6662 - val_acc: 0.7586\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.66791 to 0.66618, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 54/100\n",
      "11018/11018 [==============================] - 1s 81us/step - loss: 0.7013 - acc: 0.6283 - val_loss: 0.6643 - val_acc: 0.7597\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.66618 to 0.66434, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 55/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7019 - acc: 0.6210 - val_loss: 0.6626 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.66434 to 0.66256, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 56/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.7029 - acc: 0.6284 - val_loss: 0.6610 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.66256 to 0.66102, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 57/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.6965 - acc: 0.6410 - val_loss: 0.6590 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.66102 to 0.65902, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 58/100\n",
      "11018/11018 [==============================] - 1s 82us/step - loss: 0.7028 - acc: 0.6312 - val_loss: 0.6574 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.65902 to 0.65737, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 59/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.6988 - acc: 0.6329 - val_loss: 0.6557 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.65737 to 0.65575, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 60/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.6961 - acc: 0.6372 - val_loss: 0.6541 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.65575 to 0.65408, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 61/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.6944 - acc: 0.6398 - val_loss: 0.6523 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.65408 to 0.65227, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 62/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.6904 - acc: 0.6432 - val_loss: 0.6504 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.65227 to 0.65039, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 63/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.6905 - acc: 0.6427 - val_loss: 0.6486 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.65039 to 0.64863, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 64/100\n",
      "11018/11018 [==============================] - 1s 83us/step - loss: 0.6940 - acc: 0.6380 - val_loss: 0.6469 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.64863 to 0.64694, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 65/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.6886 - acc: 0.6477 - val_loss: 0.6452 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.64694 to 0.64523, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 66/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6859 - acc: 0.6484 - val_loss: 0.6436 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.64523 to 0.64357, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 67/100\n",
      "11018/11018 [==============================] - 1s 84us/step - loss: 0.6884 - acc: 0.6449 - val_loss: 0.6419 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.64357 to 0.64195, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 68/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6789 - acc: 0.6562 - val_loss: 0.6399 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.64195 to 0.63990, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 69/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6824 - acc: 0.6540 - val_loss: 0.6383 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.63990 to 0.63825, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 70/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6813 - acc: 0.6510 - val_loss: 0.6367 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.63825 to 0.63668, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 71/100\n",
      "11018/11018 [==============================] - 1s 85us/step - loss: 0.6811 - acc: 0.6556 - val_loss: 0.6348 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.63668 to 0.63481, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 72/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.6808 - acc: 0.6583 - val_loss: 0.6331 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.63481 to 0.63306, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 73/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6811 - acc: 0.6585 - val_loss: 0.6315 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.63306 to 0.63151, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 74/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.6768 - acc: 0.6640 - val_loss: 0.6299 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.63151 to 0.62995, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 75/100\n",
      "11018/11018 [==============================] - 1s 100us/step - loss: 0.6791 - acc: 0.6615 - val_loss: 0.6287 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.62995 to 0.62873, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 76/100\n",
      "11018/11018 [==============================] - 1s 90us/step - loss: 0.6785 - acc: 0.6599 - val_loss: 0.6273 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.62873 to 0.62734, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 77/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.6781 - acc: 0.6620 - val_loss: 0.6261 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.62734 to 0.62615, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 78/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6771 - acc: 0.6614 - val_loss: 0.6248 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.62615 to 0.62480, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 79/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.6722 - acc: 0.6684 - val_loss: 0.6233 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.62480 to 0.62327, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 80/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.6694 - acc: 0.6763 - val_loss: 0.6217 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.62327 to 0.62169, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 81/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.6760 - acc: 0.6718 - val_loss: 0.6205 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.62169 to 0.62051, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 82/100\n",
      "11018/11018 [==============================] - 1s 87us/step - loss: 0.6679 - acc: 0.6750 - val_loss: 0.6191 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.62051 to 0.61911, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 83/100\n",
      "11018/11018 [==============================] - 1s 88us/step - loss: 0.6729 - acc: 0.6694 - val_loss: 0.6180 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.61911 to 0.61796, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 84/100\n",
      "11018/11018 [==============================] - 1s 89us/step - loss: 0.6700 - acc: 0.6749 - val_loss: 0.6164 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.61796 to 0.61642, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 85/100\n",
      "11018/11018 [==============================] - 1s 105us/step - loss: 0.6713 - acc: 0.6701 - val_loss: 0.6153 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.61642 to 0.61532, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 86/100\n",
      "11018/11018 [==============================] - 1s 102us/step - loss: 0.6661 - acc: 0.6772 - val_loss: 0.6139 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.61532 to 0.61389, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 87/100\n",
      "11018/11018 [==============================] - 1s 116us/step - loss: 0.6695 - acc: 0.6712 - val_loss: 0.6128 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.61389 to 0.61277, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 88/100\n",
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.6643 - acc: 0.6765 - val_loss: 0.6115 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.61277 to 0.61152, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 89/100\n",
      "11018/11018 [==============================] - 1s 99us/step - loss: 0.6643 - acc: 0.6816 - val_loss: 0.6103 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.61152 to 0.61026, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 90/100\n",
      "11018/11018 [==============================] - 1s 106us/step - loss: 0.6630 - acc: 0.6773 - val_loss: 0.6091 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.61026 to 0.60907, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 91/100\n",
      "11018/11018 [==============================] - 1s 101us/step - loss: 0.6623 - acc: 0.6864 - val_loss: 0.6078 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.60907 to 0.60782, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 92/100\n",
      "11018/11018 [==============================] - 1s 95us/step - loss: 0.6654 - acc: 0.6809 - val_loss: 0.6066 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.60782 to 0.60658, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 93/100\n",
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.6542 - acc: 0.6889 - val_loss: 0.6050 - val_acc: 0.7684\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.60658 to 0.60504, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 94/100\n",
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.6641 - acc: 0.6893 - val_loss: 0.6039 - val_acc: 0.7688\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.60504 to 0.60395, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 95/100\n",
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.6569 - acc: 0.6895 - val_loss: 0.6027 - val_acc: 0.7684\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.60395 to 0.60274, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 96/100\n",
      "11018/11018 [==============================] - 1s 93us/step - loss: 0.6567 - acc: 0.6911 - val_loss: 0.6014 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.60274 to 0.60143, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.6510 - acc: 0.6930 - val_loss: 0.6000 - val_acc: 0.7702\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.60143 to 0.60004, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 98/100\n",
      "11018/11018 [==============================] - 1s 96us/step - loss: 0.6537 - acc: 0.6928 - val_loss: 0.5988 - val_acc: 0.7706\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.60004 to 0.59875, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 99/100\n",
      "11018/11018 [==============================] - 1s 91us/step - loss: 0.6528 - acc: 0.6977 - val_loss: 0.5976 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.59875 to 0.59755, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 100/100\n",
      "11018/11018 [==============================] - 1s 97us/step - loss: 0.6548 - acc: 0.6936 - val_loss: 0.5964 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.59755 to 0.59645, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efd11c5a90>"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nonulls.fit(Xtrain_nonulls, Ytrain_nonulls, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_nonulls.load_weights('saved_models/weights.best.mlp_nonulls.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 85.5092%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_nonulls.evaluate(Xtest_nonulls, Ytest_nonulls, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.5504.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_nonulls = mlp_nonulls.predict(Xtest_nonulls)\n",
    "mlp_nonulls_ROC = roc_auc_score(Ytest_nonulls, Ypred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nonulls_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary library for the ROC Curve.\n",
    "from sklearn import metrics\n",
    "\n",
    "def roc_plot(model, x_test, y_test, title):\n",
    "    '''\n",
    "    This function plots ROC curves. It takes in\n",
    "    the model, title, x_test, y_test as arguments.\n",
    "    '''\n",
    "    # This sets the size of the plot\n",
    "    # This comes from Reference 12 in References\n",
    "    plt.figure(figsize=(6.0,6.0))\n",
    "    \n",
    "    # This finds the predicted values of y from the x_test\n",
    "    # data.\n",
    "    # This comes from Reference 11 in References.\n",
    "    pred_probs = model.predict_proba(x_test)\n",
    "    \n",
    "    # This finds the false and true positive rates for\n",
    "    # the ROC Curve.\n",
    "    # This comes from Reference 11 in References.\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, pred_probs)\n",
    "    \n",
    "    # This finds the AUC score.\n",
    "    # This comes from Reference 11 in References.\n",
    "    auc = metrics.roc_auc_score(y_test, pred_probs)\n",
    "    auc = str(auc)\n",
    "    auc = auc[0:6]\n",
    "    \n",
    "    # This plots the ROC Curve with the AUC label.\n",
    "    plt.plot(fpr,tpr,label='auc: ' + auc)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This plots the ROC curve for the model.\n",
    "roc_plot(model=mlp_nonulls, x_test=Xtest_nonulls,\n",
    "        y_test=Ytest_nonulls,title=\"ROC: MLP No Nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Nulls Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Nulls Only, default learning rate in Stochastic Gradient Loss Optimizer\n",
    "This is a comparison MLP model that uses the default learning rate of 0.01 to descend down the loss gradient in an attempt to find the global minimum (Reference 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "DEFAULT_stochastic = SGD()\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for Nulls Only.\n",
    "mlp_nullsonly_DEFAULT = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nullsonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the Nulls Only data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nullsonly_DEFAULT.compile(loss='binary_crossentropy',\n",
    "              optimizer= DEFAULT_stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65512 samples, validate on 16378 samples\n",
      "Epoch 1/100\n",
      "65512/65512 [==============================] - 5s 81us/step - loss: 0.7261 - acc: 0.6398 - val_loss: 0.9572 - val_acc: 0.4154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95716, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 2/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.6954 - acc: 0.6558 - val_loss: 0.9465 - val_acc: 0.4391\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95716 to 0.94647, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 3/100\n",
      "65512/65512 [==============================] - 4s 58us/step - loss: 0.6787 - acc: 0.6624 - val_loss: 0.9388 - val_acc: 0.4463\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.94647 to 0.93878, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 4/100\n",
      "65512/65512 [==============================] - 4s 58us/step - loss: 0.6672 - acc: 0.6693 - val_loss: 0.9068 - val_acc: 0.4581\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.93878 to 0.90675, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 5/100\n",
      "65512/65512 [==============================] - 4s 56us/step - loss: 0.6597 - acc: 0.6797 - val_loss: 0.8945 - val_acc: 0.4701\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.90675 to 0.89447, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 6/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6537 - acc: 0.6897 - val_loss: 0.8767 - val_acc: 0.4812\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.89447 to 0.87670, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 7/100\n",
      "65512/65512 [==============================] - 4s 59us/step - loss: 0.6435 - acc: 0.7058 - val_loss: 0.8690 - val_acc: 0.4962\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.87670 to 0.86904, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 8/100\n",
      "65512/65512 [==============================] - 4s 56us/step - loss: 0.6355 - acc: 0.7168 - val_loss: 0.8707 - val_acc: 0.4984\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.86904\n",
      "Epoch 9/100\n",
      "65512/65512 [==============================] - 4s 58us/step - loss: 0.6261 - acc: 0.7214 - val_loss: 0.8549 - val_acc: 0.5147\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.86904 to 0.85494, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 10/100\n",
      "65512/65512 [==============================] - 4s 60us/step - loss: 0.6200 - acc: 0.7250 - val_loss: 0.8252 - val_acc: 0.5195\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.85494 to 0.82524, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 11/100\n",
      "65512/65512 [==============================] - 4s 60us/step - loss: 0.6142 - acc: 0.7257 - val_loss: 0.8136 - val_acc: 0.5322\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.82524 to 0.81357, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 12/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.6083 - acc: 0.7300 - val_loss: 0.7817 - val_acc: 0.5497\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.81357 to 0.78170, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 13/100\n",
      "65512/65512 [==============================] - 4s 59us/step - loss: 0.6026 - acc: 0.7293 - val_loss: 0.8013 - val_acc: 0.5476\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.78170\n",
      "Epoch 14/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5998 - acc: 0.7304 - val_loss: 0.7715 - val_acc: 0.5584\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78170 to 0.77147, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 15/100\n",
      "65512/65512 [==============================] - 4s 58us/step - loss: 0.5975 - acc: 0.7280 - val_loss: 0.7739 - val_acc: 0.5656\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77147\n",
      "Epoch 16/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5925 - acc: 0.7357 - val_loss: 0.7880 - val_acc: 0.5639\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.77147\n",
      "Epoch 17/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5901 - acc: 0.7349 - val_loss: 0.7446 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.77147 to 0.74460, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 18/100\n",
      "65512/65512 [==============================] - 4s 60us/step - loss: 0.5849 - acc: 0.7378 - val_loss: 0.7750 - val_acc: 0.5800\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.74460\n",
      "Epoch 19/100\n",
      "65512/65512 [==============================] - 4s 60us/step - loss: 0.5813 - acc: 0.7391 - val_loss: 0.7832 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.74460\n",
      "Epoch 20/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5808 - acc: 0.7387 - val_loss: 0.7452 - val_acc: 0.6034\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.74460\n",
      "Epoch 21/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5775 - acc: 0.7404 - val_loss: 0.7800 - val_acc: 0.5797\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.74460\n",
      "Epoch 22/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5753 - acc: 0.7407 - val_loss: 0.7376 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.74460 to 0.73764, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 23/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5722 - acc: 0.7309 - val_loss: 0.7294 - val_acc: 0.5870\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.73764 to 0.72942, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 24/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5706 - acc: 0.7270 - val_loss: 0.7833 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.72942\n",
      "Epoch 25/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5680 - acc: 0.7266 - val_loss: 0.7530 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.72942\n",
      "Epoch 26/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5666 - acc: 0.7278 - val_loss: 0.7349 - val_acc: 0.5913\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.72942\n",
      "Epoch 27/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5642 - acc: 0.7291 - val_loss: 0.7691 - val_acc: 0.6028\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.72942\n",
      "Epoch 28/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5627 - acc: 0.7302 - val_loss: 0.7055 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.72942 to 0.70555, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 29/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5597 - acc: 0.7311 - val_loss: 0.8079 - val_acc: 0.5608\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.70555\n",
      "Epoch 30/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5578 - acc: 0.7305 - val_loss: 0.6986 - val_acc: 0.6504\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.70555 to 0.69863, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 31/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5560 - acc: 0.7321 - val_loss: 0.7643 - val_acc: 0.5929\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.69863\n",
      "Epoch 32/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5551 - acc: 0.7314 - val_loss: 0.7078 - val_acc: 0.6511\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.69863\n",
      "Epoch 33/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5521 - acc: 0.7322 - val_loss: 0.6921 - val_acc: 0.6355\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.69863 to 0.69206, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 34/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5532 - acc: 0.7331 - val_loss: 0.7401 - val_acc: 0.5992\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.69206\n",
      "Epoch 35/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5518 - acc: 0.7340 - val_loss: 0.7505 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.69206\n",
      "Epoch 36/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.5494 - acc: 0.7336 - val_loss: 0.7009 - val_acc: 0.6676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss did not improve from 0.69206\n",
      "Epoch 37/100\n",
      "65512/65512 [==============================] - 5s 76us/step - loss: 0.5486 - acc: 0.7361 - val_loss: 0.7564 - val_acc: 0.6286\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.69206\n",
      "Epoch 38/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5464 - acc: 0.7475 - val_loss: 0.7152 - val_acc: 0.6590\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.69206\n",
      "Epoch 39/100\n",
      "65512/65512 [==============================] - 5s 76us/step - loss: 0.5441 - acc: 0.7519 - val_loss: 0.7513 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.69206\n",
      "Epoch 40/100\n",
      "65512/65512 [==============================] - 6s 88us/step - loss: 0.5456 - acc: 0.7496 - val_loss: 0.6587 - val_acc: 0.6930\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.69206 to 0.65874, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 41/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5447 - acc: 0.7505 - val_loss: 0.7256 - val_acc: 0.6159\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.65874\n",
      "Epoch 42/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.5418 - acc: 0.7479 - val_loss: 0.6133 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.65874 to 0.61335, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 43/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5417 - acc: 0.7540 - val_loss: 0.8399 - val_acc: 0.5598\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.61335\n",
      "Epoch 44/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5432 - acc: 0.7531 - val_loss: 0.6627 - val_acc: 0.6675\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.61335\n",
      "Epoch 45/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5418 - acc: 0.7547 - val_loss: 0.7408 - val_acc: 0.6217\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.61335\n",
      "Epoch 46/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5364 - acc: 0.7552 - val_loss: 0.6353 - val_acc: 0.7745\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.61335\n",
      "Epoch 47/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5392 - acc: 0.7576 - val_loss: 0.6028 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.61335 to 0.60279, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 48/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5368 - acc: 0.7571 - val_loss: 0.7960 - val_acc: 0.6114\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.60279\n",
      "Epoch 49/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5396 - acc: 0.7573 - val_loss: 0.6667 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.60279\n",
      "Epoch 50/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.5378 - acc: 0.7582 - val_loss: 0.7705 - val_acc: 0.6348\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.60279\n",
      "Epoch 51/100\n",
      "65512/65512 [==============================] - 5s 82us/step - loss: 0.5373 - acc: 0.7567 - val_loss: 0.8076 - val_acc: 0.5961\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.60279\n",
      "Epoch 52/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5374 - acc: 0.7580 - val_loss: 0.6181 - val_acc: 0.7464\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.60279\n",
      "Epoch 53/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5348 - acc: 0.7609 - val_loss: 0.8044 - val_acc: 0.5914\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.60279\n",
      "Epoch 54/100\n",
      "65512/65512 [==============================] - 6s 88us/step - loss: 0.5326 - acc: 0.7595 - val_loss: 0.6535 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.60279\n",
      "Epoch 55/100\n",
      "65512/65512 [==============================] - 6s 84us/step - loss: 0.5334 - acc: 0.7602 - val_loss: 0.7424 - val_acc: 0.6365\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.60279\n",
      "Epoch 56/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5335 - acc: 0.7597 - val_loss: 0.6905 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.60279\n",
      "Epoch 57/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5305 - acc: 0.7605 - val_loss: 0.7776 - val_acc: 0.6228\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.60279\n",
      "Epoch 58/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5300 - acc: 0.7628 - val_loss: 0.6838 - val_acc: 0.6685\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.60279\n",
      "Epoch 59/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5309 - acc: 0.7614 - val_loss: 0.6527 - val_acc: 0.7337\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.60279\n",
      "Epoch 60/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5316 - acc: 0.7621 - val_loss: 0.6130 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.60279\n",
      "Epoch 61/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5300 - acc: 0.7620 - val_loss: 0.7083 - val_acc: 0.6807\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.60279\n",
      "Epoch 62/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.5289 - acc: 0.7634 - val_loss: 0.8407 - val_acc: 0.5807\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.60279\n",
      "Epoch 63/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5283 - acc: 0.7615 - val_loss: 0.5928 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.60279 to 0.59283, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 64/100\n",
      "65512/65512 [==============================] - 5s 76us/step - loss: 0.5289 - acc: 0.7654 - val_loss: 0.8599 - val_acc: 0.6131\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.59283\n",
      "Epoch 65/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.5285 - acc: 0.7652 - val_loss: 0.6440 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.59283\n",
      "Epoch 66/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.5263 - acc: 0.7630 - val_loss: 0.8106 - val_acc: 0.5959\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.59283\n",
      "Epoch 67/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.5267 - acc: 0.7642 - val_loss: 0.7786 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.59283\n",
      "Epoch 68/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5260 - acc: 0.7644 - val_loss: 0.5963 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.59283\n",
      "Epoch 69/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5238 - acc: 0.7665 - val_loss: 0.7270 - val_acc: 0.6852\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.59283\n",
      "Epoch 70/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5275 - acc: 0.7660 - val_loss: 0.6992 - val_acc: 0.6540\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.59283\n",
      "Epoch 71/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5264 - acc: 0.7646 - val_loss: 0.5228 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.59283 to 0.52281, saving model to saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5\n",
      "Epoch 72/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5267 - acc: 0.7667 - val_loss: 0.6191 - val_acc: 0.7495\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.52281\n",
      "Epoch 73/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5252 - acc: 0.7642 - val_loss: 0.7371 - val_acc: 0.6448\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.52281\n",
      "Epoch 74/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5243 - acc: 0.7666 - val_loss: 0.7180 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.52281\n",
      "Epoch 75/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5227 - acc: 0.7671 - val_loss: 0.7996 - val_acc: 0.6002\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.52281\n",
      "Epoch 76/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5245 - acc: 0.7663 - val_loss: 0.7193 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.52281\n",
      "Epoch 77/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5235 - acc: 0.7674 - val_loss: 0.6161 - val_acc: 0.7562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00077: val_loss did not improve from 0.52281\n",
      "Epoch 78/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5231 - acc: 0.7691 - val_loss: 0.6548 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.52281\n",
      "Epoch 79/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5230 - acc: 0.7693 - val_loss: 0.7324 - val_acc: 0.6674\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.52281\n",
      "Epoch 80/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5214 - acc: 0.7694 - val_loss: 0.6201 - val_acc: 0.7318\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.52281\n",
      "Epoch 81/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.5234 - acc: 0.7676 - val_loss: 0.5990 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.52281\n",
      "Epoch 82/100\n",
      "65512/65512 [==============================] - 5s 82us/step - loss: 0.5218 - acc: 0.7703 - val_loss: 0.6229 - val_acc: 0.7386\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.52281\n",
      "Epoch 83/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.5227 - acc: 0.7703 - val_loss: 0.6276 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.52281\n",
      "Epoch 84/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.5213 - acc: 0.7703 - val_loss: 0.6918 - val_acc: 0.6849\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.52281\n",
      "Epoch 85/100\n",
      "65512/65512 [==============================] - 5s 82us/step - loss: 0.5235 - acc: 0.7699 - val_loss: 0.5996 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.52281\n",
      "Epoch 86/100\n",
      "65512/65512 [==============================] - 6s 86us/step - loss: 0.5207 - acc: 0.7699 - val_loss: 0.7064 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.52281\n",
      "Epoch 87/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5218 - acc: 0.7678 - val_loss: 0.7238 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.52281\n",
      "Epoch 88/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5238 - acc: 0.7687 - val_loss: 0.7074 - val_acc: 0.6483\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.52281\n",
      "Epoch 89/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5225 - acc: 0.7696 - val_loss: 0.7685 - val_acc: 0.6694\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.52281\n",
      "Epoch 90/100\n",
      "65512/65512 [==============================] - 5s 83us/step - loss: 0.5223 - acc: 0.7701 - val_loss: 0.6151 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.52281\n",
      "Epoch 91/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5190 - acc: 0.7694 - val_loss: 0.7247 - val_acc: 0.6637\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.52281\n",
      "Epoch 92/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5201 - acc: 0.7678 - val_loss: 0.6073 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.52281\n",
      "Epoch 93/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5201 - acc: 0.7692 - val_loss: 0.7311 - val_acc: 0.6671\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.52281\n",
      "Epoch 94/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5200 - acc: 0.7693 - val_loss: 0.5265 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.52281\n",
      "Epoch 95/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5201 - acc: 0.7716 - val_loss: 0.6551 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.52281\n",
      "Epoch 96/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5179 - acc: 0.7709 - val_loss: 0.6499 - val_acc: 0.6921\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.52281\n",
      "Epoch 97/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5185 - acc: 0.7735 - val_loss: 0.6150 - val_acc: 0.7753\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.52281\n",
      "Epoch 98/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5193 - acc: 0.7739 - val_loss: 0.5879 - val_acc: 0.7884\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.52281\n",
      "Epoch 99/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5200 - acc: 0.7707 - val_loss: 0.6508 - val_acc: 0.7352\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.52281\n",
      "Epoch 100/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5201 - acc: 0.7720 - val_loss: 0.8286 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.52281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b92b5cdef0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nullsonly_DEFAULT.fit(Xtrain_nullsonly, Ytrain_nullsonly, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_nullsonly_DEFAULT.load_weights('saved_models/weights.best.mlp_nullsonly_DEFAULT.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 73.4891%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_nullsonly_DEFAULT.evaluate(Xtest_nullsonly, Ytest_nullsonly, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6082.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_nullsonly_DEFAULT= mlp_nullsonly_DEFAULT.predict(Xtest_nullsonly)\n",
    "mlp_nullsonly_ROC_DEFAULT = roc_auc_score(Ytest_nullsonly, Ypred_nullsonly_DEFAULT)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nullsonly_ROC_DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Nulls Only Final Model\n",
    "This is the only model in which the the higher learning rate resulted in a higher AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for Nulls Only.\n",
    "mlp_nullsonly = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nullsonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the Nulls Only data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nullsonly.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nullsonly.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45858 samples, validate on 11465 samples\n",
      "Epoch 1/100\n",
      "45858/45858 [==============================] - 8s 169us/step - loss: 0.7686 - acc: 0.5644 - val_loss: 0.7395 - val_acc: 0.6666\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73955, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 2/100\n",
      "45858/45858 [==============================] - 4s 91us/step - loss: 0.7558 - acc: 0.6195 - val_loss: 0.7317 - val_acc: 0.6829\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73955 to 0.73169, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 3/100\n",
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.7489 - acc: 0.6370 - val_loss: 0.7235 - val_acc: 0.6867\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.73169 to 0.72352, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 4/100\n",
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.7402 - acc: 0.6434 - val_loss: 0.7162 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.72352 to 0.71625, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 5/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.7370 - acc: 0.6483 - val_loss: 0.7091 - val_acc: 0.6911\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.71625 to 0.70908, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 6/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.7340 - acc: 0.6486 - val_loss: 0.7041 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.70908 to 0.70407, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 7/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.7296 - acc: 0.6511 - val_loss: 0.6996 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.70407 to 0.69960, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 8/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.7250 - acc: 0.6523 - val_loss: 0.6951 - val_acc: 0.6925\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.69960 to 0.69510, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 9/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.7264 - acc: 0.6518 - val_loss: 0.6923 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69510 to 0.69234, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 10/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.7219 - acc: 0.6550 - val_loss: 0.6902 - val_acc: 0.6926\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.69234 to 0.69018, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 11/100\n",
      "45858/45858 [==============================] - 5s 102us/step - loss: 0.7191 - acc: 0.6552 - val_loss: 0.6884 - val_acc: 0.6931\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.69018 to 0.68838, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 12/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.7195 - acc: 0.6551 - val_loss: 0.6868 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.68838 to 0.68675, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 13/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.7174 - acc: 0.6563 - val_loss: 0.6850 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.68675 to 0.68502, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 14/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.7121 - acc: 0.6593 - val_loss: 0.6830 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.68502 to 0.68299, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 15/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.7127 - acc: 0.6596 - val_loss: 0.6817 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.68299 to 0.68174, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 16/100\n",
      "45858/45858 [==============================] - 4s 98us/step - loss: 0.7112 - acc: 0.6585 - val_loss: 0.6805 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.68174 to 0.68055, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 17/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.7083 - acc: 0.6612 - val_loss: 0.6792 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.68055 to 0.67916, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 18/100\n",
      "45858/45858 [==============================] - 5s 100us/step - loss: 0.7086 - acc: 0.6603 - val_loss: 0.6784 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.67916 to 0.67836, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 19/100\n",
      "45858/45858 [==============================] - 5s 105us/step - loss: 0.7095 - acc: 0.6613 - val_loss: 0.6776 - val_acc: 0.6947\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.67836 to 0.67763, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 20/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.7059 - acc: 0.6623 - val_loss: 0.6763 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.67763 to 0.67629, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 21/100\n",
      "45858/45858 [==============================] - 4s 91us/step - loss: 0.7055 - acc: 0.6617 - val_loss: 0.6755 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.67629 to 0.67548, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 22/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.7042 - acc: 0.6608 - val_loss: 0.6750 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.67548 to 0.67500, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 23/100\n",
      "45858/45858 [==============================] - 4s 98us/step - loss: 0.7007 - acc: 0.6648 - val_loss: 0.6739 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.67500 to 0.67388, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 24/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.7011 - acc: 0.6641 - val_loss: 0.6731 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.67388 to 0.67309, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 25/100\n",
      "45858/45858 [==============================] - 5s 98us/step - loss: 0.7003 - acc: 0.6651 - val_loss: 0.6720 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.67309 to 0.67205, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 26/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.6993 - acc: 0.6673 - val_loss: 0.6712 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.67205 to 0.67124, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 27/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6988 - acc: 0.6671 - val_loss: 0.6699 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.67124 to 0.66992, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 28/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.6972 - acc: 0.6659 - val_loss: 0.6691 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.66992 to 0.66908, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 29/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6953 - acc: 0.6679 - val_loss: 0.6678 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.66908 to 0.66783, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 30/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6955 - acc: 0.6656 - val_loss: 0.6672 - val_acc: 0.6932\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.66783 to 0.66716, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 31/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6950 - acc: 0.6673 - val_loss: 0.6663 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.66716 to 0.66634, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 32/100\n",
      "45858/45858 [==============================] - 4s 97us/step - loss: 0.6944 - acc: 0.6682 - val_loss: 0.6655 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.66634 to 0.66549, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.6945 - acc: 0.6681 - val_loss: 0.6650 - val_acc: 0.6932\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.66549 to 0.66497, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 34/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.6923 - acc: 0.6694 - val_loss: 0.6640 - val_acc: 0.6932\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.66497 to 0.66402, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 35/100\n",
      "45858/45858 [==============================] - 5s 107us/step - loss: 0.6918 - acc: 0.6710 - val_loss: 0.6631 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.66402 to 0.66309, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 36/100\n",
      "45858/45858 [==============================] - 5s 109us/step - loss: 0.6903 - acc: 0.6707 - val_loss: 0.6623 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.66309 to 0.66228, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 37/100\n",
      "45858/45858 [==============================] - 5s 117us/step - loss: 0.6917 - acc: 0.6701 - val_loss: 0.6620 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.66228 to 0.66200, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 38/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.6902 - acc: 0.6706 - val_loss: 0.6611 - val_acc: 0.6934\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.66200 to 0.66115, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 39/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6890 - acc: 0.6709 - val_loss: 0.6605 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.66115 to 0.66049, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 40/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.6893 - acc: 0.6701 - val_loss: 0.6598 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.66049 to 0.65975, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 41/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6860 - acc: 0.6707 - val_loss: 0.6588 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.65975 to 0.65875, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 42/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6874 - acc: 0.6733 - val_loss: 0.6581 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.65875 to 0.65814, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 43/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6849 - acc: 0.6729 - val_loss: 0.6573 - val_acc: 0.6936\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.65814 to 0.65731, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 44/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.6861 - acc: 0.6709 - val_loss: 0.6567 - val_acc: 0.6937\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.65731 to 0.65669, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 45/100\n",
      "45858/45858 [==============================] - 6s 140us/step - loss: 0.6839 - acc: 0.6726 - val_loss: 0.6558 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.65669 to 0.65577, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 46/100\n",
      "45858/45858 [==============================] - 5s 101us/step - loss: 0.6816 - acc: 0.6740 - val_loss: 0.6548 - val_acc: 0.6940\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.65577 to 0.65479, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 47/100\n",
      "45858/45858 [==============================] - 6s 125us/step - loss: 0.6852 - acc: 0.6736 - val_loss: 0.6544 - val_acc: 0.6939\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.65479 to 0.65440, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 48/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.6838 - acc: 0.6733 - val_loss: 0.6537 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.65440 to 0.65373, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 49/100\n",
      "45858/45858 [==============================] - 5s 99us/step - loss: 0.6839 - acc: 0.6725 - val_loss: 0.6534 - val_acc: 0.6941\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.65373 to 0.65344, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 50/100\n",
      "45858/45858 [==============================] - 5s 105us/step - loss: 0.6824 - acc: 0.6725 - val_loss: 0.6528 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.65344 to 0.65280, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 51/100\n",
      "45858/45858 [==============================] - 8s 164us/step - loss: 0.6810 - acc: 0.6753 - val_loss: 0.6522 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.65280 to 0.65223, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 52/100\n",
      "45858/45858 [==============================] - 6s 121us/step - loss: 0.6786 - acc: 0.6753 - val_loss: 0.6512 - val_acc: 0.6949\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.65223 to 0.65122, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 53/100\n",
      "45858/45858 [==============================] - 5s 112us/step - loss: 0.6813 - acc: 0.6754 - val_loss: 0.6508 - val_acc: 0.6946\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.65122 to 0.65085, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 54/100\n",
      "45858/45858 [==============================] - 5s 117us/step - loss: 0.6801 - acc: 0.6756 - val_loss: 0.6501 - val_acc: 0.6950\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.65085 to 0.65005, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 55/100\n",
      "45858/45858 [==============================] - 5s 106us/step - loss: 0.6773 - acc: 0.6771 - val_loss: 0.6490 - val_acc: 0.6959\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.65005 to 0.64900, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 56/100\n",
      "45858/45858 [==============================] - 5s 113us/step - loss: 0.6772 - acc: 0.6771 - val_loss: 0.6485 - val_acc: 0.6962\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.64900 to 0.64851, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 57/100\n",
      "45858/45858 [==============================] - 5s 107us/step - loss: 0.6756 - acc: 0.6783 - val_loss: 0.6475 - val_acc: 0.6965\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.64851 to 0.64748, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 58/100\n",
      "45858/45858 [==============================] - 5s 104us/step - loss: 0.6762 - acc: 0.6780 - val_loss: 0.6466 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.64748 to 0.64665, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 59/100\n",
      "45858/45858 [==============================] - 5s 114us/step - loss: 0.6760 - acc: 0.6781 - val_loss: 0.6462 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.64665 to 0.64616, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 60/100\n",
      "45858/45858 [==============================] - 5s 109us/step - loss: 0.6744 - acc: 0.6783 - val_loss: 0.6455 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.64616 to 0.64547, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 61/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6750 - acc: 0.6773 - val_loss: 0.6450 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.64547 to 0.64503, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 62/100\n",
      "45858/45858 [==============================] - 5s 106us/step - loss: 0.6734 - acc: 0.6776 - val_loss: 0.6443 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.64503 to 0.64433, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 63/100\n",
      "45858/45858 [==============================] - 4s 97us/step - loss: 0.6755 - acc: 0.6784 - val_loss: 0.6440 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.64433 to 0.64400, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 64/100\n",
      "45858/45858 [==============================] - 5s 101us/step - loss: 0.6729 - acc: 0.6786 - val_loss: 0.6431 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.64400 to 0.64314, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.6728 - acc: 0.6797 - val_loss: 0.6422 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.64314 to 0.64221, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 66/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6718 - acc: 0.6789 - val_loss: 0.6417 - val_acc: 0.6965\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.64221 to 0.64165, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 67/100\n",
      "45858/45858 [==============================] - 5s 108us/step - loss: 0.6718 - acc: 0.6778 - val_loss: 0.6410 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.64165 to 0.64100, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 68/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.6721 - acc: 0.6779 - val_loss: 0.6404 - val_acc: 0.6968\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.64100 to 0.64041, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 69/100\n",
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.6705 - acc: 0.6790 - val_loss: 0.6396 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.64041 to 0.63963, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 70/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.6711 - acc: 0.6805 - val_loss: 0.6387 - val_acc: 0.6967\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.63963 to 0.63870, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 71/100\n",
      "45858/45858 [==============================] - 8s 164us/step - loss: 0.6739 - acc: 0.6793 - val_loss: 0.6382 - val_acc: 0.6969\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.63870 to 0.63823, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 72/100\n",
      "45858/45858 [==============================] - 6s 127us/step - loss: 0.6679 - acc: 0.6798 - val_loss: 0.6372 - val_acc: 0.6973\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.63823 to 0.63725, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 73/100\n",
      "45858/45858 [==============================] - 5s 104us/step - loss: 0.6679 - acc: 0.6821 - val_loss: 0.6366 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.63725 to 0.63655, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 74/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6695 - acc: 0.6808 - val_loss: 0.6362 - val_acc: 0.6976\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.63655 to 0.63620, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 75/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6678 - acc: 0.6808 - val_loss: 0.6352 - val_acc: 0.6978\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.63620 to 0.63522, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 76/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6674 - acc: 0.6822 - val_loss: 0.6349 - val_acc: 0.6978\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.63522 to 0.63489, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 77/100\n",
      "45858/45858 [==============================] - 5s 106us/step - loss: 0.6653 - acc: 0.6820 - val_loss: 0.6342 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.63489 to 0.63416, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 78/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6649 - acc: 0.6832 - val_loss: 0.6335 - val_acc: 0.6984\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.63416 to 0.63348, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 79/100\n",
      "45858/45858 [==============================] - 5s 104us/step - loss: 0.6655 - acc: 0.6820 - val_loss: 0.6327 - val_acc: 0.6986\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.63348 to 0.63270, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 80/100\n",
      "45858/45858 [==============================] - 5s 117us/step - loss: 0.6684 - acc: 0.6821 - val_loss: 0.6323 - val_acc: 0.6990\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.63270 to 0.63231, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 81/100\n",
      "45858/45858 [==============================] - 5s 103us/step - loss: 0.6647 - acc: 0.6828 - val_loss: 0.6318 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.63231 to 0.63180, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 82/100\n",
      "45858/45858 [==============================] - 5s 114us/step - loss: 0.6645 - acc: 0.6840 - val_loss: 0.6312 - val_acc: 0.6992\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.63180 to 0.63115, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 83/100\n",
      "45858/45858 [==============================] - 5s 109us/step - loss: 0.6657 - acc: 0.6827 - val_loss: 0.6307 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.63115 to 0.63069, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 84/100\n",
      "45858/45858 [==============================] - 6s 128us/step - loss: 0.6620 - acc: 0.6855 - val_loss: 0.6298 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.63069 to 0.62982, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 85/100\n",
      "45858/45858 [==============================] - 8s 166us/step - loss: 0.6613 - acc: 0.6859 - val_loss: 0.6289 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.62982 to 0.62889, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 86/100\n",
      "45858/45858 [==============================] - 5s 109us/step - loss: 0.6630 - acc: 0.6848 - val_loss: 0.6283 - val_acc: 0.6993\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.62889 to 0.62829, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 87/100\n",
      "45858/45858 [==============================] - 4s 95us/step - loss: 0.6610 - acc: 0.6855 - val_loss: 0.6278 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.62829 to 0.62777, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 88/100\n",
      "45858/45858 [==============================] - 4s 97us/step - loss: 0.6612 - acc: 0.6864 - val_loss: 0.6273 - val_acc: 0.6992\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.62777 to 0.62727, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 89/100\n",
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.6608 - acc: 0.6859 - val_loss: 0.6267 - val_acc: 0.6995\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.62727 to 0.62666, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 90/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6617 - acc: 0.6863 - val_loss: 0.6260 - val_acc: 0.6996\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.62666 to 0.62597, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 91/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.6612 - acc: 0.6870 - val_loss: 0.6256 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.62597 to 0.62560, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 92/100\n",
      "45858/45858 [==============================] - 4s 96us/step - loss: 0.6581 - acc: 0.6866 - val_loss: 0.6251 - val_acc: 0.7003\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.62560 to 0.62507, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 93/100\n",
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.6590 - acc: 0.6879 - val_loss: 0.6246 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.62507 to 0.62464, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 94/100\n",
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.6595 - acc: 0.6857 - val_loss: 0.6241 - val_acc: 0.7004\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.62464 to 0.62408, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 95/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6592 - acc: 0.6874 - val_loss: 0.6233 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.62408 to 0.62329, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 96/100\n",
      "45858/45858 [==============================] - 4s 94us/step - loss: 0.6576 - acc: 0.6893 - val_loss: 0.6226 - val_acc: 0.7000\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.62329 to 0.62262, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.6554 - acc: 0.6878 - val_loss: 0.6219 - val_acc: 0.7002\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.62262 to 0.62191, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 98/100\n",
      "45858/45858 [==============================] - 4s 93us/step - loss: 0.6537 - acc: 0.6889 - val_loss: 0.6212 - val_acc: 0.7009\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.62191 to 0.62122, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 99/100\n",
      "45858/45858 [==============================] - 4s 92us/step - loss: 0.6569 - acc: 0.6888 - val_loss: 0.6205 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.62122 to 0.62055, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n",
      "Epoch 100/100\n",
      "45858/45858 [==============================] - 5s 107us/step - loss: 0.6549 - acc: 0.6909 - val_loss: 0.6200 - val_acc: 0.7016\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.62055 to 0.61995, saving model to saved_models/weights.best.mlp_nullsonly.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efca0269e8>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nullsonly.fit(Xtrain_nullsonly, Ytrain_nullsonly, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_nullsonly.load_weights('saved_models/weights.best.mlp_nullsonly.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 56.9616%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_nullsonly.evaluate(Xtest_nullsonly, Ytest_nullsonly, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6142.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_nullsonly= mlp_nullsonly.predict(Xtest_nullsonly)\n",
    "mlp_nullsonly_ROC = roc_auc_score(Ytest_nullsonly, Ypred_nullsonly)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nullsonly_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGDCAYAAADEegxVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XecVNX5x/HPw8KyLL1L711YhAVFExsiiL0RjI3EaGJi/JmokWg0ETXBgrFriLFGjYINEAso1oiAhbZLr0vvfdn2/P6YYbPiltllZ+/uzPf9eu3LvXfOzP3ewZ1nzjkz95i7IyIiAlAt6AAiIlJ5qCiIiEg+FQUREcmnoiAiIvlUFEREJJ+KgoiI5FNREKlkzOw5M7s7/PvJZpYRdA6JHyoKEjVmtsrMDpjZXjPbGH6RqXNYm+PN7CMz22Nmu8xsspn1PKxNPTN7yMzWhB9rWXi7SYQ53Mw2mVn1Avuqm9lmM/MC+z42s18Ucv/24cfYG/5ZZWajizjWobbvHLb/32b2l0jylpdInluRw6koSLSd7e51gL7AMcAfD91gZoOAD4C3gZZAB2Au8IWZdQy3SQQ+BHoBw4B6wPHANmBgKXLsBM4osD0c2FHKc2kQPpdLgDvMbFgxbY8zsxNK+fjlJpLnVqQwKgpSIdx9I/A+oeJwyH3AC+7+sLvvcfft7v4nYCbwl3CbK4C2wPnunubuee6+2d3vcveppYjwYvixDrkCeKGM5/IlsBA4uphm9wGFDr2Y2Sgz+/ywfW5mnUs6tpndYmbrwu/+F5vZ4GKOX+xze2hoysxuDPeaNpjZz4o47gIzO7vAdg0z22pmfQtrL1WXioJUCDNrTeid+rLwdjKhd/wTCmn+GjAk/PtpwHvuvreYx37CzJ4oIcJbwIlm1sDMGgA/JvQuulQs5ARCPZdvi2n6ONDVzE4r7TGKOXY34DpggLvXBYYCqwppF+lzC3AUUB9oBVwFPG5mDQu53wvAZQW2hwMb3P270p+JVGbVS24ickTeCo/b1wE+Av4c3t+I0JuSDYXcZwNwaL6gMfB1cQdw919HkCMTmAz8BDBgUnhfaWwFHNgIjHb3D0s43j2EegvTS3mcouQCNYGeZrbF3VcV0S7S5xYgGxjj7jnAVDPbC3Qj1KMo6N/A7WZWz913A5cT6n1JjFFPQaLtvPC72pOB7vzvBWkHkAe0KOQ+LQi9AENo7qCwNmXxAqFho7IOHTVx94bu3sPdH4mg/T+B5gWHXY6Euy8DbiA0/LPZzP5jZi0LaRrpcwuwLVwQDtlPqIAffuz1wBfAheGe1hnAS2U5D6ncVBSkQrj7J8BzwAPh7X3Al8DFhTQfQWhyGULvsoeaWe1yiPEZoRfF5sDnJbQ9Yu6eDdwJ3EWod3LIPiD50IaZHVWKx3zZ3X8EtCPUa7m3kDaRPrel9TyhIaSLgS/dfV0ZH0cqMRUFqUgPAUMKTE6OBq40s+vNrK6ZNQx/Ln4QoRdTCA1RrAVeN7PuZlbNzBqb2a1mNrw0B/fQdeLPBs7xoq8ZX93Mkgr81Cj1WX7fi4SGfAp+Umku0MvM+ppZEv+bVC+WmXUzs1PNrCah4akDhIaUChPJc1tabwH9gP+jjJP0UvmpKEiFcfcthF5Mbg9vf05osvQCQmPdqwl9bPVH7r403OYgocnmRcA0YDcwi9Aw1FcAZvaUmT0VYYaF7r6wmCZPEnqxPfTzbOnO8gfHyyU0j9KowL4lwBhCvaClRN5rqQmMJTT8sxFoBtxaxHFLfG7LcC4HgNcJfbz1jbI8hlR+pkV2RCRSZnYH0NXdLyuxsVRJ+vSRiETEzBoR+tjq5UFnkejR8JGIlMjMriY0t/Ouu38adB6JHg0fiYhIPvUUREQkn4qCiIjkq3ITzU2aNPH27dsHHUNEpEr5+uuvt7p705LaVbmi0L59e+bMmRN0DBGRKsXMVkfSTsNHIiKST0VBRETyqSiIiEg+FQUREcmnoiAiIvlUFEREJJ+KgoiI5FNREBGRfCoKIiKSL2pFwcyeMbPNZragiNvNzB4xs2VmNs/M+kUri4iIRCaaPYXn+P66tIc7A+gS/rmG0DKIIiISoKgVhfBCHNuLaXIu8IKHzAQamFmLaOUREamq3J0PFm5kd2Z21I8V5JxCK0IrOR2SEd73A2Z2jZnNMbM5W7ZsqZBwIiKVwaKNu7n06a+45sWveWnmmqgfL8irpFoh+wpdBs7dxwPjAVJTU7VUnIjEvG17D/LgtCW8MmsN9WrVYMy5vfjpwLZRP26QRSEDaFNguzWwPqAsIiKVQnZuHi98uZqHpi9hf1YuVwxqzw2ndaFBcmKFHD/IojAJuM7M/gMcC+xy9w0B5hERCdSMRZu56500VmzZx4ldm3L7mT3o0rxuhWaIWlEws1eAk4EmZpYB/BmoAeDuTwFTgeHAMmA/8LNoZRERqcyWbd7DXVPS+WTJFjo2qc0zo1I5pVszzAobZY+uqBUFd7+khNsd+E20ji8iUtnt3J/FQ9OX8uLM1SQnJvCnM3twxaD2JFYP7jNAVW45ThGRqi4nN4+XZ63hwWlL2H0gm5ED23LjkK40rlMz6GgqCiIiFenzpVsZM2UhSzbtZVDHxtxxdk96tKgXdKx8KgoiIhVg5dZ93PNOOtPTN9G2UTJPXdafob2aBzJvUBwVBRGRKNqdmc1jHy3j2S9WkphQjVuGdednJ7QnqUZC0NEKpaIgIhIFuXnOhDlreeCDxWzbl8XF/Vtz09BuNKubFHS0YqkoiIiUs69WbOPOyWmkbdhNaruGPDtqIL1b1w86VkRUFEREysna7fv527vpTJ2/kVYNavHoJcdwVp8WlW7eoDgqCiIiR2jfwRye+HgZ//xsJQlm/H5IV645sWOlnTcojoqCiEgZ5eU5b3y7jvveW8TmPQc5r29LbjmjOy3q1wo6WpmpKIiIlMHXq3cwZvJC5mbsIqVNA566vD/92jYMOtYRU1EQESmF9TsPcO97i3j7u/U0r1eTB0ekcF7fVlSrVnXmDYqjoiAiEoEDWbn849PlPPXJctzht6d25lcndaJ2zdh6GY2tsxERKWfuzuR5Gxg7NZ31uzI5s3cLRp/RnTaNkoOOFhUqCiIiRZiXsZMxk9OYs3oHvVrW4+8/6cuxHRsHHSuqVBRERA6zeXcm972/mIlfZ9CkTiL3Xtibi/q3ISFG5g2Ko6IgIhKWmZ3Lvz5fyRMzlpGVm8cvT+rIdad0pm5SjaCjVRgVBRGJe+7O+ws3cs/UdNZuP8CQns25bXgP2jepHXS0CqeiICJxLW39bsZMWcjMFdvp1rwuL/3iWE7o3CToWIFRURCRuLR170HGfbCEV2evoX6tGtx13tFcMqAN1ROCWwqzMlBREJG4kpWTx/P/XcUjHy7lQHYuo47vwP8N7kL95PiZNyiOioKIxAV356NFm7n7nXRWbt3Hyd2a8qcze9K5WZ2go1UqKgoiEvOWbtrDmClpfLZ0Kx2b1ubZnw3glG7Ngo5VKakoiEjM2rk/i79PW8K/v1pD7cQE7jirJ5cPakeNOJ83KI6KgojEnJzcPF76ag0PTlvCnsxsLj22Hb8b0pVGtRODjlbpqSiISEz5dMkW7pqSxtLNezmhc2NuP6sn3Y+qF3SsKkNFQURiwoote7nnnXQ+XLSZdo2TGX95f4b0bF6llsKsDFQURKRK23Ugm0c/XMrzX66iZvUE/nhGd0ad0J6a1aveUpiVgYqCiFRJuXnOf2avYdwHS9ixP4sR/dtw09BuNK1bM+hoVZqKgohUOV8u38aYKWmkb9jNwPaNuOPsnhzdqn7QsWKCioKIVBlrtu3nr1PTeW/hRlo1qMXjP+3H8N5Had6gHKkoiEilt/dgDo/PWMa/PltJ9QTjptO78osfdySphuYNypuKgohUWnl5zuvfZHDf+4vZsucgFxzTij8M685R9ZOCjhazVBREpFKas2o7Y6akMS9jF8e0bcD4y/tzTNuGQceKeSoKIlKprNt5gLHvLmLy3PUcVS+Jh37Sl3P7ttS8QQVRURCRSmF/Vg5PfbKC8Z8uxx2uH9yFX53UkeREvUxVJD3bIhIod+ft79Yz9t1FbNydyVl9WjD6jO60bpgcdLS4pKIgIoGZu3Ynd05eyDdrdtK7VX0e/ekxDGjfKOhYcU1FQUQq3Kbdmdz73iLe+GYdTerU5L6L+nBRv9ZUq6Z5g6CpKIhIhcnMzuXpz1bwxMfLycl1rj25E785pTN1auqlqLLQv4SIRJ278+6CjdzzTjrrdh5gaK/m3Da8J20ba96gslFREJGoWrBuF2OmpDFr5Xa6H1WXl68+luM7NQk6lhRBRUFEomLLnoOM+2Axr85ZS8PkRO45/2hGDmhLguYNKjUVBREpVwdzcnnui1U8+tEyMrNzueqEDvx2cBfq16oRdDSJgIqCiJQLd2da2ibumZrO6m37ObV7M247swedmtYJOpqUgoqCiByxxRv3cNeUND5ftpXOzerw/M8HclLXpkHHkjJQURCRMtu+L4u/T1vCS1+tpm5SDf5ydk8uPa4dNRKqBR1NykhFQURKLTs3jxe/XM1D05ewLyuXy49rxw2ndaVh7cSgo8kRUlEQkVL5ePFm7pqSxvIt+/hxlybcflZPujavG3QsKScqCiISkeVb9nL3lDRmLN5C+8bJPH1FKoN7NNMlrWOMioKIFGvX/mwe/nApL3y5ilo1ErhteA+uPL49idU1bxCLoloUzGwY8DCQADzt7mMPu70t8DzQINxmtLtPjWYmEYlMTm4er8xey4MfLGbngWxGDmjLjad3pUmdmkFHkyiKWlEwswTgcWAIkAHMNrNJ7p5WoNmfgNfc/Ukz6wlMBdpHK5OIROaLZVsZMzmNxZv2cGyHRtxxdk96tawfdCypANHsKQwElrn7CgAz+w9wLlCwKDhQL/x7fWB9FPOISAlWb9vHPe+k80HaJlo3rMWTl/Zj2NFHad4gjkSzKLQC1hbYzgCOPazNX4APzOy3QG3gtCjmEZEi7MnM5rEZy3j281VUTzBuHtqNq37UgaQaCUFHkwoWzaJQ2FsLP2z7EuA5dx9nZoOAF83saHfP+94DmV0DXAPQtm3bqIQViUe5ec7Er9dy//tL2Lr3IBf2a80fhnWjeb2koKNJQKJZFDKANgW2W/PD4aGrgGEA7v6lmSUBTYDNBRu5+3hgPEBqaurhhUVEymDWyu2MmbKQBet2079dQ/51ZSopbRoEHUsCFs2iMBvoYmYdgHXASOCnh7VZAwwGnjOzHkASsCWKmUTiXsaO/fzt3UW8M28DLeon8fDIvpyT0lLzBgJEsSi4e46ZXQe8T+jjps+4+0IzGwPMcfdJwI3AP83sd4SGlka5u3oCIlGwPyuHJz9ezvhPV2AGN5zWhV+e2IlaiZo3kP+J6vcUwt85mHrYvjsK/J4GnBDNDCLxLi/Peeu7ddz73iI27T7IOSktGX1Gd1o2qBV0NKmE9I1mkRj2zZodjJmcxndrd9KndX2euLQf/ds1CjqWVGIqCiIxaOOuTO59bxFvfruOZnVr8sDFKVxwTCuqaSlMKYGKgkgMyczOZfynK3jy4+XkuvObUzrx65M7U7um/tQlMvo/RSQGuDtT5m1g7LuLWLfzAGccfRS3Du9Bm0bJQUeTKkZFQaSKW7BuF3dOXsjsVTvo0aIe40akcFzHxkHHkipKRUGkitq8J5MH3l/MhK8zaJScyN8u6M2I1DYkaN5AjoCKgkgVczAnl2c+X8XjM5ZxMCeXq3/cketO7Uy9pBpBR5MYoKIgUkW4O+8v3MRfp6azZvt+TuvRjNvO7EmHJrWDjiYxREVBpApI37CbMZPT+HLFNro2r8OLVw3kx12aBh1LYpCKgkgltm3vQR6ctoRXZq2hXq0ajDm3Fz8d2JbqCVoKU6JDRUGkEsrKyeOFL1fx8IdL2Z+VyxWD2nPDaV1okJwYdDSJcSoKIpXMjEWbuWtKGiu27uPErk25/cwedGleN+hYEidUFEQqiWWb93DXlHQ+WbKFjk1q88yoVE7p1kyXtJYKpaIgErCd+7N4aPpSXpy5muTEBP50Zg+uGNSexOqaN5CKp6IgEpCc3DxenrWGB6ctYfeBbEYObMuNQ7rSuE7NoKNJHFNREAnAZ0u3cNeUNJZs2sugjo254+ye9GhRL+hYIioKIhVp5dZ93PNOOtPTN9G2UTJPXdafob2aa95AKg0VBZEKsDszm8c+WsazX6wkMaEatwzrzs9/1J6a1bUUplQuKgoiUZSb57w2Zy3jPljMtn1ZXNy/NTcN7UazuklBRxMplIqCSJTMXLGNMZPTSNuwm9R2DXl21EB6t64fdCyRYqkoiJSztdv387d305k6fyOtGtTi0UuO4aw+LTRvIFWCioJIOdl3MIcnPl7GPz9bSYIZvx/SlWtO7EhSDc0bSNWhoiByhPLynDe+Xcd97y1i856DnNe3Jbec0Z0W9WsFHU2k1FQURI7A16t3MGbyQuZm7CKlTQOeurw//do2DDqWSJmpKIiUwfqdB7j3vUW8/d16mteryYMjUjivbyuqaSlMqeJUFERK4UBWLv/4dDlPfbIcd/jtqZ351UmdqF1Tf0oSG/R/skgE3J1Jc9dz77uLWL8rkzN7t2D0Gd1p0yg56Ggi5UpFQaQE8zJ2cufkNL5evYNeLevx95/05diOjYOOJRIVKgoih/l86VZ27M/CgY8Xb+aNb9bRpE4i917Ym4v6tyFB8wYSw1QURApYu30/l/3rq/ztGgnGL0/qyHWndKZuUo0Ak4lUDBUFkQIO5uQBcP3gLpyT0oKGyYla30DiioqCxLWc3Dwmz1vPvoO5AGzZcxCAzs3q0LmZ1kWW+KOiIHHt27U7+d2rc3+wv1ld9Q4kPqkoSFzLDg8XPXVZf/q1awBAzYQE6idr/kDik4qCxK2d+7OYumADAA2Ta2iNAxGgWtABRILyzvwN/HvmGmpWr0bzeioIIqCegsSx3DwH4LM/nEIzFQURQD0FiWNT54eGjvRlNJH/UVGQuPX16h0A1ElSh1nkEP01SNxKqp7A5ce1oWZ1rYwmckhERcHMEoG27r4synlEou7jxZt5f+EmMnNyg44iUumUOHxkZmcC84Fp4e2+ZvZmtIOJRMu/Pl/JhDlraVQ7kZQ29YOOI1KpRNJTGAMcC8wAcPfvzKxzVFOJRJE7pLRpwOvXHh90FJFKJ5KikO3uO82+9wkNj1IekXK3bPNeHp+xjKzwt5cXbdxNu8a1A04lUjlFUhTSzWwEUM3MOgD/B8yMbiyR8nEwJ5ffvPQNizftoXOzOgA0SE7klG5NA04mUjlFUhSuA+4A8oA3gPeBP0YzlEh5eXj6UhZv2sOzowZwSvdmQccRqfQiKQpD3f0W4JZDO8zsAkIFQqTS+nbNDp76ZDkjUlurIIhEKJIvr/2pkH23lXcQkfKUmZ3LjRPmclS9JP50Vs+g44hUGUX2FMxsKDAMaGVmDxa4qR6hoSSRSuuB9xezYss+/n3VsdTTMpoiEStu+GgzsADIBBYW2L8HGB3NUCJHYtbK7fzri5VcdlxbftSlSdBxRKqUIouCu38LfGtmL7l7ZgVmEimzfQdzuGnCXNo0TOaPZ/QIOo5IlRPJnEIrM/uPmc0zsyWHfiJ5cDMbZmaLzWyZmRXauzCzEWaWZmYLzezlUqUXOczYdxexdsd+7r+oD7Vr6tJeIqUVyV/Nc8DdwAPAGcDPiGBOwcwSgMeBIUAGMNvMJrl7WoE2XQh9vPUEd99hZvqIiJTZF8u28uLM1Vz1ow4c27Fx0HFEqqRIegrJ7v4+gLsvd/c/AadEcL+BwDJ3X+HuWcB/gHMPa3M18Li77wg//ubIo4v8z57MbP4wcR4dm9bm5qHdgo4jUmVFUhQOWugaF8vN7FdmdjYQyTv6VsDaAtsZ4X0FdQW6mtkXZjbTzIYV9kBmdo2ZzTGzOVu2bIng0BJv7p6SzoZdBxh3cQpJNXQpbJGyiqQo/A6oA1wPnEDo3f3PI7hfYctZHX7NpOpAF+Bk4BLgaTNr8IM7uY9391R3T23aVJcnkO+bsWgzr85Zyy9P6sQxbRsGHUekSitxTsHdvwr/uge4HMDMWkfw2BlAmwLbrYH1hbSZ6e7ZwEozW0yoSMyO4PFF2LU/m9FvzKNb87rccFqXoOOIVHnF9hTMbICZnWdmTcLbvczsBSK7IN5soIuZdQgv0jMSmHRYm7cIz0+Ej9EVWFHKc5A49pfJC9m2N4txI1K0gppIOSiyKJjZ34CXgEuB98zsNkJrKswl9OJdLHfPIXQxvfeBdOA1d19oZmPM7Jxws/eBbWaWFn7sm91925GckMSP9xZs5M1v1/GbUzpzdCstliNSHsy98KURwi/U/d39gJk1IjT0k+Luiysy4OFSU1N9zpw5QUaQSmDb3oOc/vdPOap+Em/95gRqJEQyPSYSv8zsa3dPLaldcX9Jme5+AMDdtwOLgi4IIgDuzu1vL2B3ZjbjRqSoIIiUo+Immjua2aHLYxvQvsA27n5BVJOJFGHyvA1Mnb+Rm4d2o/tR9YKOIxJTiisKFx62/Vg0g4hEYvOeTO54ewF92zTglyd2DDqOSMwp7oJ4H1ZkEJGSuDu3vjGfA1m5jBuRQnUNG4mUO/1VSZXx+jfrmJ6+mZuHdqNT0zpBxxGJSSoKUiVs2HWAOycvZGD7Rvz8hA5BxxGJWREXBTOrGc0gIkVxd/4wcR45uc79F/ehWrXCrqAiIuWhxKJgZgPNbD6wNLydYmaPRj2ZSNgrs9by2dKt3Dq8O+0a1w46jkhMi6Sn8AhwFrANwN3nEtmls0WO2Nrt+7n7nTRO6NyYS49tF3QckZgXSVGo5u6rD9uXG40wIgXl5Tk3TZhLNTPuuyhFw0YiFSCSldfWmtlAwMOrqf0WiGg5TpEj8fyXq/hq5XbuvbA3rRrUCjqOSFyIpKdwLfB7oC2wCTguvE8kalZs2cu97y3ilG5NGZHapuQ7iEi5iKSnkOPuI6OeRCQsNzxsVLN6AmMv7ENo4T8RqQiR9BRmm9lUM7vSzOpGPZHEvac/W8E3a3Zy5zm9aF4vKeg4InGlxKLg7p2Au4H+wHwze8vM1HOQqFi6aQ/jpi1haK/mnNu3ZdBxROJORF9ec/f/uvv1QD9gN6HFd0TKVU5uHjdOmEudmtW55/zeGjYSCUAkX16rY2aXmtlkYBawBTg+6skk7jz58XLmZezi7vOOpkkdfYFeJAiRTDQvACYD97n7Z1HOI3Fq4fpdPPzhUs5Oacnw3i2CjiMStyIpCh3dPS/qSSRuZeXkceNrc2lYO5Ex5/QKOo5IXCuyKJjZOHe/EXjdzH6wkLNWXpPy8siHS1m0cQ9PX5FKw9qJQccRiWvF9RReDf9XK65J1Mxdu5MnP1nOhf1ac1rP5kHHEYl7xa28Niv8aw93/15hMLPrAK3MJkckMzuXGyfMpWmdmtxxds+g44gIkX0k9eeF7LuqvINI/Hlw2hKWbd7LfRf1oX6tGkHHERGKn1P4CTAS6GBmbxS4qS6wM9rBJLbNWbWdf362gp8e25YTuzYNOo6IhBU3pzCL0BoKrYHHC+zfA3wbzVAS2/Zn5XDThLm0alCLW4f3CDqOiBRQ3JzCSmAlML3i4kg8uO+9xazatp9Xrj6OOjUj+VS0iFSU4oaPPnH3k8xsB1DwI6kGuLs3ino6iTn/Xb6V5/67ilHHt2dQp8ZBxxGRwxT3Nu3QkptNKiKIxL49mdncPGEeHZrU5pZh3YOOIyKFKPLTRwW+xdwGSHD3XGAQ8EtAq6dLqf11ajobdh3ggYv7UCsxIeg4IlKISD6S+hahpTg7AS8APYCXo5pKYs7Hizfzyqy1XP3jjvRvp5FHkcoqkqKQ5+7ZwAXAQ+7+W6BVdGNJLNl1IJvRr8+nS7M6/G5I16DjiEgxIikKOWZ2MXA5MCW8T980kojdOXkhW/YeZNyIFJJqaNhIpDKL9BvNpxC6dPYKM+sAvBLdWBIrpqVt4o1v1vGbkzvRp3WDoOOISAlK/JC4uy8ws+uBzmbWHVjm7vdEP5pUdTv2ZfHHN+bTs0U9rju1S9BxRCQCJRYFM/sx8CKwjtB3FI4ys8vd/Ytoh5Oq7fa3F7DrQBYvXjWQxOoRrfwqIgGL5OukfweGu3sagJn1IFQkUqMZTKq2KfPWM2XeBm46vSs9WtQLOo6IRCiSt2+JhwoCgLunA1oJRYq0Zc9Bbn9rASmt6/OrkzoFHUdESiGSnsI3ZvYPQr0DgEvRBfGkCO7OrW/OZ19WLuNGpFA9QcNGIlVJJH+xvwKWA38AbgFWEPpWs8gPvPntOqalbeKm07vSuVndoOOISCkV21Mws95AJ+BNd7+vYiJJVbVxVyZ/nrSQ1HYNuepHHYOOIyJlUGRPwcxuJXSJi0uBaWZW2ApsIkBo2OiW1+eRnZvH/RenkFDNgo4kImVQXE/hUqCPu+8zs6bAVOCZioklVc2rs9fyyZIt/OXsnnRoouslilRVxc0pHHT3fQDuvqWEthLHMnbs5+530hnUsTFXDGofdBwROQLF9RQ6Flib2YBOBddqdvcLoppMqoS8POcPE+fh7tx3UR+qadhIpEorrihceNj2Y9EMIlXTv79azX+Xb+NvF/SmTaPkoOOIyBEqbo3mDysyiFQ9q7bu429TF3FS16aMHNAm6DgiUg40TyBlkpvn3DRhLtUTjLEX9sZMw0YisSCSbzSL/MAzn69kzuodjLs4hRb1awUdR0TKScQ9BTOrGc0gUnUs27yH+z9YzGk9mnNBPy3CJxJLSiwKZjbQzOYDS8PbKWb2aNSTSaWUk5vHja/NJTkxgb9ecLSGjURiTCQ9hUeAs4BtAO4+l9BKbCUys2FmttjMlpnZ6GLaXWRmbma6HHcl949PVzA3Yxd3nXs0zeomBR1HRMpZJEWhmruvPmxfbkl3MrME4HHgDKAncImZ9SykXV3geuCrCLJIgNI37Oah6Us4s08Lzk5pGXQcEYmCSIrCWjMbCLiZJZjZDcCSCO43kNDSnSvcPQv4D3BuIe3uAu4DMiMNLRUvKyc0bFS/Vg3uOvfooOOISJREUhSuBX4PtAU2AceF95WkFbC2wHaE/DWxAAAaxUlEQVRGeF8+MzsGaOPuU4p7IDO7xszmmNmcLVu2RHBoKW+PzVhG2obd/PX83jSqrTWWRGJViR9JdffNwMgyPHZhM5Cef6NZNUJLfY6KIMN4YDxAamqql9Bcytn8jF08PmMZFxzTitN7HRV0HBGJohKLgpn9kwIv5oe4+zUl3DUDKPg119bA+gLbdYGjgY/Dn2A5CphkZue4+5yScknFyMzO5fevfUeTOon8+exeQccRkSiL5Mtr0wv8ngScz/eHhYoyG+hiZh2AdYR6Gz89dKO77wKaHNo2s4+Bm1QQKpe/T1/C0s17ee5nA6ifXCPoOCISZZEMH71acNvMXgSmRXC/HDO7DngfSACecfeFZjYGmOPuk8qYWSrI16t38M9PVzByQBtO7tYs6DgiUgHKcpmLDkC7SBq6+1RCi/MU3HdHEW1PLkMWiZIDWbncNGEuLerX4rYzewQdR0QqSCRzCjv435xCNWA7UOQX0SQ23Pf+IlZu3cfLvziWukkaNhKJF8UWBQvNAKcQmhMAyHN3ffonxs1csY1nv1jFFYPacXznJiXfQURiRrHfUwgXgDfdPTf8o4IQ4/YdzOHmiXNp1ziZ0Wd0DzqOiFSwSL68NsvM+kU9iVQKf52aTsaOA4y7OIXkRF1ZXSTeFPlXb2bV3T0H+BFwtZktB/YR+lKau7sKRYz5dMkWXvpqDVf/uAOp7RsFHUdEAlDcW8FZQD/gvArKIgHadSCbW16fR6emtbnx9G5BxxGRgBRXFAzA3ZdXUBYJ0F1T0ti0O5M3fn0CSTUSgo4jIgEprig0NbPfF3Wjuz8YhTwSgA/TNzHx6wx+c0on+rZpEHQcEQlQcUUhAahD4Re2kxixY18Wo9+YT/ej6nL94C5BxxGRgBVXFDa4+5gKSyKB+POkhezYl8VzPxtAzeoaNhKJd8V9JFU9hBj37vwNTJq7nt+e2oVeLesHHUdEKoHiisLgCkshFW7r3oPc9tYCereqz69P6RR0HBGpJIosCu6+vSKDSMVxd/705gL2ZuYwbkQKNRIi+Q6jiMQDvRrEoUlz1/Pewo38/vSudG1eN+g4IlKJqCjEmU27M7nj7YX0a9uAq3/cMeg4IlLJqCjEEXdn9OvzOJiTywMXp5BQTZ8lEJHvU1GIIxPmZDBj8Rb+MLQ7HZvWCTqOiFRCKgpxYt3OA4yZksaxHRox6vj2QccRkUpKRSEOuDu3TJxHnjv3X5RCNQ0biUgRVBTiwL+/WsPny7Zy6/AetG2cHHQcEanEVBRi3Jpt+/nb1HR+3KUJlx7bNug4IlLJqSjEsLw856aJc0kw494L+xBacltEpGgqCjHs2f+uYtbK7dxxdk9aNqgVdBwRqQJUFGLU8i17ue+9RQzu3oyL+rcOOo6IVBEqCjEoJzePG1+bS1KNBP52QW8NG4lIxIpbT0GqqPGfreC7tTt5eGRfmtVLCjqOiFQh6inEmMUb9/DQtKWccfRRnJPSMug4IlLFqCjEkOzcPH7/2nfUTarO3ecdrWEjESk1DR/FkMdnLGPh+t08dVk/GtepGXQcEamC1FOIEQvW7eKxj5Zxbt+WDDu6RdBxRKSKUlGIAQdzcrnxtbk0qp3Inef0CjqOiFRhGj6KAQ9PX8riTXt4ZlQqDZITg44jIlWYegpV3LdrdvDUJ8sZkdqaU7s3DzqOiFRxKgpVWGZ2LjdOmMtR9ZL401k9g44jIjFAw0dV2P3vL2bFln38+6pjqZdUI+g4IhID1FOoomat3M4zX6zksuPa8qMuTYKOIyIxQkWhCtp3MIebJsylTcNk/nhGj6DjiEgM0fBRFTT23UWs3bGf/1x9HLVr6p9QRMqPegpVzBfLtvLizNX87PgOHNuxcdBxRCTGqChUIXsys/nDxHl0bFKbPwzrFnQcEYlBGnuoQu6eks6GXQeYeO3xJNVICDqOiMQg9RSqiBmLNvPqnLVcc2In+rVtGHQcEYlRKgpVwM79Wdzy+jy6Nq/D74Z0CTqOiMQwDR9VAX+ZtJDt+7J4ZtQAalbXsJGIRI96CpXcews28tZ36/nNKZ05ulX9oOOISIxTUajEtu09yG1vzqdXy3pcd2rnoOOISBzQ8FEl5e7c/vYCdmdm89LVx1IjQfVbRKJPrzSV1OR5G5g6fyM3nNaV7kfVCzqOiMQJFYVKaPOeTO54ewEpbRrwyxM7Bh1HROKIikIl4+7c+sZ8DmTlMu7iFKpr2EhEKlBUX3HMbJiZLTazZWY2upDbf29maWY2z8w+NLN20cxTFbz+zTqmp2/m5qHd6NysTtBxRCTORK0omFkC8DhwBtATuMTMDl8e7Fsg1d37ABOB+6KVpypYv/MAd05eyID2DfnZCR2CjiMicSiaPYWBwDJ3X+HuWcB/gHMLNnD3Ge6+P7w5E2gdxTyVmrtzy+vzyMl1Hrg4hYRqFnQkEYlD0SwKrYC1BbYzwvuKchXwbhTzVGovz1rDZ0u3cuvw7rRrXDvoOCISp6L5PYXC3up6oQ3NLgNSgZOKuP0a4BqAtm3blle+SmPt9v3c8046J3RuzKXHxv20iogEKJo9hQygTYHt1sD6wxuZ2WnAbcA57n6wsAdy9/HunuruqU2bNo1K2KDk5Tk3TZhLNTPuuyiFaho2EpEARbMozAa6mFkHM0sERgKTCjYws2OAfxAqCJujmKXSev7LVXy1cju3n9WDVg1qBR1HROJc1IqCu+cA1wHvA+nAa+6+0MzGmNk54Wb3A3WACWb2nZlNKuLhYtKKLXu5971FnNKtKSNS25R8BxGRKIvqtY/cfSow9bB9dxT4/bRoHr8yyw0PGyUmVGPshX0w07CRiARPF8QLyNOfreCbNTv5+09SaF4vKeg4IiKALnMRiCWb9jDugyWc3rM55/Ut7lO6IiIVS0WhgmXn5nHja3Opk1Sde87vrWEjEalUNHxUwZ78eDnz1+3iiUv70bRuzaDjiIh8j3oKFWjh+l088uFSzk5pyfDeLYKOIyLyAyoKFSQrJzRs1LB2ImPO6RV0HBGRQmn4qII88uFSFm3cw9NXpNKwdmLQcURECqWeQgWYu3YnT36ynAv7tea0ns2DjiMiUiQVhSjLzM7lxglzaVqnJnecffhyEiIilYuGj6LswWlLWLZ5L8//fCD1a9UIOo6ISLHUU4iiOau288/PVnDJwLac1DW2ru4qIrFJRSFK9mflcNOEubRqUIvbzuwRdBwRkYho+ChK7n13Eau27eeVq4+jTk09zSJSNainEAX/XbaV579czajj2zOoU+Og44iIRExFoZztyczm5onz6NCkNrcM6x50HBGRUtG4Rjn769R0Nuw6wIRfDaJWYkLQcURESkU9hXL08eLNvDJrLVf/uCP92zUKOo6ISKmpKJSTXQeyGf36fLo0q8PvhnQNOo6ISJlo+Kic3Dl5IVv2HmT8Ff1JqqFhIxGpmtRTKAfT0jbxxjfr+PXJnejTukHQcUREykxF4Qht35fFH9+YT48W9fjtqV2CjiMickQ0fHSEbn97AbsOZPHCzweSWF01VkSqNr2KHYEp89bzzrwN/N/gLvRsWS/oOCIiR0xFoYy27DnI7W8tIKV1fX51Uqeg44iIlAsVhTJwd259cz77snIZNyKF6gl6GkUkNujVrAze/HYd09I2cdPpXencrG7QcUREyo2KQilt3JXJnyctJLVdQ676Uceg44iIlCsVhVJwd255fR7ZuXncf3EKCdUs6EgiIuVKRaEUXp29lk+WbGH0sO50aFI76DgiIuVORSFCa7fv564paQzq2JgrBrUPOo6ISFSoKEQgL8/5w8R5ANx3UR+qadhIRGKUikIEXpy5mi9XbOO2M3vSplFy0HFERKJGRaEEq7buY+y7izixa1MuGdgm6DgiIlGlolCM3DznpglzqZ5g3Hthb8w0bCQisU0XxCvGM5+vZM7qHYy7OIUW9WsFHUdEJOrUUyjCss17uP+DxZzWozkX9GsVdBwRkQqholCInNw8bnxtLsmJCfz1gqM1bCQicUPDR4X4x6crmJuxi0cvOYZmdZOCjiMiUmHUUzhM+obdPDR9CWf2bsHZKS2DjiMiUqFUFArIygkNG9WvVYO7zjs66DgiIhVOw0cFPPbRUtI27OYfl/enUe3EoOOIiFQ49RTC5mXs5PGPl3P+Ma0Y2uuooOOIiARCRQHIzM7lxtfm0qROIn85u1fQcUREAqPhI+Dv05ewdPNenvvZAOon1wg6johIYOK+p/D16h3889MVjBzQhpO7NQs6johIoOK6KBzIyuWmCXNpUb8Wt53ZI+g4IiKBi+vho/veX8TKrft4+RfHUjdJw0Yi5Sk7O5uMjAwyMzODjhJXkpKSaN26NTVqlO01LW6LwswV23j2i1VcMagdx3duEnQckZiTkZFB3bp1ad++vS4VU0HcnW3btpGRkUGHDh3K9BhxOXy092AON0+cS7vGyYw+o3vQcURiUmZmJo0bN1ZBqEBmRuPGjY+odxaXPYW/Tk0nY8cBXvvlIJIT4/IpEKkQKggV70if87jrKXy6ZAsvf7WGq07owID2jYKOIyJV2Pbt2xkyZAhdunRhyJAh7Nixo9B2a9as4fTTT6dHjx707NmTVatWAfDYY4/RuXNnzIytW7f+4H6zZ88mISGBiRMnAvDdd98xaNAgevXqRZ8+fXj11VfL/ZyiWhTMbJiZLTazZWY2upDba5rZq+HbvzKz9tHMs+tANre8Po9OTWtz09Bu0TyUiMSBsWPHMnjwYJYuXcrgwYMZO3Zsoe2uuOIKbr75ZtLT05k1axbNmoU+/n7CCScwffp02rVr94P75ObmcssttzB06ND8fcnJybzwwgssXLiQ9957jxtuuIGdO3eW6zlFrSiYWQLwOHAG0BO4xMx6HtbsKmCHu3cG/g7cG608AHdNSWPT7kzGjehLUo2EaB5KRCqB8847j/79+9OrVy/Gjx+fv79OnTr5v0+cOJFRo0YBsGnTJs4//3xSUlJISUnhv//9b7GP//bbb3PllVcCcOWVV/LWW2/9oE1aWho5OTkMGTIk/9jJyckAHHPMMbRv377Qx3700Ue58MIL8wsIQNeuXenSpQsALVu2pFmzZmzZsqWEZ6F0ojmgPhBY5u4rAMzsP8C5QFqBNucCfwn/PhF4zMzM3b28w3yYvomJX2fwm1M60bdNg/J+eBEpxp2TF5K2fne5PmbPlvX4cwmXpXnmmWdo1KgRBw4cYMCAAVx44YU0bty4yPbXX389J510Em+++Sa5ubns3bsXgOHDh/P000/TsuX3L6e/adMmWrRoAUCLFi3YvHnzDx5zyZIlNGjQgAsuuICVK1dy2mmnMXbsWBISin5jum7dOt58800++ugjZs+eXWibWbNmkZWVRadOnYp9DkormsNHrYC1BbYzwvsKbePuOcAu4Af/YmZ2jZnNMbM5Za2KOXnOoI6NuX5wlzLdX0SqnkceeYSUlBSOO+441q5dy9KlS4tt/9FHH3HttdcCkJCQQP369QGYOnXqDwpCpHJycvjss8944IEHmD17NitWrOC5554r9j433HAD9957b5GFY8OGDVx++eU8++yzVKtWvi/j0ewpFDYFfngPIJI2uPt4YDxAampqmXoRQ3sdxek9m+vTECIBKOkdfTR8/PHHTJ8+nS+//JLk5GROPvnk/I9qFnwdOJKPbzZv3pwNGzbQokULNmzY8L2hnkNat27NMcccQ8eOHYHQkNbMmTO56qqrinzcOXPmMHLkSAC2bt3K1KlTqV69Oueddx67d+/mzDPP5O677+a4444rc/aiRLOnkAG0KbDdGlhfVBszqw7UB7ZHK5AKgkj82LVrFw0bNiQ5OZlFixYxc+bM/NuaN29Oeno6eXl5vPnmm/n7Bw8ezJNPPgmEJnp37y5+yOucc87h+eefB+D555/n3HPP/UGbAQMGsGPHjvyx/48++oiePQ+fXv2+lStXsmrVKlatWsVFF13EE088wXnnnUdWVhbnn38+V1xxBRdffHFkT0QpRbMozAa6mFkHM0sERgKTDmszCbgy/PtFwEfRmE8QkfgzbNgwcnJy6NOnD7fffvv33lWPHTuWs846i1NPPTV/TgDg4YcfZsaMGfTu3Zv+/fuzcOFCIDSnsH794e9pYfTo0UybNo0uXbowbdo0Ro8Ofchyzpw5/OIXvwBCw1APPPAAgwcPpnfv3rg7V199NRAa3mrdujUZGRn06dMn/z5Fee211/j000957rnn6Nu3L3379uW77747sifqMBbN12AzGw48BCQAz7j7PWY2Bpjj7pPMLAl4ETiGUA9h5KGJ6aKkpqb6nDlzopZZRMpHeno6PXroQpNBKOy5N7Ov3T21pPtG9eu87j4VmHrYvjsK/J4JRKcPJCIipRZ332gWEZGiqSiIiEg+FQURiRp9bqTiHelzrqIgIlGRlJTEtm3bVBgq0KH1FJKSksr8GLputIhExaGPWpb3tXmkeIdWXisrFQURiYoaNWqUefUvCY6Gj0REJJ+KgoiI5FNREBGRfFG9zEU0mNkWYHUZ794E+OGad7FN5xwfdM7x4UjOuZ27Ny2pUZUrCkfCzOZEcu2PWKJzjg865/hQEees4SMREcmnoiAiIvnirSiML7lJzNE5xwedc3yI+jnH1ZyCiIgUL956CiIiUoyYLApmNszMFpvZMjMbXcjtNc3s1fDtX5lZ+4pPWb4iOOffm1mamc0zsw/NrF0QOctTSedcoN1FZuZmVuU/qRLJOZvZiPC/9UIze7miM5a3CP7fbmtmM8zs2/D/38ODyFlezOwZM9tsZguKuN3M7JHw8zHPzPqVawB3j6kfQkt/Lgc6AonAXKDnYW1+DTwV/n0k8GrQuSvgnE8BksO/XxsP5xxuVxf4FJgJpAaduwL+nbsA3wINw9vNgs5dAec8Hrg2/HtPYFXQuY/wnE8E+gELirh9OPAuYMBxwFflefxY7CkMBJa5+wp3zwL+A5x7WJtzgefDv08EBpuZVWDG8lbiObv7DHffH96cCZT9MoqVQyT/zgB3AfcBmRUZLkoiOeergcfdfQeAu2+u4IzlLZJzdqBe+Pf6wPoKzFfu3P1TQmvWF+Vc4AUPmQk0MLMW5XX8WCwKrYC1BbYzwvsKbePuOcAuoHGFpIuOSM65oKsIvdOoyko8ZzM7Bmjj7lMqMlgURfLv3BXoamZfmNlMMxtWYemiI5Jz/gtwmZllEFoT/rcVEy0wpf17L5VYvHR2Ye/4D/+IVSRtqpKIz8fMLgNSgZOimij6ij1nM6sG/B0YVVGBKkAk/87VCQ0hnUyoN/iZmR3t7jujnC1aIjnnS4Dn3H2cmQ0CXgyfc1704wUiqq9fsdhTyADaFNhuzQ+7k/ltzKw6oS5ncd21yi6Sc8bMTgNuA85x94MVlC1aSjrnusDRwMdmtorQ2OukKj7ZHOn/22+7e7a7rwQWEyoSVVUk53wV8BqAu38JJBG6RlCsiujvvaxisSjMBrqYWQczSyQ0kTzpsDaTgCvDv18EfOThGZwqqsRzDg+l/INQQajq48xQwjm7+y53b+Lu7d29PaF5lHPcfU4wcctFJP9vv0XoQwWYWRNCw0krKjRl+YrknNcAgwHMrAehohDLy71NAq4IfwrpOGCXu28orwePueEjd88xs+uA9wl9cuEZd19oZmOAOe4+CfgXoS7mMkI9hJHBJT5yEZ7z/UAdYEJ4Tn2Nu58TWOgjFOE5x5QIz/l94HQzSwNygZvdfVtwqY9MhOd8I/BPM/sdoWGUUVX5TZ6ZvUJo+K9JeJ7kz0ANAHd/itC8yXBgGbAf+Fm5Hr8KP3ciIlLOYnH4SEREykhFQURE8qkoiIhIPhUFERHJp6IgIiL5VBSk0jGzXDP7rsBP+2Lati/qapKlPObH4Stxzg1fIqJbGR7jV2Z2Rfj3UWbWssBtT5tZz3LOOdvM+kZwnxvMLPlIjy3xQUVBKqMD7t63wM+qCjrupe6eQuhiifeX9s7u/pS7vxDeHAW0LHDbL9w9rVxS/i/nE0SW8wZARUEioqIgVUK4R/CZmX0T/jm+kDa9zGxWuHcxz8y6hPdfVmD/P8wsoYTDfQp0Dt93cPg6/fPD17mvGd4/1v63PsUD4X1/MbObzOwiQteXeil8zFrhd/ipZnatmd1XIPMoM3u0jDm/pMCF0MzsSTObY6F1FO4M77ueUHGaYWYzwvtON7Mvw8/jBDOrU8JxJI6oKEhlVKvA0NGb4X2bgSHu3g/4CfBIIff7FfCwu/cl9KKcEb7swU+AE8L7c4FLSzj+2cB8M0sCngN+4u69CV0B4FozawScD/Ry9z7A3QXv7O4TgTmE3tH3dfcDBW6eCFxQYPsnwKtlzDmM0GUtDrnN3VOBPsBJZtbH3R8hdF2cU9z9lPClL/4EnBZ+LucAvy/hOBJHYu4yFxITDoRfGAuqATwWHkPPJXRNn8N9CdxmZq2BN9x9qZkNBvoDs8OX96hFqMAU5iUzOwCsInT55W7ASndfEr79eeA3wGOE1md42szeASK+NLe7bzGzFeFr1iwNH+OL8OOWJmdtQpd9KLjq1ggzu4bQ33ULQgvOzDvsvseF938RPk4ioedNBFBRkKrjd8AmIIVQD/cHi+a4+8tm9hVwJvC+mf2C0GWGn3f3P0ZwjEsLXjDPzApdYyN8PZ6BhC7CNhK4Dji1FOfyKjACWAS86e5uoVfoiHMSWoFsLPA4cIGZdQBuAga4+w4ze47QheEOZ8A0d7+kFHkljmj4SKqK+sCG8DXyLyf0Lvl7zKwjsCI8ZDKJ0DDKh8BFZtYs3KaRRb4+9SKgvZl1Dm9fDnwSHoOv7+5TCU3iFvYJoD2ELt9dmDeA8witA/BqeF+pcrp7NqFhoOPCQ0/1gH3ALjNrDpxRRJaZwAmHzsnMks2ssF6XxCkVBakqngCuNLOZhIaO9hXS5ifAAjP7DuhOaMnCNEIvnh+Y2TxgGqGhlRK5eyahK1BOMLP5QB7wFKEX2Cnhx/uEUC/mcM8BTx2aaD7scXcAaUA7d58V3lfqnOG5inHATe4+l9DazAuBZwgNSR0yHnjXzGa4+xZCn4x6JXycmYSeKxFAV0kVEZEC1FMQEZF8KgoiIpJPRUFERPKpKIiISD4VBRERyaeiICIi+VQUREQkn4qCiIjk+38B62B56Eck7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This plots the ROC curve for the model.\n",
    "roc_plot(model=mlp_nullsonly, x_test=Xtest_nullsonly,\n",
    "        y_test=Ytest_nullsonly,title=\"ROC: MLP Nulls Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model One Hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model One Hot, default learning rate in Stochastic Gradient Descent Optimizer.\n",
    "This is a comparison MLP model that uses the default learning rate of 0.01 to descend down the loss gradient in an attempt to find the global minimum (Reference 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "DEFAULT_stochastic = SGD()\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for One Hot.\n",
    "mlp_onehot_DEFAULT = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=64,\n",
    "                             second_dense=32,\n",
    "                             third_dense=16,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the One Hot data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_onehot_DEFAULT.compile(loss='binary_crossentropy',\n",
    "              optimizer= DEFAULT_stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_onehot_DEFAULT.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65512 samples, validate on 16378 samples\n",
      "Epoch 1/100\n",
      "65512/65512 [==============================] - 5s 83us/step - loss: 0.7180 - acc: 0.7258 - val_loss: 0.7899 - val_acc: 0.7804\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78988, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 2/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.6015 - acc: 0.7895 - val_loss: 0.6984 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78988 to 0.69841, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 3/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5542 - acc: 0.8119 - val_loss: 0.5880 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69841 to 0.58802, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 4/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5223 - acc: 0.8251 - val_loss: 0.5706 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58802 to 0.57061, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 5/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5064 - acc: 0.8325 - val_loss: 0.5508 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.57061 to 0.55081, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 6/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.4881 - acc: 0.8382 - val_loss: 0.5451 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.55081 to 0.54515, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 7/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.4753 - acc: 0.8431 - val_loss: 0.5359 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.54515 to 0.53585, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 8/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.4655 - acc: 0.8451 - val_loss: 0.5125 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.53585 to 0.51255, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 9/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.4560 - acc: 0.8471 - val_loss: 0.5126 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51255\n",
      "Epoch 10/100\n",
      "65512/65512 [==============================] - 6s 86us/step - loss: 0.4455 - acc: 0.8487 - val_loss: 0.5089 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.51255 to 0.50889, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 11/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.4370 - acc: 0.8510 - val_loss: 0.5203 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50889\n",
      "Epoch 12/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.4321 - acc: 0.8510 - val_loss: 0.4916 - val_acc: 0.8706\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.50889 to 0.49164, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 13/100\n",
      "65512/65512 [==============================] - 5s 79us/step - loss: 0.4242 - acc: 0.8538 - val_loss: 0.4893 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.49164 to 0.48929, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 14/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.4195 - acc: 0.8545 - val_loss: 0.4759 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.48929 to 0.47593, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 15/100\n",
      "65512/65512 [==============================] - 5s 76us/step - loss: 0.4146 - acc: 0.8551 - val_loss: 0.4811 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47593\n",
      "Epoch 16/100\n",
      "65512/65512 [==============================] - 5s 79us/step - loss: 0.4092 - acc: 0.8563 - val_loss: 0.4388 - val_acc: 0.8760\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.47593 to 0.43883, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 17/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.4035 - acc: 0.8563 - val_loss: 0.4421 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.43883\n",
      "Epoch 18/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3993 - acc: 0.8582 - val_loss: 0.4763 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.43883\n",
      "Epoch 19/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3965 - acc: 0.8589 - val_loss: 0.4841 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.43883\n",
      "Epoch 20/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3939 - acc: 0.8575 - val_loss: 0.4357 - val_acc: 0.8781\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.43883 to 0.43572, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 21/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.3886 - acc: 0.8587 - val_loss: 0.5073 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.43572\n",
      "Epoch 22/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3860 - acc: 0.8587 - val_loss: 0.4030 - val_acc: 0.8946\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.43572 to 0.40302, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 23/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3836 - acc: 0.8590 - val_loss: 0.4255 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.40302\n",
      "Epoch 24/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3800 - acc: 0.8601 - val_loss: 0.4526 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.40302\n",
      "Epoch 25/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.3782 - acc: 0.8591 - val_loss: 0.4683 - val_acc: 0.8786\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.40302\n",
      "Epoch 26/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3755 - acc: 0.8600 - val_loss: 0.4221 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.40302\n",
      "Epoch 27/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.3730 - acc: 0.8599 - val_loss: 0.4463 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.40302\n",
      "Epoch 28/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.3717 - acc: 0.8610 - val_loss: 0.4581 - val_acc: 0.8226\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.40302\n",
      "Epoch 29/100\n",
      "65512/65512 [==============================] - 5s 76us/step - loss: 0.3691 - acc: 0.8626 - val_loss: 0.3914 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.40302 to 0.39135, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 30/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3657 - acc: 0.8619 - val_loss: 0.4280 - val_acc: 0.8782\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.39135\n",
      "Epoch 31/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.3657 - acc: 0.8611 - val_loss: 0.4782 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.39135\n",
      "Epoch 32/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3627 - acc: 0.8616 - val_loss: 0.4208 - val_acc: 0.8755\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.39135\n",
      "Epoch 33/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3625 - acc: 0.8608 - val_loss: 0.4458 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.39135\n",
      "Epoch 34/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3603 - acc: 0.8622 - val_loss: 0.4044 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.39135\n",
      "Epoch 35/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3593 - acc: 0.8627 - val_loss: 0.4266 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.39135\n",
      "Epoch 36/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3574 - acc: 0.8626 - val_loss: 0.3838 - val_acc: 0.8840\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.39135 to 0.38383, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3560 - acc: 0.8621 - val_loss: 0.4019 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.38383\n",
      "Epoch 38/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3545 - acc: 0.8643 - val_loss: 0.4540 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.38383\n",
      "Epoch 39/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3536 - acc: 0.8636 - val_loss: 0.3886 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.38383\n",
      "Epoch 40/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3522 - acc: 0.8619 - val_loss: 0.4034 - val_acc: 0.8737\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.38383\n",
      "Epoch 41/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3517 - acc: 0.8633 - val_loss: 0.4298 - val_acc: 0.8530\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.38383\n",
      "Epoch 42/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.3518 - acc: 0.8632 - val_loss: 0.4530 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.38383\n",
      "Epoch 43/100\n",
      "65512/65512 [==============================] - 5s 79us/step - loss: 0.3494 - acc: 0.8637 - val_loss: 0.4815 - val_acc: 0.8448\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.38383\n",
      "Epoch 44/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3494 - acc: 0.8638 - val_loss: 0.3649 - val_acc: 0.8891\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.38383 to 0.36494, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 45/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3461 - acc: 0.8633 - val_loss: 0.3947 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.36494\n",
      "Epoch 46/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3461 - acc: 0.8638 - val_loss: 0.3917 - val_acc: 0.8702\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.36494\n",
      "Epoch 47/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3471 - acc: 0.8643 - val_loss: 0.3780 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.36494\n",
      "Epoch 48/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3450 - acc: 0.8642 - val_loss: 0.3516 - val_acc: 0.8841\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.36494 to 0.35162, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 49/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3472 - acc: 0.8645 - val_loss: 0.4017 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.35162\n",
      "Epoch 50/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3459 - acc: 0.8645 - val_loss: 0.3950 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.35162\n",
      "Epoch 51/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3458 - acc: 0.8636 - val_loss: 0.3644 - val_acc: 0.8875\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.35162\n",
      "Epoch 52/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3432 - acc: 0.8644 - val_loss: 0.3762 - val_acc: 0.8821\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.35162\n",
      "Epoch 53/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3442 - acc: 0.8640 - val_loss: 0.4491 - val_acc: 0.8358\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.35162\n",
      "Epoch 54/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3436 - acc: 0.8650 - val_loss: 0.3845 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.35162\n",
      "Epoch 55/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3422 - acc: 0.8650 - val_loss: 0.4773 - val_acc: 0.8323\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.35162\n",
      "Epoch 56/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3423 - acc: 0.8658 - val_loss: 0.3893 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.35162\n",
      "Epoch 57/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.3412 - acc: 0.8657 - val_loss: 0.3703 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.35162\n",
      "Epoch 58/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.3406 - acc: 0.8642 - val_loss: 0.4101 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.35162\n",
      "Epoch 59/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3403 - acc: 0.8656 - val_loss: 0.3872 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.35162\n",
      "Epoch 60/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3385 - acc: 0.8666 - val_loss: 0.4102 - val_acc: 0.8717\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.35162\n",
      "Epoch 61/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3407 - acc: 0.8656 - val_loss: 0.3688 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.35162\n",
      "Epoch 62/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3376 - acc: 0.8656 - val_loss: 0.3899 - val_acc: 0.8836\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.35162\n",
      "Epoch 63/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3377 - acc: 0.8662 - val_loss: 0.4015 - val_acc: 0.8546\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.35162\n",
      "Epoch 64/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3378 - acc: 0.8660 - val_loss: 0.4422 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.35162\n",
      "Epoch 65/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3375 - acc: 0.8661 - val_loss: 0.3898 - val_acc: 0.8759\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.35162\n",
      "Epoch 66/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3373 - acc: 0.8663 - val_loss: 0.4309 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.35162\n",
      "Epoch 67/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.3356 - acc: 0.8665 - val_loss: 0.3914 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.35162\n",
      "Epoch 68/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.3360 - acc: 0.8668 - val_loss: 0.4340 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.35162\n",
      "Epoch 69/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.3366 - acc: 0.8663 - val_loss: 0.4202 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.35162\n",
      "Epoch 70/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.3348 - acc: 0.8668 - val_loss: 0.4119 - val_acc: 0.8753\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.35162\n",
      "Epoch 71/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.3345 - acc: 0.8672 - val_loss: 0.3354 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.35162 to 0.33543, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 72/100\n",
      "65512/65512 [==============================] - 8s 115us/step - loss: 0.3357 - acc: 0.8646 - val_loss: 0.3578 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.33543\n",
      "Epoch 73/100\n",
      "65512/65512 [==============================] - 6s 96us/step - loss: 0.3345 - acc: 0.8649 - val_loss: 0.3240 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.33543 to 0.32397, saving model to saved_models/weights.best.mlp_onehot_DEFAULT.hdf5\n",
      "Epoch 74/100\n",
      "65512/65512 [==============================] - 7s 104us/step - loss: 0.3353 - acc: 0.8659 - val_loss: 0.3939 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.32397\n",
      "Epoch 75/100\n",
      "65512/65512 [==============================] - 7s 110us/step - loss: 0.3345 - acc: 0.8667 - val_loss: 0.4003 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.32397\n",
      "Epoch 76/100\n",
      "65512/65512 [==============================] - 7s 103us/step - loss: 0.3349 - acc: 0.8659 - val_loss: 0.4225 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.32397\n",
      "Epoch 77/100\n",
      "65512/65512 [==============================] - 7s 108us/step - loss: 0.3323 - acc: 0.8661 - val_loss: 0.4330 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.32397\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65512/65512 [==============================] - 6s 96us/step - loss: 0.3340 - acc: 0.8663 - val_loss: 0.3467 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.32397\n",
      "Epoch 79/100\n",
      "65512/65512 [==============================] - 6s 97us/step - loss: 0.3342 - acc: 0.8656 - val_loss: 0.4163 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.32397\n",
      "Epoch 80/100\n",
      "65512/65512 [==============================] - 5s 81us/step - loss: 0.3325 - acc: 0.8678 - val_loss: 0.4236 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.32397\n",
      "Epoch 81/100\n",
      "65512/65512 [==============================] - 5s 80us/step - loss: 0.3335 - acc: 0.8660 - val_loss: 0.4515 - val_acc: 0.8159\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.32397\n",
      "Epoch 82/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.3332 - acc: 0.8672 - val_loss: 0.4182 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.32397\n",
      "Epoch 83/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.3339 - acc: 0.8676 - val_loss: 0.3905 - val_acc: 0.8773\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.32397\n",
      "Epoch 84/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.3335 - acc: 0.8669 - val_loss: 0.4262 - val_acc: 0.8271\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.32397\n",
      "Epoch 85/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.3323 - acc: 0.8666 - val_loss: 0.5018 - val_acc: 0.7983\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.32397\n",
      "Epoch 86/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.3324 - acc: 0.8672 - val_loss: 0.4076 - val_acc: 0.8734\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.32397\n",
      "Epoch 87/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.3310 - acc: 0.8675 - val_loss: 0.4078 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.32397\n",
      "Epoch 88/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.3328 - acc: 0.8658 - val_loss: 0.3814 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.32397\n",
      "Epoch 89/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.3313 - acc: 0.8668 - val_loss: 0.3687 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.32397\n",
      "Epoch 90/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.3313 - acc: 0.8668 - val_loss: 0.3953 - val_acc: 0.8690\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.32397\n",
      "Epoch 91/100\n",
      "65512/65512 [==============================] - 6s 85us/step - loss: 0.3319 - acc: 0.8682 - val_loss: 0.4563 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.32397\n",
      "Epoch 92/100\n",
      "65512/65512 [==============================] - 7s 101us/step - loss: 0.3306 - acc: 0.8669 - val_loss: 0.4328 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.32397\n",
      "Epoch 93/100\n",
      "65512/65512 [==============================] - 7s 107us/step - loss: 0.3302 - acc: 0.8665 - val_loss: 0.3818 - val_acc: 0.8889\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.32397\n",
      "Epoch 94/100\n",
      "65512/65512 [==============================] - 6s 85us/step - loss: 0.3303 - acc: 0.8645 - val_loss: 0.3743 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.32397\n",
      "Epoch 95/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.3290 - acc: 0.8667 - val_loss: 0.3867 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.32397\n",
      "Epoch 96/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.3298 - acc: 0.8672 - val_loss: 0.3965 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.32397\n",
      "Epoch 97/100\n",
      "65512/65512 [==============================] - 5s 80us/step - loss: 0.3281 - acc: 0.8668 - val_loss: 0.3520 - val_acc: 0.8950\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.32397\n",
      "Epoch 98/100\n",
      "65512/65512 [==============================] - 5s 75us/step - loss: 0.3284 - acc: 0.8669 - val_loss: 0.3737 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.32397\n",
      "Epoch 99/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3286 - acc: 0.8669 - val_loss: 0.4158 - val_acc: 0.8437\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.32397\n",
      "Epoch 100/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.3301 - acc: 0.8667 - val_loss: 0.4425 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.32397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b929183748>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_onehot_DEFAULT.fit(Xtrain_onehot, Ytrain_onehot, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_onehot_DEFAULT.load_weights('saved_models/weights.best.mlp_onehot_DEFAULT.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 95.3923%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_onehot_DEFAULT.evaluate(Xtest_onehot, Ytest_onehot, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.5384.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_onehot_DEFAULT = mlp_onehot_DEFAULT.predict(Xtest_onehot)\n",
    "ROC_mlp_onehot_DEFAULT = roc_auc_score(Ytest_onehot, Ypred_onehot_DEFAULT)\n",
    "print(\"The AUC score for the model is %.4f.\" % ROC_mlp_onehot_DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model One Hot Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for One Hot.\n",
    "mlp_onehot = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=64,\n",
    "                             second_dense=32,\n",
    "                             third_dense=16,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the One Hot data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_onehot.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_onehot.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45858 samples, validate on 11465 samples\n",
      "Epoch 1/100\n",
      "45858/45858 [==============================] - 10s 222us/step - loss: 0.8830 - acc: 0.5230 - val_loss: 0.7933 - val_acc: 0.7262\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79333, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 2/100\n",
      "45858/45858 [==============================] - 5s 113us/step - loss: 0.8345 - acc: 0.5857 - val_loss: 0.7752 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79333 to 0.77520, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 3/100\n",
      "45858/45858 [==============================] - 5s 112us/step - loss: 0.8141 - acc: 0.6359 - val_loss: 0.7637 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77520 to 0.76375, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 4/100\n",
      "45858/45858 [==============================] - 5s 114us/step - loss: 0.7964 - acc: 0.6778 - val_loss: 0.7537 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.76375 to 0.75372, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 5/100\n",
      "45858/45858 [==============================] - 5s 114us/step - loss: 0.7850 - acc: 0.6978 - val_loss: 0.7453 - val_acc: 0.7737\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.75372 to 0.74532, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 6/100\n",
      "45858/45858 [==============================] - 5s 113us/step - loss: 0.7757 - acc: 0.7079 - val_loss: 0.7351 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.74532 to 0.73509, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 7/100\n",
      "45858/45858 [==============================] - 5s 113us/step - loss: 0.7708 - acc: 0.7154 - val_loss: 0.7277 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.73509 to 0.72774, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 8/100\n",
      "45858/45858 [==============================] - 5s 113us/step - loss: 0.7572 - acc: 0.7250 - val_loss: 0.7181 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.72774 to 0.71814, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 9/100\n",
      "45858/45858 [==============================] - 5s 114us/step - loss: 0.7505 - acc: 0.7288 - val_loss: 0.7105 - val_acc: 0.7773\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.71814 to 0.71047, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 10/100\n",
      "45858/45858 [==============================] - 8s 166us/step - loss: 0.7467 - acc: 0.7307 - val_loss: 0.7041 - val_acc: 0.7785\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.71047 to 0.70414, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 11/100\n",
      "45858/45858 [==============================] - 6s 127us/step - loss: 0.7405 - acc: 0.7338 - val_loss: 0.6976 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.70414 to 0.69758, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 12/100\n",
      "45858/45858 [==============================] - 5s 116us/step - loss: 0.7315 - acc: 0.7401 - val_loss: 0.6894 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.69758 to 0.68936, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 13/100\n",
      "45858/45858 [==============================] - 5s 117us/step - loss: 0.7293 - acc: 0.7412 - val_loss: 0.6826 - val_acc: 0.7993\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.68936 to 0.68256, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 14/100\n",
      "45858/45858 [==============================] - 5s 119us/step - loss: 0.7216 - acc: 0.7453 - val_loss: 0.6755 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.68256 to 0.67555, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 15/100\n",
      "45858/45858 [==============================] - 5s 118us/step - loss: 0.7172 - acc: 0.7485 - val_loss: 0.6678 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.67555 to 0.66781, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 16/100\n",
      "45858/45858 [==============================] - 6s 120us/step - loss: 0.7074 - acc: 0.7554 - val_loss: 0.6593 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.66781 to 0.65927, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 17/100\n",
      "45858/45858 [==============================] - 6s 120us/step - loss: 0.7067 - acc: 0.7544 - val_loss: 0.6535 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.65927 to 0.65346, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 18/100\n",
      "45858/45858 [==============================] - 6s 121us/step - loss: 0.6967 - acc: 0.7612 - val_loss: 0.6463 - val_acc: 0.8202\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.65346 to 0.64634, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 19/100\n",
      "45858/45858 [==============================] - 6s 120us/step - loss: 0.6891 - acc: 0.7651 - val_loss: 0.6372 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.64634 to 0.63718, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 20/100\n",
      "45858/45858 [==============================] - 6s 121us/step - loss: 0.6861 - acc: 0.7658 - val_loss: 0.6316 - val_acc: 0.8188\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.63718 to 0.63161, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 21/100\n",
      "45858/45858 [==============================] - 6s 132us/step - loss: 0.6842 - acc: 0.7670 - val_loss: 0.6256 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.63161 to 0.62562, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 22/100\n",
      "45858/45858 [==============================] - 6s 124us/step - loss: 0.6759 - acc: 0.7714 - val_loss: 0.6201 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.62562 to 0.62008, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 23/100\n",
      "45858/45858 [==============================] - 6s 127us/step - loss: 0.6699 - acc: 0.7759 - val_loss: 0.6132 - val_acc: 0.8166\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.62008 to 0.61320, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 24/100\n",
      "45858/45858 [==============================] - 6s 124us/step - loss: 0.6631 - acc: 0.7794 - val_loss: 0.6082 - val_acc: 0.8156\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.61320 to 0.60821, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 25/100\n",
      "45858/45858 [==============================] - 6s 138us/step - loss: 0.6616 - acc: 0.7787 - val_loss: 0.6027 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.60821 to 0.60272, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 26/100\n",
      "45858/45858 [==============================] - 7s 143us/step - loss: 0.6568 - acc: 0.7794 - val_loss: 0.5977 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.60272 to 0.59766, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 27/100\n",
      "45858/45858 [==============================] - 8s 176us/step - loss: 0.6564 - acc: 0.7796 - val_loss: 0.5940 - val_acc: 0.8158\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.59766 to 0.59401, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 28/100\n",
      "45858/45858 [==============================] - 7s 145us/step - loss: 0.6510 - acc: 0.7820 - val_loss: 0.5894 - val_acc: 0.8158\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.59401 to 0.58939, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 29/100\n",
      "45858/45858 [==============================] - 6s 134us/step - loss: 0.6477 - acc: 0.7849 - val_loss: 0.5861 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.58939 to 0.58607, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 30/100\n",
      "45858/45858 [==============================] - 6s 122us/step - loss: 0.6460 - acc: 0.7863 - val_loss: 0.5824 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.58607 to 0.58237, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 31/100\n",
      "45858/45858 [==============================] - 9s 189us/step - loss: 0.6430 - acc: 0.7886 - val_loss: 0.5798 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.58237 to 0.57982, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 32/100\n",
      "45858/45858 [==============================] - 11s 245us/step - loss: 0.6366 - acc: 0.7887 - val_loss: 0.5753 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.57982 to 0.57531, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 10s 211us/step - loss: 0.6346 - acc: 0.7903 - val_loss: 0.5721 - val_acc: 0.8172\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.57531 to 0.57212, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 34/100\n",
      "45858/45858 [==============================] - 8s 185us/step - loss: 0.6301 - acc: 0.7953 - val_loss: 0.5704 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.57212 to 0.57037, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 35/100\n",
      "45858/45858 [==============================] - 9s 206us/step - loss: 0.6296 - acc: 0.7930 - val_loss: 0.5649 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.57037 to 0.56490, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 36/100\n",
      "45858/45858 [==============================] - 9s 205us/step - loss: 0.6252 - acc: 0.7955 - val_loss: 0.5624 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.56490 to 0.56239, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 37/100\n",
      "45858/45858 [==============================] - 10s 208us/step - loss: 0.6226 - acc: 0.7972 - val_loss: 0.5610 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.56239 to 0.56096, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 38/100\n",
      "45858/45858 [==============================] - 8s 166us/step - loss: 0.6198 - acc: 0.7975 - val_loss: 0.5573 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.56096 to 0.55735, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 39/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.6185 - acc: 0.7993 - val_loss: 0.5551 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.55735 to 0.55514, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 40/100\n",
      "45858/45858 [==============================] - 6s 132us/step - loss: 0.6180 - acc: 0.7987 - val_loss: 0.5541 - val_acc: 0.8235\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.55514 to 0.55409, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 41/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.6133 - acc: 0.8001 - val_loss: 0.5508 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.55409 to 0.55081, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 42/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.6080 - acc: 0.8029 - val_loss: 0.5464 - val_acc: 0.8242\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.55081 to 0.54637, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 43/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.6078 - acc: 0.8020 - val_loss: 0.5440 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.54637 to 0.54402, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 44/100\n",
      "45858/45858 [==============================] - 6s 132us/step - loss: 0.6015 - acc: 0.8061 - val_loss: 0.5417 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.54402 to 0.54170, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 45/100\n",
      "45858/45858 [==============================] - 6s 133us/step - loss: 0.6033 - acc: 0.8039 - val_loss: 0.5392 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.54170 to 0.53921, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 46/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.6042 - acc: 0.8044 - val_loss: 0.5378 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.53921 to 0.53776, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 47/100\n",
      "45858/45858 [==============================] - 6s 134us/step - loss: 0.5979 - acc: 0.8063 - val_loss: 0.5343 - val_acc: 0.8290\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.53776 to 0.53433, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 48/100\n",
      "45858/45858 [==============================] - 7s 142us/step - loss: 0.5934 - acc: 0.8062 - val_loss: 0.5314 - val_acc: 0.8303\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.53433 to 0.53137, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 49/100\n",
      "45858/45858 [==============================] - 7s 152us/step - loss: 0.5913 - acc: 0.8083 - val_loss: 0.5287 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.53137 to 0.52874, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 50/100\n",
      "45858/45858 [==============================] - 7s 146us/step - loss: 0.5927 - acc: 0.8082 - val_loss: 0.5277 - val_acc: 0.8329\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.52874 to 0.52771, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 51/100\n",
      "45858/45858 [==============================] - 6s 140us/step - loss: 0.5886 - acc: 0.8092 - val_loss: 0.5254 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.52771 to 0.52539, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 52/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.5882 - acc: 0.8103 - val_loss: 0.5224 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.52539 to 0.52243, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 53/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.5790 - acc: 0.8149 - val_loss: 0.5195 - val_acc: 0.8454\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.52243 to 0.51953, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 54/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.5785 - acc: 0.8133 - val_loss: 0.5172 - val_acc: 0.8486\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.51953 to 0.51722, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 55/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.5792 - acc: 0.8146 - val_loss: 0.5158 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.51722 to 0.51576, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 56/100\n",
      "45858/45858 [==============================] - 7s 145us/step - loss: 0.5761 - acc: 0.8153 - val_loss: 0.5134 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.51576 to 0.51338, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 57/100\n",
      "45858/45858 [==============================] - 6s 138us/step - loss: 0.5742 - acc: 0.8141 - val_loss: 0.5094 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.51338 to 0.50939, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 58/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.5718 - acc: 0.8178 - val_loss: 0.5087 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.50939 to 0.50875, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 59/100\n",
      "45858/45858 [==============================] - 6s 137us/step - loss: 0.5703 - acc: 0.8152 - val_loss: 0.5064 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.50875 to 0.50641, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 60/100\n",
      "45858/45858 [==============================] - 6s 137us/step - loss: 0.5732 - acc: 0.8132 - val_loss: 0.5046 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.50641 to 0.50459, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 61/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.5702 - acc: 0.8151 - val_loss: 0.5042 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.50459 to 0.50422, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 62/100\n",
      "45858/45858 [==============================] - 6s 140us/step - loss: 0.5672 - acc: 0.8166 - val_loss: 0.5012 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.50422 to 0.50116, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 63/100\n",
      "45858/45858 [==============================] - 6s 138us/step - loss: 0.5636 - acc: 0.8193 - val_loss: 0.4997 - val_acc: 0.8478\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.50116 to 0.49966, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 64/100\n",
      "45858/45858 [==============================] - 6s 137us/step - loss: 0.5651 - acc: 0.8169 - val_loss: 0.4978 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.49966 to 0.49783, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 65/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5627 - acc: 0.8177 - val_loss: 0.4965 - val_acc: 0.8476\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.49783 to 0.49653, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 66/100\n",
      "45858/45858 [==============================] - 6s 131us/step - loss: 0.5625 - acc: 0.8162 - val_loss: 0.4953 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.49653 to 0.49534, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 67/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5603 - acc: 0.8192 - val_loss: 0.4926 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.49534 to 0.49260, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 68/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5544 - acc: 0.8208 - val_loss: 0.4906 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.49260 to 0.49058, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 69/100\n",
      "45858/45858 [==============================] - 6s 129us/step - loss: 0.5539 - acc: 0.8210 - val_loss: 0.4884 - val_acc: 0.8482\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.49058 to 0.48836, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 70/100\n",
      "45858/45858 [==============================] - 6s 129us/step - loss: 0.5523 - acc: 0.8216 - val_loss: 0.4877 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.48836 to 0.48773, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 71/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5529 - acc: 0.8207 - val_loss: 0.4862 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.48773 to 0.48623, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 72/100\n",
      "45858/45858 [==============================] - 6s 131us/step - loss: 0.5513 - acc: 0.8217 - val_loss: 0.4839 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.48623 to 0.48387, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 73/100\n",
      "45858/45858 [==============================] - 6s 131us/step - loss: 0.5483 - acc: 0.8222 - val_loss: 0.4829 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.48387 to 0.48291, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 74/100\n",
      "45858/45858 [==============================] - 6s 132us/step - loss: 0.5491 - acc: 0.8215 - val_loss: 0.4822 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.48291 to 0.48222, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 75/100\n",
      "45858/45858 [==============================] - 6s 131us/step - loss: 0.5458 - acc: 0.8212 - val_loss: 0.4800 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.48222 to 0.47996, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 76/100\n",
      "45858/45858 [==============================] - 6s 133us/step - loss: 0.5462 - acc: 0.8235 - val_loss: 0.4791 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.47996 to 0.47910, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 77/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5425 - acc: 0.8249 - val_loss: 0.4769 - val_acc: 0.8498\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.47910 to 0.47687, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 78/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5418 - acc: 0.8239 - val_loss: 0.4747 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.47687 to 0.47473, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 79/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5398 - acc: 0.8244 - val_loss: 0.4744 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.47473 to 0.47440, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 80/100\n",
      "45858/45858 [==============================] - 6s 131us/step - loss: 0.5419 - acc: 0.8263 - val_loss: 0.4727 - val_acc: 0.8502\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.47440 to 0.47267, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 81/100\n",
      "45858/45858 [==============================] - 6s 130us/step - loss: 0.5374 - acc: 0.8260 - val_loss: 0.4709 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.47267 to 0.47090, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 82/100\n",
      "45858/45858 [==============================] - 7s 143us/step - loss: 0.5356 - acc: 0.8273 - val_loss: 0.4692 - val_acc: 0.8506\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.47090 to 0.46919, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 83/100\n",
      "45858/45858 [==============================] - 6s 138us/step - loss: 0.5368 - acc: 0.8261 - val_loss: 0.4676 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.46919 to 0.46757, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 84/100\n",
      "45858/45858 [==============================] - 6s 123us/step - loss: 0.5312 - acc: 0.8277 - val_loss: 0.4662 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.46757 to 0.46618, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 85/100\n",
      "45858/45858 [==============================] - 11s 235us/step - loss: 0.5330 - acc: 0.8263 - val_loss: 0.4650 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.46618 to 0.46503, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 86/100\n",
      "45858/45858 [==============================] - 8s 177us/step - loss: 0.5329 - acc: 0.8283 - val_loss: 0.4647 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.46503 to 0.46471, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 87/100\n",
      "45858/45858 [==============================] - 7s 147us/step - loss: 0.5285 - acc: 0.8300 - val_loss: 0.4635 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.46471 to 0.46348, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 88/100\n",
      "45858/45858 [==============================] - 6s 138us/step - loss: 0.5306 - acc: 0.8261 - val_loss: 0.4624 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.46348 to 0.46244, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 89/100\n",
      "45858/45858 [==============================] - 6s 139us/step - loss: 0.5334 - acc: 0.8269 - val_loss: 0.4618 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.46244 to 0.46176, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 90/100\n",
      "45858/45858 [==============================] - 6s 133us/step - loss: 0.5290 - acc: 0.8297 - val_loss: 0.4604 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.46176 to 0.46040, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 91/100\n",
      "45858/45858 [==============================] - 6s 137us/step - loss: 0.5253 - acc: 0.8303 - val_loss: 0.4590 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.46040 to 0.45899, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 92/100\n",
      "45858/45858 [==============================] - 6s 135us/step - loss: 0.5247 - acc: 0.8287 - val_loss: 0.4572 - val_acc: 0.8526\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.45899 to 0.45725, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 93/100\n",
      "45858/45858 [==============================] - 6s 140us/step - loss: 0.5233 - acc: 0.8310 - val_loss: 0.4568 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.45725 to 0.45680, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 94/100\n",
      "45858/45858 [==============================] - 7s 144us/step - loss: 0.5225 - acc: 0.8310 - val_loss: 0.4562 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.45680 to 0.45623, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 95/100\n",
      "45858/45858 [==============================] - 6s 136us/step - loss: 0.5215 - acc: 0.8303 - val_loss: 0.4549 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.45623 to 0.45487, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 96/100\n",
      "45858/45858 [==============================] - 6s 141us/step - loss: 0.5235 - acc: 0.8312 - val_loss: 0.4538 - val_acc: 0.8534\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.45487 to 0.45384, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45858/45858 [==============================] - 6s 127us/step - loss: 0.5186 - acc: 0.8317 - val_loss: 0.4526 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.45384 to 0.45257, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 98/100\n",
      "45858/45858 [==============================] - 6s 127us/step - loss: 0.5197 - acc: 0.8326 - val_loss: 0.4521 - val_acc: 0.8546\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.45257 to 0.45207, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 99/100\n",
      "45858/45858 [==============================] - 6s 126us/step - loss: 0.5182 - acc: 0.8318 - val_loss: 0.4504 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.45207 to 0.45045, saving model to saved_models/weights.best.mlp_onehot.hdf5\n",
      "Epoch 100/100\n",
      "45858/45858 [==============================] - 6s 126us/step - loss: 0.5159 - acc: 0.8333 - val_loss: 0.4500 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.45045 to 0.44999, saving model to saved_models/weights.best.mlp_onehot.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1efdb525c88>"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_onehot.fit(Xtrain_onehot, Ytrain_onehot, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_onehot.load_weights('saved_models/weights.best.mlp_onehot.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 68.4140%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_onehot.evaluate(Xtest_onehot, Ytest_onehot, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.7289.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_onehot = mlp_onehot.predict(Xtest_onehot)\n",
    "ROC_mlp_onehot = roc_auc_score(Ytest_onehot, Ypred_onehot)\n",
    "print(\"The AUC score for the model is %.4f.\" % ROC_mlp_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This plots the ROC curve for the model.\n",
    "roc_plot(model=mlp_onehot, x_test=Xtest_onehot,\n",
    "        y_test=Ytest_onehot,title=\"ROC: MLP One Hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Sum, default learning rate in Stochastic Gradient Descent Optimizer.\n",
    "This is a comparison MLP model that uses the default learning rate of 0.01 to descend down the loss gradient in an attempt to find the global minimum (Reference 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "DEFAULT_stochastic = SGD()\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for SUM.\n",
    "mlp_sum_DEFAULT = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the SUM data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_sum_DEFAULT.compile(loss='binary_crossentropy',\n",
    "              optimizer= DEFAULT_stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_sum_DEFAULT.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65512 samples, validate on 16378 samples\n",
      "Epoch 1/100\n",
      "65512/65512 [==============================] - 5s 83us/step - loss: 0.7120 - acc: 0.6816 - val_loss: 0.9036 - val_acc: 0.4429\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90364, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 2/100\n",
      "65512/65512 [==============================] - 4s 59us/step - loss: 0.6798 - acc: 0.7040 - val_loss: 0.9185 - val_acc: 0.4433\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.90364\n",
      "Epoch 3/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6625 - acc: 0.7122 - val_loss: 0.9077 - val_acc: 0.4438\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.90364\n",
      "Epoch 4/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6497 - acc: 0.7160 - val_loss: 0.9109 - val_acc: 0.4436\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.90364\n",
      "Epoch 5/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6394 - acc: 0.7180 - val_loss: 0.8686 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.90364 to 0.86855, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 6/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6319 - acc: 0.7197 - val_loss: 0.8694 - val_acc: 0.4456\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.86855\n",
      "Epoch 7/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6239 - acc: 0.7216 - val_loss: 0.8471 - val_acc: 0.4475\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.86855 to 0.84709, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 8/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6164 - acc: 0.7236 - val_loss: 0.8234 - val_acc: 0.4512\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.84709 to 0.82342, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 9/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6089 - acc: 0.7259 - val_loss: 0.7941 - val_acc: 0.4570\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.82342 to 0.79408, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 10/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.6045 - acc: 0.7279 - val_loss: 0.7792 - val_acc: 0.4608\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.79408 to 0.77924, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 11/100\n",
      "65512/65512 [==============================] - 4s 56us/step - loss: 0.5991 - acc: 0.7289 - val_loss: 0.8181 - val_acc: 0.4607\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.77924\n",
      "Epoch 12/100\n",
      "65512/65512 [==============================] - 4s 57us/step - loss: 0.5946 - acc: 0.7298 - val_loss: 0.7834 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.77924\n",
      "Epoch 13/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5869 - acc: 0.7339 - val_loss: 0.7756 - val_acc: 0.4784\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.77924 to 0.77558, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 14/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5864 - acc: 0.7344 - val_loss: 0.7889 - val_acc: 0.4724\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77558\n",
      "Epoch 15/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5809 - acc: 0.7351 - val_loss: 0.7521 - val_acc: 0.4859\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.77558 to 0.75207, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 16/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5778 - acc: 0.7373 - val_loss: 0.7399 - val_acc: 0.4907\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.75207 to 0.73995, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 17/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5748 - acc: 0.7361 - val_loss: 0.7359 - val_acc: 0.4908\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.73995 to 0.73591, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 18/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5757 - acc: 0.7353 - val_loss: 0.7397 - val_acc: 0.4880\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.73591\n",
      "Epoch 19/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5706 - acc: 0.7386 - val_loss: 0.7478 - val_acc: 0.4952\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.73591\n",
      "Epoch 20/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5672 - acc: 0.7382 - val_loss: 0.6973 - val_acc: 0.5216\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73591 to 0.69733, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 21/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5628 - acc: 0.7433 - val_loss: 0.7903 - val_acc: 0.4878\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.69733\n",
      "Epoch 22/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5635 - acc: 0.7394 - val_loss: 0.7339 - val_acc: 0.4968\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.69733\n",
      "Epoch 23/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5627 - acc: 0.7396 - val_loss: 0.7375 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.69733\n",
      "Epoch 24/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5585 - acc: 0.7407 - val_loss: 0.7749 - val_acc: 0.4917\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.69733\n",
      "Epoch 25/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5567 - acc: 0.7422 - val_loss: 0.7638 - val_acc: 0.4874\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.69733\n",
      "Epoch 26/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5556 - acc: 0.7422 - val_loss: 0.7066 - val_acc: 0.5507\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.69733\n",
      "Epoch 27/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5538 - acc: 0.7452 - val_loss: 0.7124 - val_acc: 0.5354\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.69733\n",
      "Epoch 28/100\n",
      "65512/65512 [==============================] - 6s 86us/step - loss: 0.5519 - acc: 0.7447 - val_loss: 0.7254 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.69733\n",
      "Epoch 29/100\n",
      "65512/65512 [==============================] - 5s 80us/step - loss: 0.5498 - acc: 0.7467 - val_loss: 0.6893 - val_acc: 0.5474\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.69733 to 0.68932, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 30/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5510 - acc: 0.7455 - val_loss: 0.7248 - val_acc: 0.5363\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.68932\n",
      "Epoch 31/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5467 - acc: 0.7469 - val_loss: 0.6986 - val_acc: 0.5485\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.68932\n",
      "Epoch 32/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5477 - acc: 0.7458 - val_loss: 0.7317 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68932\n",
      "Epoch 33/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5472 - acc: 0.7465 - val_loss: 0.7460 - val_acc: 0.5289\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.68932\n",
      "Epoch 34/100\n",
      "65512/65512 [==============================] - 5s 81us/step - loss: 0.5443 - acc: 0.7500 - val_loss: 0.7294 - val_acc: 0.5904\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.68932\n",
      "Epoch 35/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.5402 - acc: 0.7510 - val_loss: 0.7008 - val_acc: 0.5909\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.68932\n",
      "Epoch 36/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.5452 - acc: 0.7491 - val_loss: 0.7168 - val_acc: 0.5912\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.68932\n",
      "Epoch 37/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5410 - acc: 0.7532 - val_loss: 0.7411 - val_acc: 0.5446\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.68932\n",
      "Epoch 38/100\n",
      "65512/65512 [==============================] - 5s 78us/step - loss: 0.5397 - acc: 0.7518 - val_loss: 0.7253 - val_acc: 0.5876\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.68932\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65512/65512 [==============================] - 6s 87us/step - loss: 0.5389 - acc: 0.7506 - val_loss: 0.6649 - val_acc: 0.6501\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.68932 to 0.66487, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 40/100\n",
      "65512/65512 [==============================] - 5s 78us/step - loss: 0.5380 - acc: 0.7522 - val_loss: 0.6959 - val_acc: 0.6313\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.66487\n",
      "Epoch 41/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5382 - acc: 0.7519 - val_loss: 0.6322 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.66487 to 0.63222, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 42/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5370 - acc: 0.7516 - val_loss: 0.6336 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.63222\n",
      "Epoch 43/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5379 - acc: 0.7511 - val_loss: 0.6661 - val_acc: 0.6713\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.63222\n",
      "Epoch 44/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5361 - acc: 0.7516 - val_loss: 0.6893 - val_acc: 0.6514\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.63222\n",
      "Epoch 45/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5333 - acc: 0.7552 - val_loss: 0.6232 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.63222 to 0.62319, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 46/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5344 - acc: 0.7554 - val_loss: 0.7112 - val_acc: 0.6416\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.62319\n",
      "Epoch 47/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5329 - acc: 0.7564 - val_loss: 0.7133 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.62319\n",
      "Epoch 48/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5347 - acc: 0.7542 - val_loss: 0.6569 - val_acc: 0.6693\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.62319\n",
      "Epoch 49/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5334 - acc: 0.7564 - val_loss: 0.6936 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.62319\n",
      "Epoch 50/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5319 - acc: 0.7562 - val_loss: 0.6873 - val_acc: 0.6242\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.62319\n",
      "Epoch 51/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5301 - acc: 0.7572 - val_loss: 0.6898 - val_acc: 0.6379\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.62319\n",
      "Epoch 52/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5326 - acc: 0.7541 - val_loss: 0.6918 - val_acc: 0.6672\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.62319\n",
      "Epoch 53/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5292 - acc: 0.7590 - val_loss: 0.7521 - val_acc: 0.6094\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.62319\n",
      "Epoch 54/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5289 - acc: 0.7596 - val_loss: 0.6398 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.62319\n",
      "Epoch 55/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5280 - acc: 0.7587 - val_loss: 0.6703 - val_acc: 0.6802\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.62319\n",
      "Epoch 56/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5297 - acc: 0.7585 - val_loss: 0.7176 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.62319\n",
      "Epoch 57/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5289 - acc: 0.7603 - val_loss: 0.6773 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.62319\n",
      "Epoch 58/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5286 - acc: 0.7587 - val_loss: 0.7987 - val_acc: 0.5822\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.62319\n",
      "Epoch 59/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5268 - acc: 0.7591 - val_loss: 0.6742 - val_acc: 0.6535\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.62319\n",
      "Epoch 60/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5281 - acc: 0.7595 - val_loss: 0.7366 - val_acc: 0.5933\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.62319\n",
      "Epoch 61/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5269 - acc: 0.7593 - val_loss: 0.6428 - val_acc: 0.7418\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.62319\n",
      "Epoch 62/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5244 - acc: 0.7617 - val_loss: 0.6589 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.62319\n",
      "Epoch 63/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5257 - acc: 0.7614 - val_loss: 0.6970 - val_acc: 0.6319\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.62319\n",
      "Epoch 64/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5267 - acc: 0.7581 - val_loss: 0.7447 - val_acc: 0.6263\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.62319\n",
      "Epoch 65/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.5242 - acc: 0.7610 - val_loss: 0.6111 - val_acc: 0.7447\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.62319 to 0.61111, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 66/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5257 - acc: 0.7615 - val_loss: 0.6047 - val_acc: 0.7434\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.61111 to 0.60468, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 67/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.5223 - acc: 0.7633 - val_loss: 0.5972 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.60468 to 0.59716, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 68/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.5247 - acc: 0.7625 - val_loss: 0.6474 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.59716\n",
      "Epoch 69/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.5250 - acc: 0.7622 - val_loss: 0.5941 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.59716 to 0.59411, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 70/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5221 - acc: 0.7639 - val_loss: 0.7768 - val_acc: 0.5982\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.59411\n",
      "Epoch 71/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5218 - acc: 0.7669 - val_loss: 0.6635 - val_acc: 0.6799\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.59411\n",
      "Epoch 72/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5230 - acc: 0.7624 - val_loss: 0.5599 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.59411 to 0.55989, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 73/100\n",
      "65512/65512 [==============================] - 5s 77us/step - loss: 0.5195 - acc: 0.7668 - val_loss: 0.6042 - val_acc: 0.7376\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.55989\n",
      "Epoch 74/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.5251 - acc: 0.7631 - val_loss: 0.6761 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.55989\n",
      "Epoch 75/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5257 - acc: 0.7641 - val_loss: 0.6213 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.55989\n",
      "Epoch 76/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.5245 - acc: 0.7628 - val_loss: 0.6468 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.55989\n",
      "Epoch 77/100\n",
      "65512/65512 [==============================] - 5s 82us/step - loss: 0.5223 - acc: 0.7653 - val_loss: 0.7330 - val_acc: 0.6877\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.55989\n",
      "Epoch 78/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5236 - acc: 0.7648 - val_loss: 0.7730 - val_acc: 0.5859\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.55989\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65512/65512 [==============================] - 5s 82us/step - loss: 0.5215 - acc: 0.7644 - val_loss: 0.6453 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.55989\n",
      "Epoch 80/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.5216 - acc: 0.7648 - val_loss: 0.7316 - val_acc: 0.6393\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.55989\n",
      "Epoch 81/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5220 - acc: 0.7645 - val_loss: 0.6419 - val_acc: 0.7246\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.55989\n",
      "Epoch 82/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5219 - acc: 0.7655 - val_loss: 0.6391 - val_acc: 0.7179\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.55989\n",
      "Epoch 83/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.5222 - acc: 0.7653 - val_loss: 0.6758 - val_acc: 0.6629\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.55989\n",
      "Epoch 84/100\n",
      "65512/65512 [==============================] - 6s 97us/step - loss: 0.5219 - acc: 0.7651 - val_loss: 0.5834 - val_acc: 0.8131\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.55989\n",
      "Epoch 85/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.5218 - acc: 0.7669 - val_loss: 0.5791 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.55989\n",
      "Epoch 86/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5190 - acc: 0.7671 - val_loss: 0.6232 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.55989\n",
      "Epoch 87/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5198 - acc: 0.7675 - val_loss: 0.6287 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.55989\n",
      "Epoch 88/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5191 - acc: 0.7679 - val_loss: 0.6052 - val_acc: 0.7221\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.55989\n",
      "Epoch 89/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.5210 - acc: 0.7644 - val_loss: 0.6343 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.55989\n",
      "Epoch 90/100\n",
      "65512/65512 [==============================] - 4s 64us/step - loss: 0.5213 - acc: 0.7650 - val_loss: 0.7722 - val_acc: 0.6142\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.55989\n",
      "Epoch 91/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5191 - acc: 0.7692 - val_loss: 0.6651 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.55989\n",
      "Epoch 92/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5200 - acc: 0.7663 - val_loss: 0.4202 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.55989 to 0.42018, saving model to saved_models/weights.best.mlp_sum_DEFAULT.hdf5\n",
      "Epoch 93/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5177 - acc: 0.7689 - val_loss: 0.7079 - val_acc: 0.6317\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.42018\n",
      "Epoch 94/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.5216 - acc: 0.7674 - val_loss: 0.5857 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.42018\n",
      "Epoch 95/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5167 - acc: 0.7704 - val_loss: 0.6705 - val_acc: 0.6887\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.42018\n",
      "Epoch 96/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5197 - acc: 0.7692 - val_loss: 0.5274 - val_acc: 0.8154\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.42018\n",
      "Epoch 97/100\n",
      "65512/65512 [==============================] - 4s 62us/step - loss: 0.5174 - acc: 0.7687 - val_loss: 0.7151 - val_acc: 0.6468\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.42018\n",
      "Epoch 98/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5188 - acc: 0.7677 - val_loss: 0.6921 - val_acc: 0.6696\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.42018\n",
      "Epoch 99/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5191 - acc: 0.7680 - val_loss: 0.7293 - val_acc: 0.6351\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.42018\n",
      "Epoch 100/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.5179 - acc: 0.7699 - val_loss: 0.6760 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.42018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b921be9710>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_sum_DEFAULT.fit(Xtrain_sum, Ytrain_sum, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_sum_DEFAULT.load_weights('saved_models/weights.best.mlp_sum_DEFAULT.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 96.6277%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_sum_DEFAULT.evaluate(Xtest_sum, Ytest_sum, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUvC score for the model is 0.3763.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_sum_DEFAULT = mlp_sum_DEFAULT.predict(Xtest_sum)\n",
    "ROC_mlp_sum_DEFAULT = roc_auc_score(Ytest_sum, Ypred_sum_DEFAULT)\n",
    "print(\"The AUvC score for the model is %.4f.\" % ROC_mlp_sum_DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model Sum Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD(lr=0.001)\n",
    "nad = Nadam()\n",
    "RMS = RMSprop()\n",
    "\n",
    "# This builds the MLP model for SUM.\n",
    "mlp_sum = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the SUM data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_sum.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 6 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_sum.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65512 samples, validate on 16378 samples\n",
      "Epoch 1/100\n",
      "65512/65512 [==============================] - 5s 81us/step - loss: 0.7625 - acc: 0.6400 - val_loss: 0.7715 - val_acc: 0.4418\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77153, saving model to saved_models/weights.best.mlp_sum.hdf5\n",
      "Epoch 2/100\n",
      "65512/65512 [==============================] - 4s 59us/step - loss: 0.7325 - acc: 0.6648 - val_loss: 0.8398 - val_acc: 0.4400\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.77153\n",
      "Epoch 3/100\n",
      "65512/65512 [==============================] - 4s 60us/step - loss: 0.7217 - acc: 0.6675 - val_loss: 0.8858 - val_acc: 0.4396\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.77153\n",
      "Epoch 4/100\n",
      "65512/65512 [==============================] - 4s 63us/step - loss: 0.7117 - acc: 0.6709 - val_loss: 0.9119 - val_acc: 0.4393\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.77153\n",
      "Epoch 5/100\n",
      "65512/65512 [==============================] - 4s 61us/step - loss: 0.7054 - acc: 0.6708 - val_loss: 0.9280 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.77153\n",
      "Epoch 6/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.7056 - acc: 0.6706 - val_loss: 0.9429 - val_acc: 0.4393\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.77153\n",
      "Epoch 7/100\n",
      "65512/65512 [==============================] - 4s 65us/step - loss: 0.7001 - acc: 0.6736 - val_loss: 0.9492 - val_acc: 0.4393\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.77153\n",
      "Epoch 8/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.6980 - acc: 0.6744 - val_loss: 0.9548 - val_acc: 0.4399\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.77153\n",
      "Epoch 9/100\n",
      "65512/65512 [==============================] - 4s 66us/step - loss: 0.6948 - acc: 0.6770 - val_loss: 0.9593 - val_acc: 0.4403\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.77153\n",
      "Epoch 10/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6914 - acc: 0.6788 - val_loss: 0.9584 - val_acc: 0.4407\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.77153\n",
      "Epoch 11/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6898 - acc: 0.6804 - val_loss: 0.9576 - val_acc: 0.4411\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.77153\n",
      "Epoch 12/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.6895 - acc: 0.6817 - val_loss: 0.9479 - val_acc: 0.4410\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.77153\n",
      "Epoch 13/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.6827 - acc: 0.6856 - val_loss: 0.9421 - val_acc: 0.4403\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77153\n",
      "Epoch 14/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.6828 - acc: 0.6872 - val_loss: 0.9366 - val_acc: 0.4403\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77153\n",
      "Epoch 15/100\n",
      "65512/65512 [==============================] - 5s 72us/step - loss: 0.6804 - acc: 0.6895 - val_loss: 0.9298 - val_acc: 0.4405\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77153\n",
      "Epoch 16/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.6761 - acc: 0.6919 - val_loss: 0.9224 - val_acc: 0.4411\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.77153\n",
      "Epoch 17/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.6733 - acc: 0.6925 - val_loss: 0.9130 - val_acc: 0.4412\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.77153\n",
      "Epoch 18/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.6731 - acc: 0.6948 - val_loss: 0.9109 - val_acc: 0.4411\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.77153\n",
      "Epoch 19/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6695 - acc: 0.6982 - val_loss: 0.9023 - val_acc: 0.4413\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.77153\n",
      "Epoch 20/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.6693 - acc: 0.6990 - val_loss: 0.8985 - val_acc: 0.4419\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.77153\n",
      "Epoch 21/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.6670 - acc: 0.6998 - val_loss: 0.8925 - val_acc: 0.4421\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.77153\n",
      "Epoch 22/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.6653 - acc: 0.6996 - val_loss: 0.8903 - val_acc: 0.4425\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.77153\n",
      "Epoch 23/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6618 - acc: 0.7015 - val_loss: 0.8866 - val_acc: 0.4430\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.77153\n",
      "Epoch 24/100\n",
      "65512/65512 [==============================] - 5s 74us/step - loss: 0.6607 - acc: 0.7039 - val_loss: 0.8801 - val_acc: 0.4437\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.77153\n",
      "Epoch 25/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6588 - acc: 0.7031 - val_loss: 0.8846 - val_acc: 0.4438\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.77153\n",
      "Epoch 26/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.6589 - acc: 0.7037 - val_loss: 0.8826 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.77153\n",
      "Epoch 27/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.6559 - acc: 0.7059 - val_loss: 0.8766 - val_acc: 0.4441\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.77153\n",
      "Epoch 28/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6554 - acc: 0.7028 - val_loss: 0.8741 - val_acc: 0.4447\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.77153\n",
      "Epoch 29/100\n",
      "65512/65512 [==============================] - 4s 68us/step - loss: 0.6531 - acc: 0.7064 - val_loss: 0.8719 - val_acc: 0.4455\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.77153\n",
      "Epoch 30/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.6525 - acc: 0.7065 - val_loss: 0.8734 - val_acc: 0.4462\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.77153\n",
      "Epoch 31/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.6516 - acc: 0.7061 - val_loss: 0.8689 - val_acc: 0.4474\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.77153\n",
      "Epoch 32/100\n",
      "65512/65512 [==============================] - 5s 73us/step - loss: 0.6487 - acc: 0.7080 - val_loss: 0.8594 - val_acc: 0.4494\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.77153\n",
      "Epoch 33/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.6471 - acc: 0.7090 - val_loss: 0.8576 - val_acc: 0.4497\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.77153\n",
      "Epoch 34/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.6480 - acc: 0.7085 - val_loss: 0.8525 - val_acc: 0.4507\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.77153\n",
      "Epoch 35/100\n",
      "65512/65512 [==============================] - 5s 71us/step - loss: 0.6479 - acc: 0.7086 - val_loss: 0.8586 - val_acc: 0.4508\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.77153\n",
      "Epoch 36/100\n",
      "65512/65512 [==============================] - 5s 70us/step - loss: 0.6452 - acc: 0.7101 - val_loss: 0.8530 - val_acc: 0.4521\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.77153\n",
      "Epoch 37/100\n",
      "65512/65512 [==============================] - 5s 69us/step - loss: 0.6407 - acc: 0.7108 - val_loss: 0.8526 - val_acc: 0.4529\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.77153\n",
      "Epoch 38/100\n",
      "65512/65512 [==============================] - 4s 67us/step - loss: 0.6420 - acc: 0.7107 - val_loss: 0.8476 - val_acc: 0.4554\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.77153\n",
      "Epoch 39/100\n",
      "41350/65512 [=================>............] - ETA: 1s - loss: 0.6386 - acc: 0.7109"
     ]
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_sum.fit(Xtrain_sum, Ytrain_sum, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This loads the best weights from the model.\n",
    "# This comes from Reference 6 in References.\n",
    "mlp_sum.load_weights('saved_models/weights.best.mlp_sum.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 10.0167%\n"
     ]
    }
   ],
   "source": [
    "# This prints the accuracy of the model.\n",
    "# This comes from Reference 9 in References.\n",
    "score = mlp_sum.evaluate(Xtest_sum, Ytest_sum, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUvC score for the model is 0.5446.\n"
     ]
    }
   ],
   "source": [
    "# This prints the AUC score for the model.\n",
    "Ypred_sum = mlp_sum.predict(Xtest_sum)\n",
    "ROC_mlp_sum = roc_auc_score(Ytest_sum, Ypred_sum)\n",
    "print(\"The AUvC score for the model is %.4f.\" % ROC_mlp_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This plots the ROC curve for the model.\n",
    "roc_plot(model=mlp_sum, x_test=Xtest_sum,\n",
    "        y_test=Ytest_sum,title=\"ROC: MLP Sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Conclusion'></a>\n",
    "## Conclusion\n",
    "It's difficult to interpret the results of the models, because one has to make an assumption about null values. On the one hand, null values are very important indicators that a company could be willingly not reporting values to hide financial difficulties within the company. However, when null values are imputed into the training dataset, assumptions about these values must be made; in other words, the training set is no longer simply processing raw data, it is also processing the assumptions about the null values.\n",
    "\n",
    "AUC scores were used to assess the performance of the Logistic Regression and MLP models. Different datasets were used on these models, one with no null variables(the \"No Nulls\" dataset), one with imputed null variables(the \"Nulls Only\" dataset), one with imputed null variables and a dummy variable for each column (the \"One Hot\" dataset), indicating whether a given value in the column was null, and a dataset with imputed values and a counting variable that counted the number of null values in a given row of data (the \"Sum\" dataset). The logistic regression models showed the following AUC scores: 0.3675 for the \"No Nulls\" dataset, 0.6903 for \"Nulls Only\", 0.6872 for \"One Hot\", and \"0.6822\" for \"Sum.\" The MLP AUC scores were as follows: 0.5504 for the \"No Nulls\" dataset, 0.6142 for \"Nulls Only\", 0.7289 for \"One Hot\", and 0.7452 for \"Sum.\" All the MLPs outperformed the Logistic Regression benchmark models with the exception of the MLP model for \"Nulls Only\".\n",
    "\n",
    "There's no correct answer in addressing whether a financial institution should use the MLP models. While the MLP models on the \"One Hot\" and \"Sum\" datasets make correct predictions over 70% of the time, a penalty should be added to these models. The difference between the MLP \"Nulls Only\" and the MLP \"NO Null's\" AUC scores should be used to subtract from the AUC score of both the \"One Hot\" and \"Sum\" MLP models. Models with imputed null values have overly optimistic biases due to the assumptions made about the null values; these models do not simply operate on raw data. Subtracting the difference is a way to penalize the model for the bias. The revised scores, using the difference of \"Nulls Only\" and \"No Nulls\" (0.0638), should be 0.6651, and 0.6814 for the MLP One Hot and Sum models respectively. Theoreticallly, a firm investing in Polish manufacturing companies could use these AUC scores to create hedges. Based upon these scores, no firm sould invest entirely long or short in a company, the AUC scores are not good enough to take those risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "3. https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9\n",
    "4. https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\n",
    "5. https://keras.io/getting-started/sequential-model-guide/\n",
    "6. Udacity Machine Learning Engineer Nanodegree Program, Semester 2, Brian Campbell - Dog Breed Classifier Project\n",
    "7. https://keras.io/getting-started/sequential-model-guide/\n",
    "8. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.size.html\n",
    "9. https://keras.io/models/sequential/\n",
    "10. https://machinelearningmastery.com/reproducible-results-neural-networks-keras/\n",
    "11. https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "12. https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n",
    "13. https://stackoverflow.com/questions/273192/how-can-i-safely-create-a-nested-directory\n",
    "14. https://keras.io/optimizers/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
