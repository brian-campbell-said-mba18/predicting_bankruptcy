{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: The Efficacy of Multilayer Perceptron Algorithms in Predicting Bankruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><a href=\"#introduction\">INTRODUCTION</a></li>\n",
    "<li><a href=\"#Benchmark Logistic Regression\">Benchmark Logistic Regression</a></li>\n",
    "<li><a href=\"#MLP Model\">MLP Model</a></li>\n",
    "<li><a href=\"#assess\">Data Assessment</a></li>\n",
    "<li><a href=\"#cleaning\">Data Cleaning</a></li>\n",
    "<li><a href=\"#target\">Separation of Target Variables</a></li>\n",
    "<li><a href=\"#no nulls\">No Nulls Data</a></li>\n",
    "<li><a href=\"#one-hot null\">Creation of One-hot Null Variable</a></li>\n",
    "<li><a href=\"#Creation of Sum Null Variables\">Creation of Sum Null Variables</a></li>\n",
    "<li><a href=\"#PIVOT\">PIVOT</a></li>\n",
    "<li><a href=\"#Data Reorganization\">Data Reorganization</a></li>\n",
    "<li><a href=\"#Data Exploration - Descriptive Statistics\">Data Exploration - Descriptive Statistics</a></li>\n",
    "<li><a href=\"#Exploratory Visualization\">Exploratory Visualization</a></li>\n",
    "<li><a href=\"#Preprocessing\">Preprocessing</a></li>\n",
    "<li><a href=\"#Benchmark: Logistic Regression\">Benchmark: Logistic Regression</a></li>\n",
    "<li><a href=\"#originalMLP\">Origial MLP</a></li>\n",
    "<li><a href=\"#conclusion\">Conclusion</a></li> \n",
    "<li><a href=\"#references\">References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Benchmark Logistic Regression'></a>\n",
    "## Benchmark Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary libraries for the logistic regression models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This imports the AUC score for scoring the models.\n",
    "# This comes from Reference 27 in References.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# These are libraries that will be needed to organize data,\n",
    "# graph data, and change the working directory.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the no_nulls X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nonulls = pd.read_csv('no-peaking/Xtrain_nonulls.csv')\n",
    "Xtrain_nonulls = np.array(Xtrain_nonulls)\n",
    "Xtest_nonulls = pd.read_csv('no-peaking/Xtest_nonulls.csv')\n",
    "Xtest_nonulls = np.array(Xtest_nonulls)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nonulls = pd.read_csv('no-peaking/Ytrain_nonulls.csv')\n",
    "Ytrain_nonulls = np.array(Ytrain_nonulls)\n",
    "Ytrain_nonulls = Ytrain_nonulls.ravel() \n",
    "Ytest_nonulls = pd.read_csv('no-peaking/Ytest_nonulls.csv')\n",
    "Ytest_nonulls = np.array(Ytest_nonulls)\n",
    "Ytest_nonulls = Ytest_nonulls.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Hold-out Cross-Validation on just the training set\n",
    "K-fold Cross-Validation is a standard method for preventing a logistic regression model from overfitting. However, because the testing set, the fifth year of the dataset, is arbitrarily chosen (not random), K-fold Cross-Validation cannot be applied to the dataset (Reference 3). K-fold Cross-Validation would corrupt the testing set with data leakage considering that the dataset is a time-series set (Reference 3). To prevent data leakage, Hold-out cross-Validation will only be applied to the training set (References 3 & 4). Hold-out Cross-Validation takes a percentage of the training set as a validation set to test the accuracy of the model during the training stage. This method of cross validation, like all methods, is used to prevent the overfitting of a model and poor accuracy performance when applying the testing data to the fitted model (Reference 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Nulls Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import train_test_split from sklearn.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the No Nulls dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nonulls, Xval_nonulls, Ytrain_nonulls, Yval_nonulls = train_test_split(\n",
    "                    Xtrain_nonulls, Ytrain_nonulls, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8506022247116249"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the No Nulls Dataset.\n",
    "log_nonulls = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_nonulls model with the training data.\n",
    "log_nonulls.fit(Xtrain_nonulls,Ytrain_nonulls)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nonulls = log_nonulls.predict(Xval_nonulls)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nonulls = roc_auc_score(Yval_nonulls, yval_pred_nonulls)\n",
    "VAL_auc_nonulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.3163.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nonulls = log_nonulls.predict(Xtest_nonulls)\n",
    "TEST_auc_nonulls = roc_auc_score(Ytest_nonulls, ytest_pred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Nulls only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This loads the Nulls only training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nullsonly = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_nullsonly = Xtrain_nullsonly.drop(\n",
    "    Xtrain_nullsonly.columns[64], axis=1)\n",
    "Xtrain_nullsonly = np.array(Xtrain_nullsonly)\n",
    "Xtest_nullsonly = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_nullsonly = Xtest_nullsonly.drop(\n",
    "    Xtest_nullsonly.columns[64], axis=1)\n",
    "Xtest_nullsonly = np.array(Xtest_nullsonly)\n",
    "\n",
    "# This loads the Nulls only Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nullsonly = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_nullsonly = np.array(Ytrain_nullsonly)\n",
    "Ytrain_nullsonly = Ytrain_nullsonly.ravel() \n",
    "Ytest_nullsonly = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_nullsonly = np.array(Ytest_nullsonly)\n",
    "Ytest_nullsonly = Ytest_nullsonly.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls Only Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nullsonly, Xval_nullsonly, Ytrain_nullsonly, Yval_nullsonly = train_test_split(\n",
    "                    Xtrain_nullsonly, Ytrain_nullsonly, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7350826303765834"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the Nulls Only Dataset.\n",
    "log_nullsonly = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_nullsonly.fit(Xtrain_nullsonly,Ytrain_nullsonly)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nullsonly = log_nullsonly.predict(Xval_nullsonly)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nullsonly = roc_auc_score(Yval_nullsonly, yval_pred_nullsonly)\n",
    "VAL_auc_nullsonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 68.29 %\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nullsonly = log_nullsonly.predict(Xtest_nullsonly)\n",
    "TEST_auc_nullsonly = roc_auc_score(Ytest_nullsonly, ytest_pred_nullsonly)\n",
    "TEST_auc_nullsonly = TEST_auc_nullsonly * 100\n",
    "print(\"The AUC score for the model is %.2f\" % TEST_auc_nullsonly, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the one hot X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_onehot = pd.read_csv('no-peaking/Xtrain_onehot.csv')\n",
    "Xtrain_onehot = np.array(Xtrain_onehot)\n",
    "Xtest_onehot = pd.read_csv('no-peaking/Xtest_onehot.csv')\n",
    "Xtest_onehot = np.array(Xtest_onehot)\n",
    "\n",
    "# This loads the one hot Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_onehot = pd.read_csv('no-peaking/Ytrain_onehot.csv')\n",
    "Ytrain_onehot = np.array(Ytrain_onehot)\n",
    "Ytrain_onehot = Ytrain_onehot.ravel() \n",
    "Ytest_onehot = pd.read_csv('no-peaking/Ytest_onehot.csv')\n",
    "Ytest_onehot = np.array(Ytest_onehot)\n",
    "Ytest_onehot = Ytest_onehot.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the One Hot dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_onehot, Xval_onehot, Ytrain_onehot, Yval_onehot = train_test_split(\n",
    "                    Xtrain_onehot, Ytrain_onehot, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8647068328667264"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_onehot = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_onehot.fit(Xtrain_onehot,Ytrain_onehot)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_onehot = log_onehot.predict(Xval_onehot)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_onehot = roc_auc_score(Yval_onehot, yval_pred_onehot)\n",
    "VAL_auc_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6406.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_onehot = log_onehot.predict(Xtest_onehot)\n",
    "TEST_auc_onehot = roc_auc_score(Ytest_onehot, ytest_pred_onehot)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the SUM X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_sum = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_sum = np.array(Xtrain_sum)\n",
    "Xtest_sum = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_sum = np.array(Xtest_sum)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_sum = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_sum = np.array(Ytrain_sum)\n",
    "Ytrain_sum = Ytrain_sum.ravel() \n",
    "Ytest_sum = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_sum = np.array(Ytest_sum)\n",
    "Ytest_sum = Ytest_sum.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_sum, Xval_sum, Ytrain_sum, Yval_sum = train_test_split(\n",
    "                    Xtrain_sum, Ytrain_sum, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7437902237728553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_sum = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_sum.fit(Xtrain_sum,Ytrain_sum)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.c\n",
    "yval_pred_sum = log_sum.predict(Xval_sum)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_sum = roc_auc_score(Yval_sum, yval_pred_sum)\n",
    "VAL_auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6606.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_sum = log_sum.predict(Xtest_sum)\n",
    "TEST_auc_sum = roc_auc_score(Ytest_sum, ytest_pred_sum)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MLP Model'></a>\n",
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a directory to save the best models for the MLP.\n",
    "os.mkdir('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This imports the necessary libraries for the MLP.\n",
    "\n",
    "# This imports the sequential model, the layers,\n",
    "# the SGD optimizer, the regularizers from keras.\n",
    "# This comes from Reference 5 in Referenes.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "# This imports checkpointer, which records the best weights\n",
    "# for the algorithm.\n",
    "# This comes from Reference 6 in References.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drop_rate, l2_factor, first_dense, second_dense,\n",
    "                third_dense, hidden_act, out_act, x):\n",
    "    dim_int = int(np.size(x,1))\n",
    "    # This defines the model as a sequential model.\n",
    "    # This comes from References 1 in References.\n",
    "    model = Sequential()\n",
    "\n",
    "    # This is the input layer.\n",
    "    # This comes from References 1 & 3 in References.\n",
    "    model.add(Dense(first_dense, activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor),\n",
    "        input_dim = dim_int))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the first hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(second_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # This creates the second hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(third_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the output layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(1, activation=out_act))\n",
    "    # This returns the model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD()\n",
    "\n",
    "# This builds the original MLP model for year 1.\n",
    "# using Mattson and Steinart's original hyperparameters.\n",
    "mlp_nonulls = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Null data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nonulls.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 15 & 16 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nonulls.hdf5',\n",
    "                             verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11018 samples, validate on 2755 samples\n",
      "Epoch 1/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2837 - acc: 0.9316 - val_loss: 0.1713 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17128, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 2/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2936 - acc: 0.9284 - val_loss: 0.1669 - val_acc: 0.9721\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17128 to 0.16686, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 3/100\n",
      "11018/11018 [==============================] - 1s 59us/step - loss: 0.2918 - acc: 0.9258 - val_loss: 0.1656 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16686 to 0.16565, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 4/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.3038 - acc: 0.9232 - val_loss: 0.1921 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16565\n",
      "Epoch 5/100\n",
      "11018/11018 [==============================] - 1s 63us/step - loss: 0.2985 - acc: 0.9253 - val_loss: 0.1696 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16565\n",
      "Epoch 6/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2857 - acc: 0.9290 - val_loss: 0.1664 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16565\n",
      "Epoch 7/100\n",
      "11018/11018 [==============================] - 1s 59us/step - loss: 0.2941 - acc: 0.9278 - val_loss: 0.1622 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16565 to 0.16216, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 8/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2877 - acc: 0.9315 - val_loss: 0.1636 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16216\n",
      "Epoch 9/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2933 - acc: 0.9252 - val_loss: 0.1757 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16216\n",
      "Epoch 10/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2928 - acc: 0.9302 - val_loss: 0.1609 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.16216 to 0.16093, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 11/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2904 - acc: 0.9297 - val_loss: 0.2446 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.16093\n",
      "Epoch 12/100\n",
      "11018/11018 [==============================] - 1s 61us/step - loss: 0.2995 - acc: 0.9226 - val_loss: 0.1638 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.16093\n",
      "Epoch 13/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2925 - acc: 0.9269 - val_loss: 0.1698 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.16093\n",
      "Epoch 14/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2993 - acc: 0.9244 - val_loss: 0.1555 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.16093 to 0.15546, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 15/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2950 - acc: 0.9260 - val_loss: 0.1755 - val_acc: 0.9673\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.15546\n",
      "Epoch 16/100\n",
      "11018/11018 [==============================] - 1s 59us/step - loss: 0.2890 - acc: 0.9293 - val_loss: 0.1681 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.15546\n",
      "Epoch 17/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.2887 - acc: 0.9279 - val_loss: 0.1626 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15546\n",
      "Epoch 18/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2863 - acc: 0.9320 - val_loss: 0.1636 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.15546\n",
      "Epoch 19/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2916 - acc: 0.9278 - val_loss: 0.1552 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.15546 to 0.15517, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 20/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2931 - acc: 0.9319 - val_loss: 0.1881 - val_acc: 0.9612\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15517\n",
      "Epoch 21/100\n",
      "11018/11018 [==============================] - 1s 63us/step - loss: 0.2841 - acc: 0.9320 - val_loss: 0.2478 - val_acc: 0.9260\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15517\n",
      "Epoch 22/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2770 - acc: 0.9317 - val_loss: 0.1614 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15517\n",
      "Epoch 23/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2896 - acc: 0.9334 - val_loss: 0.1847 - val_acc: 0.9630\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15517\n",
      "Epoch 24/100\n",
      "11018/11018 [==============================] - 1s 64us/step - loss: 0.2916 - acc: 0.9300 - val_loss: 0.1619 - val_acc: 0.9750\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.15517\n",
      "Epoch 25/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2695 - acc: 0.9381 - val_loss: 0.1545 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.15517 to 0.15453, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 26/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2868 - acc: 0.9305 - val_loss: 0.2330 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15453\n",
      "Epoch 27/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2813 - acc: 0.9335 - val_loss: 0.1588 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15453\n",
      "Epoch 28/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2787 - acc: 0.9365 - val_loss: 0.1581 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15453\n",
      "Epoch 29/100\n",
      "11018/11018 [==============================] - 1s 61us/step - loss: 0.2818 - acc: 0.9323 - val_loss: 0.1676 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15453\n",
      "Epoch 30/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2888 - acc: 0.9297 - val_loss: 0.1653 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15453\n",
      "Epoch 31/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2763 - acc: 0.9364 - val_loss: 0.1662 - val_acc: 0.9702\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.15453\n",
      "Epoch 32/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2871 - acc: 0.9303 - val_loss: 0.1713 - val_acc: 0.9681\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.15453\n",
      "Epoch 33/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.2771 - acc: 0.9343 - val_loss: 0.1514 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.15453 to 0.15140, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 34/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2697 - acc: 0.9371 - val_loss: 0.2649 - val_acc: 0.9205\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.15140\n",
      "Epoch 35/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2789 - acc: 0.9332 - val_loss: 0.1515 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.15140\n",
      "Epoch 36/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2768 - acc: 0.9367 - val_loss: 0.1622 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.15140\n",
      "Epoch 37/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.2886 - acc: 0.9258 - val_loss: 0.1746 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15140\n",
      "Epoch 38/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2869 - acc: 0.9283 - val_loss: 0.2268 - val_acc: 0.9350\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15140\n",
      "Epoch 39/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2845 - acc: 0.9330 - val_loss: 0.2641 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15140\n",
      "Epoch 40/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2822 - acc: 0.9323 - val_loss: 0.1512 - val_acc: 0.9815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss improved from 0.15140 to 0.15121, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 41/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2949 - acc: 0.9258 - val_loss: 0.1689 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15121\n",
      "Epoch 42/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.2770 - acc: 0.9347 - val_loss: 0.1564 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15121\n",
      "Epoch 43/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2744 - acc: 0.9364 - val_loss: 0.1569 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15121\n",
      "Epoch 44/100\n",
      "11018/11018 [==============================] - 1s 50us/step - loss: 0.2694 - acc: 0.9381 - val_loss: 0.1583 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.15121\n",
      "Epoch 45/100\n",
      "11018/11018 [==============================] - 1s 51us/step - loss: 0.2776 - acc: 0.9336 - val_loss: 0.1576 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.15121\n",
      "Epoch 46/100\n",
      "11018/11018 [==============================] - 1s 49us/step - loss: 0.2761 - acc: 0.9345 - val_loss: 0.2143 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15121\n",
      "Epoch 47/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2767 - acc: 0.9350 - val_loss: 0.1680 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15121\n",
      "Epoch 48/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2859 - acc: 0.9301 - val_loss: 0.1543 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15121\n",
      "Epoch 49/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2800 - acc: 0.9329 - val_loss: 0.1587 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15121\n",
      "Epoch 50/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2845 - acc: 0.9334 - val_loss: 0.1756 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15121\n",
      "Epoch 51/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2943 - acc: 0.9264 - val_loss: 0.1532 - val_acc: 0.9782\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.15121\n",
      "Epoch 52/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2778 - acc: 0.9316 - val_loss: 0.1870 - val_acc: 0.9655\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.15121\n",
      "Epoch 53/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2779 - acc: 0.9326 - val_loss: 0.1630 - val_acc: 0.9691\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.15121\n",
      "Epoch 54/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2748 - acc: 0.9339 - val_loss: 0.1599 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.15121\n",
      "Epoch 55/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2851 - acc: 0.9304 - val_loss: 0.2096 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.15121\n",
      "Epoch 56/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2919 - acc: 0.9262 - val_loss: 0.1547 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.15121\n",
      "Epoch 57/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2854 - acc: 0.9314 - val_loss: 0.1768 - val_acc: 0.9670\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.15121\n",
      "Epoch 58/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2763 - acc: 0.9335 - val_loss: 0.2572 - val_acc: 0.9220\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.15121\n",
      "Epoch 59/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.2785 - acc: 0.9320 - val_loss: 0.2049 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.15121\n",
      "Epoch 60/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.2807 - acc: 0.9344 - val_loss: 0.1515 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.15121\n",
      "Epoch 61/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.2825 - acc: 0.9319 - val_loss: 0.1468 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.15121 to 0.14682, saving model to saved_models/weights.best.mlp_nonulls.hdf5\n",
      "Epoch 62/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2982 - acc: 0.9250 - val_loss: 0.1591 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.14682\n",
      "Epoch 63/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2774 - acc: 0.9332 - val_loss: 0.1498 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.14682\n",
      "Epoch 64/100\n",
      "11018/11018 [==============================] - 1s 52us/step - loss: 0.2825 - acc: 0.9342 - val_loss: 0.1515 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.14682\n",
      "Epoch 65/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2826 - acc: 0.9335 - val_loss: 0.1535 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.14682\n",
      "Epoch 66/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2785 - acc: 0.9342 - val_loss: 0.1522 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.14682\n",
      "Epoch 67/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2733 - acc: 0.9335 - val_loss: 0.1667 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.14682\n",
      "Epoch 68/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2811 - acc: 0.9358 - val_loss: 0.1800 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.14682\n",
      "Epoch 69/100\n",
      "11018/11018 [==============================] - ETA: 0s - loss: 0.2776 - acc: 0.932 - 1s 54us/step - loss: 0.2772 - acc: 0.9327 - val_loss: 0.1658 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.14682\n",
      "Epoch 70/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2878 - acc: 0.9281 - val_loss: 0.1532 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.14682\n",
      "Epoch 71/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2771 - acc: 0.9372 - val_loss: 0.1594 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.14682\n",
      "Epoch 72/100\n",
      "11018/11018 [==============================] - 1s 53us/step - loss: 0.2836 - acc: 0.9298 - val_loss: 0.1502 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.14682\n",
      "Epoch 73/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2850 - acc: 0.9300 - val_loss: 0.1600 - val_acc: 0.9710\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.14682\n",
      "Epoch 74/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2774 - acc: 0.9378 - val_loss: 0.2379 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14682\n",
      "Epoch 75/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2846 - acc: 0.9323 - val_loss: 0.1564 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.14682\n",
      "Epoch 76/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.2776 - acc: 0.9347 - val_loss: 0.1669 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.14682\n",
      "Epoch 77/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2753 - acc: 0.9340 - val_loss: 0.1502 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.14682\n",
      "Epoch 78/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2746 - acc: 0.9318 - val_loss: 0.3468 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.14682\n",
      "Epoch 79/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2764 - acc: 0.9347 - val_loss: 0.1500 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.14682\n",
      "Epoch 80/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2847 - acc: 0.9318 - val_loss: 0.1611 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.14682\n",
      "Epoch 81/100\n",
      "11018/11018 [==============================] - 1s 58us/step - loss: 0.2755 - acc: 0.9342 - val_loss: 0.1471 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.14682\n",
      "Epoch 82/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2834 - acc: 0.9324 - val_loss: 0.3220 - val_acc: 0.8864\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.14682\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2780 - acc: 0.9337 - val_loss: 0.1711 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.14682\n",
      "Epoch 84/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2817 - acc: 0.9319 - val_loss: 0.1515 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.14682\n",
      "Epoch 85/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2820 - acc: 0.9316 - val_loss: 0.1579 - val_acc: 0.9757\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14682\n",
      "Epoch 86/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2877 - acc: 0.9291 - val_loss: 0.1625 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14682\n",
      "Epoch 87/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2752 - acc: 0.9357 - val_loss: 0.1582 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14682\n",
      "Epoch 88/100\n",
      "11018/11018 [==============================] - 1s 56us/step - loss: 0.2849 - acc: 0.9309 - val_loss: 0.2695 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14682\n",
      "Epoch 89/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2761 - acc: 0.9330 - val_loss: 0.1734 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14682\n",
      "Epoch 90/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2917 - acc: 0.9278 - val_loss: 0.1565 - val_acc: 0.9731\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14682\n",
      "Epoch 91/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2666 - acc: 0.9361 - val_loss: 0.1557 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.14682\n",
      "Epoch 92/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2716 - acc: 0.9358 - val_loss: 0.1572 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14682\n",
      "Epoch 93/100\n",
      "11018/11018 [==============================] - 1s 57us/step - loss: 0.2931 - acc: 0.9273 - val_loss: 0.1557 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14682\n",
      "Epoch 94/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2746 - acc: 0.9357 - val_loss: 0.1501 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14682\n",
      "Epoch 95/100\n",
      "11018/11018 [==============================] - 1s 54us/step - loss: 0.2906 - acc: 0.9300 - val_loss: 0.1780 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14682\n",
      "Epoch 96/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2835 - acc: 0.9333 - val_loss: 0.1913 - val_acc: 0.9528\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.14682\n",
      "Epoch 97/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2819 - acc: 0.9319 - val_loss: 0.1549 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.14682\n",
      "Epoch 98/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2851 - acc: 0.9300 - val_loss: 0.1659 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.14682\n",
      "Epoch 99/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2733 - acc: 0.9368 - val_loss: 0.1843 - val_acc: 0.9619\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.14682\n",
      "Epoch 100/100\n",
      "11018/11018 [==============================] - 1s 55us/step - loss: 0.2787 - acc: 0.9341 - val_loss: 0.1712 - val_acc: 0.9728\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.14682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef92329f28>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nonulls.fit(Xtrain_nonulls, Ytrain_nonulls, validation_split=0.20,\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_nonulls.load_weights('saved_models/weights.best.mlp_nonulls.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acuracy: 96.6277%\n"
     ]
    }
   ],
   "source": [
    "score = mlp_nonulls.evaluate(Xtest_nonulls, Ytest_nonulls, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Test acuracy: %.4f%%' %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.5000.\n"
     ]
    }
   ],
   "source": [
    "Ypred_nonulls = mlp_nonulls.predict(Xtest_nonulls)\n",
    "mlp_nonulls_ROC = roc_auc_score(Ytest_nonulls, Ypred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % mlp_nonulls_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Ypred_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Frequency of unique values of the said array:\n",
      "[[   0.]\n",
      " [2995.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original array:\")\n",
    "print(Ypred_nonulls)\n",
    "unique_elements, counts_elements = np.unique(Ypred_nonulls, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "3. https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9\n",
    "4. https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\n",
    "5. https://keras.io/getting-started/sequential-model-guide/\n",
    "6. Udacity Machine Learning Engineer Nanodegree Program, Semester 2, Brian Campbell - Dog Breed Classifier Project\n",
    "7. https://keras.io/getting-started/sequential-model-guide/\n",
    "8. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.size.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
