{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: The Efficacy of Multilayer Perceptron Algorithms in Predicting Bankruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><a href=\"#introduction\">INTRODUCTION</a></li>\n",
    "<li><a href=\"#Benchmark Logistic Regression\">Benchmark Logistic Regression</a></li>\n",
    "<li><a href=\"#MLP Model\">MLP Model</a></li>\n",
    "<li><a href=\"#assess\">Data Assessment</a></li>\n",
    "<li><a href=\"#cleaning\">Data Cleaning</a></li>\n",
    "<li><a href=\"#target\">Separation of Target Variables</a></li>\n",
    "<li><a href=\"#no nulls\">No Nulls Data</a></li>\n",
    "<li><a href=\"#one-hot null\">Creation of One-hot Null Variable</a></li>\n",
    "<li><a href=\"#Creation of Sum Null Variables\">Creation of Sum Null Variables</a></li>\n",
    "<li><a href=\"#PIVOT\">PIVOT</a></li>\n",
    "<li><a href=\"#Data Reorganization\">Data Reorganization</a></li>\n",
    "<li><a href=\"#Data Exploration - Descriptive Statistics\">Data Exploration - Descriptive Statistics</a></li>\n",
    "<li><a href=\"#Exploratory Visualization\">Exploratory Visualization</a></li>\n",
    "<li><a href=\"#Preprocessing\">Preprocessing</a></li>\n",
    "<li><a href=\"#Benchmark: Logistic Regression\">Benchmark: Logistic Regression</a></li>\n",
    "<li><a href=\"#originalMLP\">Origial MLP</a></li>\n",
    "<li><a href=\"#conclusion\">Conclusion</a></li> \n",
    "<li><a href=\"#references\">References</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Benchmark Logistic Regression'></a>\n",
    "## Benchmark Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This imports the necessary libraries for the logistic regression models.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This imports the AUC score for scoring the models.\n",
    "# This comes from Reference 27 in References.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# These are libraries that will be needed to organize data,\n",
    "# graph data, and change the working directory.\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: No Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the no_nulls X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nonulls = pd.read_csv('no-peaking/Xtrain_nonulls.csv')\n",
    "Xtrain_nonulls = np.array(Xtrain_nonulls)\n",
    "Xtest_nonulls = pd.read_csv('no-peaking/Xtest_nonulls.csv')\n",
    "Xtest_nonulls = np.array(Xtest_nonulls)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nonulls = pd.read_csv('no-peaking/Ytrain_nonulls.csv')\n",
    "Ytrain_nonulls = np.array(Ytrain_nonulls)\n",
    "Ytrain_nonulls = Ytrain_nonulls.ravel() \n",
    "Ytest_nonulls = pd.read_csv('no-peaking/Ytest_nonulls.csv')\n",
    "Ytest_nonulls = np.array(Ytest_nonulls)\n",
    "Ytest_nonulls = Ytest_nonulls.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Hold-out Cross-Validation on just the training set\n",
    "K-fold Cross-Validation is a standard method for preventing a logistic regression model from overfitting. However, because the testing set, the fifth year of the dataset, is arbitrarily chosen (not random), K-fold Cross-Validation cannot be applied to the dataset (Reference 3). K-fold Cross-Validation would corrupt the testing set with data leakage considering that the dataset is a time-series set (Reference 3). To prevent data leakage, Hold-out cross-Validation will only be applied to the training set (References 3 & 4). Hold-out Cross-Validation takes a percentage of the training set as a validation set to test the accuracy of the model during the training stage. This method of cross validation, like all methods, is used to prevent the overfitting of a model and poor accuracy performance when applying the testing data to the fitted model (Reference 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Nulls Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import train_test_split from sklearn.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the No Nulls dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nonulls, Xval_nonulls, Ytrain_nonulls, Yval_nonulls = train_test_split(\n",
    "                    Xtrain_nonulls, Ytrain_nonulls, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8506022247116249"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the No Nulls Dataset.\n",
    "log_nonulls = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_nonulls model with the training data.\n",
    "log_nonulls.fit(Xtrain_nonulls,Ytrain_nonulls)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nonulls = log_nonulls.predict(Xval_nonulls)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nonulls = roc_auc_score(Yval_nonulls, yval_pred_nonulls)\n",
    "VAL_auc_nonulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.3163.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nonulls = log_nonulls.predict(Xtest_nonulls)\n",
    "TEST_auc_nonulls = roc_auc_score(Ytest_nonulls, ytest_pred_nonulls)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_nonulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Nulls only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This loads the Nulls only training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_nullsonly = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_nullsonly = Xtrain_nullsonly.drop(\n",
    "    Xtrain_nullsonly.columns[64], axis=1)\n",
    "Xtrain_nullsonly = np.array(Xtrain_nullsonly)\n",
    "Xtest_nullsonly = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_nullsonly = Xtest_nullsonly.drop(\n",
    "    Xtest_nullsonly.columns[64], axis=1)\n",
    "Xtest_nullsonly = np.array(Xtest_nullsonly)\n",
    "\n",
    "# This loads the Nulls only Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_nullsonly = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_nullsonly = np.array(Ytrain_nullsonly)\n",
    "Ytrain_nullsonly = Ytrain_nullsonly.ravel() \n",
    "Ytest_nullsonly = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_nullsonly = np.array(Ytest_nullsonly)\n",
    "Ytest_nullsonly = Ytest_nullsonly.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls Only Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_nullsonly, Xval_nullsonly, Ytrain_nullsonly, Yval_nullsonly = train_test_split(\n",
    "                    Xtrain_nullsonly, Ytrain_nullsonly, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7350826303765834"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the Nulls Only Dataset.\n",
    "log_nullsonly = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_nullsonly.fit(Xtrain_nullsonly,Ytrain_nullsonly)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_nullsonly = log_nullsonly.predict(Xval_nullsonly)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_nullsonly = roc_auc_score(Yval_nullsonly, yval_pred_nullsonly)\n",
    "VAL_auc_nullsonly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 68.29 %\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_nullsonly = log_nullsonly.predict(Xtest_nullsonly)\n",
    "TEST_auc_nullsonly = roc_auc_score(Ytest_nullsonly, ytest_pred_nullsonly)\n",
    "TEST_auc_nullsonly = TEST_auc_nullsonly * 100\n",
    "print(\"The AUC score for the model is %.2f\" % TEST_auc_nullsonly, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the one hot X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_onehot = pd.read_csv('no-peaking/Xtrain_onehot.csv')\n",
    "Xtrain_onehot = np.array(Xtrain_onehot)\n",
    "Xtest_onehot = pd.read_csv('no-peaking/Xtest_onehot.csv')\n",
    "Xtest_onehot = np.array(Xtest_onehot)\n",
    "\n",
    "# This loads the one hot Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_onehot = pd.read_csv('no-peaking/Ytrain_onehot.csv')\n",
    "Ytrain_onehot = np.array(Ytrain_onehot)\n",
    "Ytrain_onehot = Ytrain_onehot.ravel() \n",
    "Ytest_onehot = pd.read_csv('no-peaking/Ytest_onehot.csv')\n",
    "Ytest_onehot = np.array(Ytest_onehot)\n",
    "Ytest_onehot = Ytest_onehot.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the One Hot dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_onehot, Xval_onehot, Ytrain_onehot, Yval_onehot = train_test_split(\n",
    "                    Xtrain_onehot, Ytrain_onehot, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8647068328667264"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_onehot = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_onehot.fit(Xtrain_onehot,Ytrain_onehot)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.\n",
    "yval_pred_onehot = log_onehot.predict(Xval_onehot)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_onehot = roc_auc_score(Yval_onehot, yval_pred_onehot)\n",
    "VAL_auc_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6406.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_onehot = log_onehot.predict(Xtest_onehot)\n",
    "TEST_auc_onehot = roc_auc_score(Ytest_onehot, ytest_pred_onehot)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the SUM X training and testing data\n",
    "# from the CSVs and converts the data to np.arrays.\n",
    "Xtrain_sum = pd.read_csv('no-peaking/Xtrain_sum.csv')\n",
    "Xtrain_sum = np.array(Xtrain_sum)\n",
    "Xtest_sum = pd.read_csv('no-peaking/Xtest_sum.csv')\n",
    "Xtest_sum = np.array(Xtest_sum)\n",
    "\n",
    "# This loads the no_nulls Y training and tesing data\n",
    "# from the CSVs. It also ravels the y_data, so only \n",
    "# rows are shown, not columns.\n",
    "Ytrain_sum = pd.read_csv('no-peaking/Ytrain_sum.csv')\n",
    "Ytrain_sum = np.array(Ytrain_sum)\n",
    "Ytrain_sum = Ytrain_sum.ravel() \n",
    "Ytest_sum = pd.read_csv('no-peaking/Ytest_sum.csv')\n",
    "Ytest_sum = np.array(Ytest_sum)\n",
    "Ytest_sum = Ytest_sum.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Logistic Regression Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the hold-out validation set for the logistic regression model\n",
    "# for the Sum dataset.\n",
    "# This comes from Reference 4 in References.\n",
    "Xtrain_sum, Xval_sum, Ytrain_sum, Yval_sum = train_test_split(\n",
    "                    Xtrain_sum, Ytrain_sum, test_size = 0.3, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7437902237728553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates the logistic regression model for the One Hot Dataset.\n",
    "log_sum = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# This fits the log_onehot model with the training data.\n",
    "log_sum.fit(Xtrain_sum,Ytrain_sum)\n",
    "\n",
    "# This predicts the y values from the Xval dataset.c\n",
    "yval_pred_sum = log_sum.predict(Xval_sum)\n",
    "\n",
    "# This returns the validation AUC score.\n",
    "VAL_auc_sum = roc_auc_score(Yval_sum, yval_pred_sum)\n",
    "VAL_auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the model is 0.6606.\n"
     ]
    }
   ],
   "source": [
    "# This tests the model, built on the training data, with the\n",
    "# testing data from year 5.\n",
    "ytest_pred_sum = log_sum.predict(Xtest_sum)\n",
    "TEST_auc_sum = roc_auc_score(Ytest_sum, ytest_pred_sum)\n",
    "print(\"The AUC score for the model is %.4f.\" % TEST_auc_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MLP Model'></a>\n",
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a directory to save the best models for the MLP.\n",
    "os.mkdir('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This imports the necessary libraries for the MLP.\n",
    "\n",
    "# This imports the sequential model, the layers,\n",
    "# the SGD optimizer, the regularizers from keras.\n",
    "# This comes from Reference 5 in Referenes.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "\n",
    "# This imports checkpointer, which records the best weights\n",
    "# for the algorithm.\n",
    "# This comes from Reference 6 in References.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(drop_rate, l2_factor, first_dense, second_dense,\n",
    "                third_dense, hidden_act, out_act, x):\n",
    "    dim_int = int(np.size(x,1))\n",
    "    # This defines the model as a sequential model.\n",
    "    # This comes from References 1 in References.\n",
    "    model = Sequential()\n",
    "\n",
    "    # This is the input layer.\n",
    "    # This comes from References 1 & 3 in References.\n",
    "    model.add(Dense(first_dense, activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor),\n",
    "        input_dim = dim_int))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the first hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(second_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # This creates the second hidden layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(third_dense,\n",
    "        activation = hidden_act,\n",
    "        kernel_regularizer = regularizers.l2(l2_factor)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # This creates the output layer.\n",
    "    # This comes from Reference 7 in References.\n",
    "    model.add(Dense(1, activation=out_act))\n",
    "    # This returns the model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes from Reference 7 in References.\n",
    "n_epochs = 100\n",
    "size_of_batch = 50\n",
    "stochastic = SGD()\n",
    "\n",
    "# This builds the original MLP model for year 1.\n",
    "# using Mattson and Steinart's original hyperparameters.\n",
    "mlp_nonulls = build_model(drop_rate=0.5,\n",
    "                            l2_factor=0.001,\n",
    "                             first_dense=32,\n",
    "                             second_dense=16,\n",
    "                             third_dense=8,\n",
    "                            hidden_act='relu',\n",
    "                            out_act='sigmoid',\n",
    "                            x=Xtrain_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This compiles the MLP model for the No Null data.\n",
    "# This comes from Reference 13 in References.\n",
    "mlp_nonulls.compile(loss='binary_crossentropy',\n",
    "              optimizer= stochastic,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates checkpointer, which uses ModelCheckpoint to store the\n",
    "# best weights of the model.\n",
    "# This comes from References 15 & 16 in References.\n",
    "checkpoint = ModelCheckpoint(filepath='saved_models/weights.best.mlp_nonulls.hdf5',\n",
    "                              monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "                            mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13773 samples, validate on 5903 samples\n",
      "Epoch 1/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3771 - acc: 0.8777 - val_loss: 0.2638 - val_acc: 0.9182\n",
      "Epoch 2/100\n",
      " 3050/13773 [=====>........................] - ETA: 0s - loss: 0.3877 - acc: 0.8751"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgcam\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\callbacks.py:434: RuntimeWarning: Can save best model only with val_accuracy available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3847 - acc: 0.8766 - val_loss: 0.2717 - val_acc: 0.9261\n",
      "Epoch 3/100\n",
      "13773/13773 [==============================] - 1s 63us/step - loss: 0.3800 - acc: 0.8727 - val_loss: 0.2575 - val_acc: 0.9300\n",
      "Epoch 4/100\n",
      "13773/13773 [==============================] - 1s 64us/step - loss: 0.3829 - acc: 0.8787 - val_loss: 0.2568 - val_acc: 0.9275\n",
      "Epoch 5/100\n",
      "13773/13773 [==============================] - 1s 63us/step - loss: 0.3768 - acc: 0.8773 - val_loss: 0.2556 - val_acc: 0.9256\n",
      "Epoch 6/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3753 - acc: 0.8766 - val_loss: 0.2527 - val_acc: 0.9292\n",
      "Epoch 7/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3799 - acc: 0.8796 - val_loss: 0.2535 - val_acc: 0.9275\n",
      "Epoch 8/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3809 - acc: 0.8752 - val_loss: 0.2693 - val_acc: 0.9145\n",
      "Epoch 9/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3745 - acc: 0.8782 - val_loss: 0.2547 - val_acc: 0.9275\n",
      "Epoch 10/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3782 - acc: 0.8783 - val_loss: 0.2463 - val_acc: 0.9300\n",
      "Epoch 11/100\n",
      "13773/13773 [==============================] - 1s 63us/step - loss: 0.3724 - acc: 0.8829 - val_loss: 0.2548 - val_acc: 0.9238\n",
      "Epoch 12/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3672 - acc: 0.8864 - val_loss: 0.2446 - val_acc: 0.9311\n",
      "Epoch 13/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3702 - acc: 0.8813 - val_loss: 0.2460 - val_acc: 0.9326\n",
      "Epoch 14/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3709 - acc: 0.8819 - val_loss: 0.2482 - val_acc: 0.9326\n",
      "Epoch 15/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3628 - acc: 0.8878 - val_loss: 0.2416 - val_acc: 0.9311\n",
      "Epoch 16/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3693 - acc: 0.8855 - val_loss: 0.2551 - val_acc: 0.9209\n",
      "Epoch 17/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3666 - acc: 0.8875 - val_loss: 0.2465 - val_acc: 0.9290\n",
      "Epoch 18/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3633 - acc: 0.8883 - val_loss: 0.2587 - val_acc: 0.9263\n",
      "Epoch 19/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3689 - acc: 0.8843 - val_loss: 0.2418 - val_acc: 0.9334\n",
      "Epoch 20/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3569 - acc: 0.8900 - val_loss: 0.2367 - val_acc: 0.9349\n",
      "Epoch 21/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3560 - acc: 0.8907 - val_loss: 0.2475 - val_acc: 0.9256\n",
      "Epoch 22/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3577 - acc: 0.8941 - val_loss: 0.2388 - val_acc: 0.9329\n",
      "Epoch 23/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3550 - acc: 0.8888 - val_loss: 0.2634 - val_acc: 0.9107\n",
      "Epoch 24/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3614 - acc: 0.8917 - val_loss: 0.3166 - val_acc: 0.8853\n",
      "Epoch 25/100\n",
      "13773/13773 [==============================] - 1s 61us/step - loss: 0.3571 - acc: 0.8914 - val_loss: 0.2227 - val_acc: 0.9488\n",
      "Epoch 26/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3573 - acc: 0.8925 - val_loss: 0.2279 - val_acc: 0.9463\n",
      "Epoch 27/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3479 - acc: 0.8945 - val_loss: 0.2254 - val_acc: 0.9402\n",
      "Epoch 28/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3508 - acc: 0.8907 - val_loss: 0.2282 - val_acc: 0.9438\n",
      "Epoch 29/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3449 - acc: 0.8981 - val_loss: 0.2203 - val_acc: 0.9487\n",
      "Epoch 30/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3465 - acc: 0.8963 - val_loss: 0.2255 - val_acc: 0.9460\n",
      "Epoch 31/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3411 - acc: 0.8985 - val_loss: 0.2234 - val_acc: 0.9427\n",
      "Epoch 32/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3451 - acc: 0.8946 - val_loss: 0.2192 - val_acc: 0.9438\n",
      "Epoch 33/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3554 - acc: 0.8953 - val_loss: 0.2208 - val_acc: 0.9504\n",
      "Epoch 34/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3501 - acc: 0.8928 - val_loss: 0.2261 - val_acc: 0.9370\n",
      "Epoch 35/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3425 - acc: 0.8993 - val_loss: 0.2381 - val_acc: 0.9295\n",
      "Epoch 36/100\n",
      "13773/13773 [==============================] - 1s 57us/step - loss: 0.3446 - acc: 0.8999 - val_loss: 0.2296 - val_acc: 0.9416\n",
      "Epoch 37/100\n",
      "13773/13773 [==============================] - 1s 66us/step - loss: 0.3440 - acc: 0.8974 - val_loss: 0.2134 - val_acc: 0.9522\n",
      "Epoch 38/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3435 - acc: 0.9004 - val_loss: 0.2121 - val_acc: 0.9473\n",
      "Epoch 39/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3461 - acc: 0.8967 - val_loss: 0.2147 - val_acc: 0.9500\n",
      "Epoch 40/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3384 - acc: 0.9002 - val_loss: 0.2271 - val_acc: 0.9355\n",
      "Epoch 41/100\n",
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3437 - acc: 0.8973 - val_loss: 0.2135 - val_acc: 0.9470\n",
      "Epoch 42/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3358 - acc: 0.9021 - val_loss: 0.2090 - val_acc: 0.9471\n",
      "Epoch 43/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3369 - acc: 0.9005 - val_loss: 0.2141 - val_acc: 0.9444\n",
      "Epoch 44/100\n",
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3483 - acc: 0.8937 - val_loss: 0.2072 - val_acc: 0.9504\n",
      "Epoch 45/100\n",
      "13773/13773 [==============================] - 1s 61us/step - loss: 0.3436 - acc: 0.9006 - val_loss: 0.2056 - val_acc: 0.9585\n",
      "Epoch 46/100\n",
      "13773/13773 [==============================] - 1s 66us/step - loss: 0.3426 - acc: 0.9024 - val_loss: 0.2051 - val_acc: 0.9571\n",
      "Epoch 47/100\n",
      "13773/13773 [==============================] - 1s 57us/step - loss: 0.3369 - acc: 0.9034 - val_loss: 0.2039 - val_acc: 0.9526\n",
      "Epoch 48/100\n",
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3365 - acc: 0.9029 - val_loss: 0.2086 - val_acc: 0.9580\n",
      "Epoch 49/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3298 - acc: 0.9045 - val_loss: 0.2012 - val_acc: 0.9605\n",
      "Epoch 50/100\n",
      "13773/13773 [==============================] - 1s 57us/step - loss: 0.3347 - acc: 0.9024 - val_loss: 0.2015 - val_acc: 0.9602\n",
      "Epoch 51/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3283 - acc: 0.9018 - val_loss: 0.1996 - val_acc: 0.9500\n",
      "Epoch 52/100\n",
      "13773/13773 [==============================] - 1s 66us/step - loss: 0.3354 - acc: 0.9037 - val_loss: 0.2009 - val_acc: 0.9565\n",
      "Epoch 53/100\n",
      "13773/13773 [==============================] - 1s 61us/step - loss: 0.3375 - acc: 0.9010 - val_loss: 0.2059 - val_acc: 0.9609\n",
      "Epoch 54/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3337 - acc: 0.9042 - val_loss: 0.1997 - val_acc: 0.9526\n",
      "Epoch 55/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3341 - acc: 0.9050 - val_loss: 0.1993 - val_acc: 0.9546\n",
      "Epoch 56/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3397 - acc: 0.9016 - val_loss: 0.2009 - val_acc: 0.9543\n",
      "Epoch 57/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3245 - acc: 0.9068 - val_loss: 0.2018 - val_acc: 0.9517\n",
      "Epoch 58/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3358 - acc: 0.9066 - val_loss: 0.2011 - val_acc: 0.9632\n",
      "Epoch 59/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3342 - acc: 0.9041 - val_loss: 0.2028 - val_acc: 0.9548\n",
      "Epoch 60/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3239 - acc: 0.9092 - val_loss: 0.1962 - val_acc: 0.9592\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3267 - acc: 0.9076 - val_loss: 0.1950 - val_acc: 0.9643\n",
      "Epoch 62/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3286 - acc: 0.9078 - val_loss: 0.2006 - val_acc: 0.9531\n",
      "Epoch 63/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3237 - acc: 0.9067 - val_loss: 0.1932 - val_acc: 0.9593\n",
      "Epoch 64/100\n",
      "13773/13773 [==============================] - 1s 53us/step - loss: 0.3291 - acc: 0.9052 - val_loss: 0.1954 - val_acc: 0.9554\n",
      "Epoch 65/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3212 - acc: 0.9115 - val_loss: 0.2113 - val_acc: 0.9543\n",
      "Epoch 66/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3275 - acc: 0.9085 - val_loss: 0.1893 - val_acc: 0.9609\n",
      "Epoch 67/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3296 - acc: 0.9050 - val_loss: 0.1886 - val_acc: 0.9604\n",
      "Epoch 68/100\n",
      "13773/13773 [==============================] - 1s 62us/step - loss: 0.3290 - acc: 0.9045 - val_loss: 0.2020 - val_acc: 0.9588\n",
      "Epoch 69/100\n",
      "13773/13773 [==============================] - 1s 59us/step - loss: 0.3270 - acc: 0.9058 - val_loss: 0.2080 - val_acc: 0.9453\n",
      "Epoch 70/100\n",
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3311 - acc: 0.9068 - val_loss: 0.1981 - val_acc: 0.9531\n",
      "Epoch 71/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3256 - acc: 0.9069 - val_loss: 0.2036 - val_acc: 0.9551\n",
      "Epoch 72/100\n",
      "13773/13773 [==============================] - 1s 59us/step - loss: 0.3266 - acc: 0.9072 - val_loss: 0.1930 - val_acc: 0.9578\n",
      "Epoch 73/100\n",
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3283 - acc: 0.9058 - val_loss: 0.1922 - val_acc: 0.9541\n",
      "Epoch 74/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3229 - acc: 0.9084 - val_loss: 0.2316 - val_acc: 0.9407\n",
      "Epoch 75/100\n",
      "13773/13773 [==============================] - 1s 62us/step - loss: 0.3213 - acc: 0.9116 - val_loss: 0.1889 - val_acc: 0.9663\n",
      "Epoch 76/100\n",
      "13773/13773 [==============================] - 1s 59us/step - loss: 0.3248 - acc: 0.9084 - val_loss: 0.2245 - val_acc: 0.9429\n",
      "Epoch 77/100\n",
      "13773/13773 [==============================] - 1s 66us/step - loss: 0.3229 - acc: 0.9106 - val_loss: 0.1890 - val_acc: 0.9563\n",
      "Epoch 78/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3173 - acc: 0.9127 - val_loss: 0.1845 - val_acc: 0.9693\n",
      "Epoch 79/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3225 - acc: 0.9126 - val_loss: 0.2050 - val_acc: 0.9507\n",
      "Epoch 80/100\n",
      "13773/13773 [==============================] - 1s 58us/step - loss: 0.3236 - acc: 0.9098 - val_loss: 0.1854 - val_acc: 0.9614\n",
      "Epoch 81/100\n",
      "13773/13773 [==============================] - 1s 60us/step - loss: 0.3155 - acc: 0.9114 - val_loss: 0.1905 - val_acc: 0.9560\n",
      "Epoch 82/100\n",
      "13773/13773 [==============================] - 1s 53us/step - loss: 0.3212 - acc: 0.9119 - val_loss: 0.1861 - val_acc: 0.9683\n",
      "Epoch 83/100\n",
      "13773/13773 [==============================] - 1s 51us/step - loss: 0.3219 - acc: 0.9098 - val_loss: 0.1925 - val_acc: 0.9602\n",
      "Epoch 84/100\n",
      "13773/13773 [==============================] - 1s 50us/step - loss: 0.3224 - acc: 0.9127 - val_loss: 0.1826 - val_acc: 0.9658\n",
      "Epoch 85/100\n",
      "13773/13773 [==============================] - 1s 50us/step - loss: 0.3194 - acc: 0.9123 - val_loss: 0.1883 - val_acc: 0.9595\n",
      "Epoch 86/100\n",
      "13773/13773 [==============================] - 1s 50us/step - loss: 0.3150 - acc: 0.9140 - val_loss: 0.1781 - val_acc: 0.9661\n",
      "Epoch 87/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3185 - acc: 0.9144 - val_loss: 0.1821 - val_acc: 0.9678\n",
      "Epoch 88/100\n",
      "13773/13773 [==============================] - 1s 51us/step - loss: 0.3161 - acc: 0.9129 - val_loss: 0.1779 - val_acc: 0.9717\n",
      "Epoch 89/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3175 - acc: 0.9136 - val_loss: 0.1841 - val_acc: 0.9576\n",
      "Epoch 90/100\n",
      "13773/13773 [==============================] - 1s 72us/step - loss: 0.3136 - acc: 0.9146 - val_loss: 0.1864 - val_acc: 0.9607\n",
      "Epoch 91/100\n",
      "13773/13773 [==============================] - 1s 71us/step - loss: 0.3187 - acc: 0.9113 - val_loss: 0.1875 - val_acc: 0.9565\n",
      "Epoch 92/100\n",
      "13773/13773 [==============================] - 1s 61us/step - loss: 0.3184 - acc: 0.9115 - val_loss: 0.1809 - val_acc: 0.9641\n",
      "Epoch 93/100\n",
      "13773/13773 [==============================] - 1s 54us/step - loss: 0.3256 - acc: 0.9103 - val_loss: 0.1970 - val_acc: 0.9595\n",
      "Epoch 94/100\n",
      "13773/13773 [==============================] - 1s 66us/step - loss: 0.3129 - acc: 0.9156 - val_loss: 0.1696 - val_acc: 0.9688\n",
      "Epoch 95/100\n",
      "13773/13773 [==============================] - 1s 53us/step - loss: 0.3210 - acc: 0.9114 - val_loss: 0.1866 - val_acc: 0.9599\n",
      "Epoch 96/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3220 - acc: 0.9071 - val_loss: 0.1787 - val_acc: 0.9637\n",
      "Epoch 97/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3126 - acc: 0.9118 - val_loss: 0.1936 - val_acc: 0.9517\n",
      "Epoch 98/100\n",
      "13773/13773 [==============================] - 1s 56us/step - loss: 0.3214 - acc: 0.9083 - val_loss: 0.1742 - val_acc: 0.9705\n",
      "Epoch 99/100\n",
      "13773/13773 [==============================] - 1s 52us/step - loss: 0.3066 - acc: 0.9203 - val_loss: 0.1711 - val_acc: 0.9666\n",
      "Epoch 100/100\n",
      "13773/13773 [==============================] - 1s 55us/step - loss: 0.3168 - acc: 0.9119 - val_loss: 0.1913 - val_acc: 0.9588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef92169080>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This fits the model and runs it for 100 epochs.\n",
    "mlp_nonulls.fit(Xtrain_nonulls, Ytrain_nonulls, validation_data=\n",
    "                (Xval_nonulls, Yval_nonulls),\n",
    "                epochs=n_epochs, batch_size=size_of_batch, \n",
    "                callbacks = callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'saved_models/weights.best.mlp_nonulls.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-d218f7f7c1c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp_nonulls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved_models/weights.best.mlp_nonulls.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`load_weights` requires h5py.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1158\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'layer_names'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'model_weights'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'saved_models/weights.best.mlp_nonulls.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "mlp_nonulls.load_weights('saved_models/weights.best.mlp_nonulls.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "3. https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9\n",
    "4. https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/\n",
    "5. https://keras.io/getting-started/sequential-model-guide/\n",
    "6. Udacity Machine Learning Engineer Nanodegree Program, Semester 2, Brian Campbell - Dog Breed Classifier Project\n",
    "7. https://keras.io/getting-started/sequential-model-guide/\n",
    "8. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.size.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
